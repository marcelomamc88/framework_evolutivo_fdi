{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Initial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-11T18:51:55.569446Z",
     "iopub.execute_input": "2022-02-11T18:51:55.569750Z",
     "iopub.status.idle": "2022-02-11T18:52:36.826616Z",
     "shell.execute_reply.started": "2022-02-11T18:51:55.569715Z",
     "shell.execute_reply": "2022-02-11T18:52:36.825542Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = 500\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = 1\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output_ht_NOVO_1_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '.txt'\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS\n",
    "\n",
    "local : ./outputs/\n",
    "google colab : /content/drive/My Drive/\n",
    "'''\n",
    "if google_colab:\n",
    "    PATH_OUTPUTS = '/content/drive/My Drive/'\n",
    "elif kaggle:    \n",
    "    PATH_OUTPUTS = ''\n",
    "else:\n",
    "    PATH_OUTPUTS = './outputs/'\n",
    "\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../../dataset/original/'\n",
    "\n",
    "'''\n",
    "FLUSH FILE\n",
    "\n",
    "If output results file is ON\n",
    "'''\n",
    "FLUSH_FILE = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T19:01:09.047295Z",
     "iopub.execute_input": "2022-02-11T19:01:09.047705Z",
     "iopub.status.idle": "2022-02-11T19:01:09.058194Z",
     "shell.execute_reply.started": "2022-02-11T19:01:09.047665Z",
     "shell.execute_reply": "2022-02-11T19:01:09.057022Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if google_colab:\n",
    "    !pip install git+https://github.com/online-ml/river --upgrade\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown --id 1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown --id 1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ \n",
    "    !gdown --id 1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown --id 1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown --id 17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown --id 1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv('F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv('F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv('F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv('F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv('F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv('F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T18:52:36.843366Z",
     "iopub.execute_input": "2022-02-11T18:52:36.843712Z",
     "iopub.status.idle": "2022-02-11T18:54:31.262782Z",
     "shell.execute_reply.started": "2022-02-11T18:52:36.843640Z",
     "shell.execute_reply": "2022-02-11T18:54:31.261078Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T18:55:30.744842Z",
     "iopub.execute_input": "2022-02-11T18:55:30.745088Z",
     "iopub.status.idle": "2022-02-11T18:55:30.758620Z",
     "shell.execute_reply.started": "2022-02-11T18:55:30.745061Z",
     "shell.execute_reply": "2022-02-11T18:55:30.757818Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T18:55:30.760538Z",
     "iopub.execute_input": "2022-02-11T18:55:30.760792Z",
     "iopub.status.idle": "2022-02-11T18:55:31.230345Z",
     "shell.execute_reply.started": "2022-02-11T18:55:30.760763Z",
     "shell.execute_reply": "2022-02-11T18:55:31.229420Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_results(data_test, phi):\n",
    "    output = np.zeros((len(data_test), 5)) # 7: none, f1, f2, f3, f4\n",
    "\n",
    "    for k, dt in enumerate(data_test):\n",
    "        a = clf.predict_proba(dt)\n",
    "\n",
    "        for t in np.arange(0, a.shape[1]):\n",
    "            output[k, t] = len(a[:,t][a[:,t] >= phi])\n",
    "            #output[k, t] = len(a[:, t][a[:, t] == True])\n",
    "\n",
    "    return output\n",
    "\n",
    "# append fault labels\n",
    "def generate_fault_label(dataset, fault_label):\n",
    "    labels = np.array([[fault_label]]*dataset.shape[0])\n",
    "\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T18:55:31.231769Z",
     "iopub.execute_input": "2022-02-11T18:55:31.231991Z",
     "iopub.status.idle": "2022-02-11T18:55:31.239355Z",
     "shell.execute_reply.started": "2022-02-11T18:55:31.231965Z",
     "shell.execute_reply": "2022-02-11T18:55:31.238343Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "faults = np.concatenate((\n",
    "    np.append(dict_ds['data_ds3_fault1_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault1_original'], 1), axis = 1),\n",
    "    np.append(dict_ds['data_ds3_fault2_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault2_original'], 2), axis = 1)\n",
    "))\n",
    "\n",
    "faults_shuffled = faults.copy()\n",
    "np.random.shuffle(faults_shuffled)\n",
    "folds = np.split(faults_shuffled, 10)\n",
    "\n",
    "fold = folds[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(fold[:, :-1], fold[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(dict_ds['data_ds3_fault3_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault3_original'], 3).reshape(-1), test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(dict_ds['data_ds3_fault4_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault4_original'], 4).reshape(-1), test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T18:55:31.240560Z",
     "iopub.execute_input": "2022-02-11T18:55:31.240810Z",
     "iopub.status.idle": "2022-02-11T18:55:32.170017Z",
     "shell.execute_reply.started": "2022-02-11T18:55:31.240781Z",
     "shell.execute_reply": "2022-02-11T18:55:32.169025Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "log = None\n",
    "if FLUSH_FILE:\n",
    "    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n",
    "\n",
    "\n",
    "p = {'grace_period': [200,100,50],\n",
    "     'split_criterion': ['info_gain', 'gini', 'hellinger'],\n",
    "     'tie_threshold': [0.05, 0.1],\n",
    "     'binary_split': [False, True],\n",
    "     'no_preprune': [False],\n",
    "     'leaf_prediction': ['nb', 'nba', 'mc'],\n",
    "     'nb_threshold': [0, 5],\n",
    "     'phis': [[.9, .66, .75], [1,1,1], [.9,.9,.9]],\n",
    "     'memory_size': [500, 1000, 2000, 5000],\n",
    "     'samples_retrain': [1000, 600, 2000, 5000]}\n",
    "\n",
    "p = {'grace_period': [100],\n",
    "     'split_criterion': ['info_gain'],\n",
    "     'tie_threshold': [ 0.1],\n",
    "     'binary_split': [False,],\n",
    "     'no_preprune': [False],\n",
    "     'leaf_prediction': ['nb'],\n",
    "     'nb_threshold': [0],\n",
    "     'phis': [[.9, .66, .75]],\n",
    "     'memory_size': [1000],\n",
    "     'samples_retrain': [2000]}\n",
    "\n",
    "samples = 10000\n",
    "samples_test = 1000\n",
    "m = 0\n",
    "\n",
    "start_no = 0\n",
    "end_no = 6500\n",
    "\n",
    "for gp in p['grace_period']:\n",
    "    for sc in p['split_criterion']:\n",
    "        for tt in p['tie_threshold']:\n",
    "            for bs in p['binary_split']:\n",
    "                for prune in p['no_preprune']:\n",
    "                    for lp in p['leaf_prediction']:\n",
    "                        for nt in p['nb_threshold']:\n",
    "                            for threshold in p['phis']:\n",
    "                                for memory_size in p['memory_size']:\n",
    "                                    for samples_new_train in p['samples_retrain']:\n",
    "                                        m = m+1\n",
    "\n",
    "                                        if m < start_no or m > end_no:\n",
    "                                            continue\n",
    "\n",
    "                                        print(str(m),'.', end='')\n",
    "\n",
    "                                        print('HT', m, gp, sc, tt, bs, prune, lp, nt, threshold, memory_size, samples, samples_test, samples_new_train, file=log)\n",
    "\n",
    "                                        ss = StandardScaler()\n",
    "                                        ht = HoeffdingTreeClassifier(grace_period=gp, split_criterion=sc, tie_threshold=tt, binary_split=bs, no_preprune=prune, leaf_prediction=lp, nb_threshold=nt)\n",
    "\n",
    "                                        ss.partial_fit(X_train[:samples])\n",
    "                                        ht.partial_fit(ss.transform(X_train[:samples]), y_train[:samples])\n",
    "\n",
    "                                        data_test = np.concatenate((\n",
    "                                            dict_ds['data_ds3_fault1_original'][50000:51000],\n",
    "                                            dict_ds['data_ds3_fault2_original'][50000:51000],\n",
    "                                            dict_ds['data_ds3_fault3_original'][50000:51000],\n",
    "                                            dict_ds['data_ds3_fault4_original'][50000:51000],\n",
    "                                            dict_ds['data_ds3_normal_t1_original'][50000:51000],\n",
    "                                            dict_ds['data_ds3_normal_t2_original'][50000:51000],\n",
    "                                        ))\n",
    "                                        y_ref = [1]*samples_test + [2]*samples_test + [3]*samples_test + [4]*samples_test + [5]*samples_test + [6]*samples_test\n",
    "\n",
    "                                        r = ht.predict_proba(ss.transform(data_test))\n",
    "\n",
    "                                        y_hat = []\n",
    "                                        phi = threshold[0]\n",
    "\n",
    "                                        for n in r:\n",
    "                                            if n[1] >= phi:\n",
    "                                                y_hat.append(1)\n",
    "                                            elif n[2] >= phi:\n",
    "                                                y_hat.append(2)\n",
    "                                            else:\n",
    "                                                y_hat.append(0)\n",
    "\n",
    "                                        y_hat[990] = 0\n",
    "                                        y_hat[991] = 1\n",
    "                                        y_hat[992] = 2\n",
    "                                        y_hat[993] = 3\n",
    "                                        y_hat[994] = 4\n",
    "                                        y_hat[995] = 5\n",
    "                                        y_hat[996] = 6\n",
    "\n",
    "                                        cm = confusion_matrix(y_ref,y_hat, normalize='true') #none true pred all\n",
    "                                        for i in cm[1:,0:5].reshape(30):\n",
    "                                            print(i, file=log)\n",
    "\n",
    "                                        # retrain 3\n",
    "                                        xoriginal_retrain = np.concatenate((X_train[:memory_size], y_train[:memory_size].reshape(-1,1)), axis=1)\n",
    "                                        x3_train = np.concatenate((X_train3[:samples_new_train], y_train3[:samples_new_train].reshape(-1,1)), axis=1)\n",
    "                                        x_retrain = np.concatenate((x3_train, xoriginal_retrain))\n",
    "                                        np.random.shuffle(x_retrain)\n",
    "\n",
    "                                        ss.partial_fit(x_retrain[:memory_size+samples_new_train, :-1])\n",
    "                                        ht.partial_fit(ss.transform(x_retrain[:,:-1]), x_retrain[:,-1])\n",
    "\n",
    "                                        r = ht.predict_proba(ss.transform(data_test))\n",
    "\n",
    "                                        y_hat = []\n",
    "                                        phi = threshold[1]\n",
    "\n",
    "                                        for n in r:\n",
    "                                            if n[1] >= phi:\n",
    "                                                y_hat.append(1)\n",
    "                                            elif n[2] >= phi:\n",
    "                                                y_hat.append(2)\n",
    "                                            elif n[3] >= phi:\n",
    "                                                y_hat.append(3)\n",
    "                                            else:\n",
    "                                                y_hat.append(0)\n",
    "\n",
    "                                        y_hat[990] = 0\n",
    "                                        y_hat[991] = 1\n",
    "                                        y_hat[992] = 2\n",
    "                                        y_hat[993] = 3\n",
    "                                        y_hat[994] = 4\n",
    "                                        y_hat[995] = 5\n",
    "                                        y_hat[996] = 6\n",
    "\n",
    "                                        cm = confusion_matrix(y_ref,y_hat, normalize='true') #none true pred all\n",
    "                                        for i in cm[1:,0:5].reshape(30):\n",
    "                                            print(i, file=log)\n",
    "\n",
    "                                        # retrain 4\n",
    "                                        x4_train = np.concatenate((X_train4[:samples_new_train], y_train4[:samples_new_train].reshape(-1,1)), axis=1)\n",
    "                                        x_retrain_2 = np.concatenate((x4_train[:memory_size], x_retrain))\n",
    "\n",
    "                                        np.random.shuffle(x_retrain_2)\n",
    "\n",
    "                                        ss.partial_fit(x_retrain_2[:memory_size+samples_new_train, :-1])\n",
    "                                        ht.partial_fit(ss.transform(x_retrain_2[:,:-1]), x_retrain_2[:,-1])\n",
    "\n",
    "                                        r = ht.predict_proba(ss.transform(data_test))\n",
    "\n",
    "                                        y_hat = []\n",
    "                                        phi = threshold[2]\n",
    "\n",
    "                                        for n in r:\n",
    "                                            if n[1] >= phi:\n",
    "                                                y_hat.append(1)\n",
    "                                            elif n[2] >= phi:\n",
    "                                                y_hat.append(2)\n",
    "                                            elif n[3] >= phi:\n",
    "                                                y_hat.append(3)\n",
    "                                            elif n[4] >= phi:\n",
    "                                                y_hat.append(4)\n",
    "                                            else:\n",
    "                                                y_hat.append(0)\n",
    "\n",
    "                                        y_hat[990] = 0\n",
    "                                        y_hat[991] = 1\n",
    "                                        y_hat[992] = 2\n",
    "                                        y_hat[993] = 3\n",
    "                                        y_hat[994] = 4\n",
    "                                        y_hat[995] = 5\n",
    "                                        y_hat[996] = 6\n",
    "\n",
    "                                        cm = confusion_matrix(y_ref,y_hat, normalize='true') #none true pred all\n",
    "                                        for i in cm[1:,0:5].reshape(30):\n",
    "                                            print(i, file=log)\n",
    "\n",
    "if FLUSH_FILE:\n",
    "    log.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-02-11T19:01:15.994508Z",
     "iopub.execute_input": "2022-02-11T19:01:15.994818Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .HT 1 100 info_gain 0.1 False False nb 0 [0.9, 0.66, 0.75] 1000 10000 1000 2000\n",
      "0.011\n",
      "0.984\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.01\n",
      "0.0\n",
      "0.99\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.505\n",
      "0.495\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.009\n",
      "0.98\n",
      "0.006\n",
      "0.002\n",
      "0.001\n",
      "0.005\n",
      "0.0\n",
      "0.995\n",
      "0.0\n",
      "0.0\n",
      "0.013\n",
      "0.0\n",
      "0.001\n",
      "0.986\n",
      "0.0\n",
      "0.005\n",
      "0.006\n",
      "0.005\n",
      "0.984\n",
      "0.0\n",
      "0.926\n",
      "0.0\n",
      "0.0\n",
      "0.074\n",
      "0.0\n",
      "0.926\n",
      "0.0\n",
      "0.0\n",
      "0.074\n",
      "0.0\n",
      "0.016\n",
      "0.973\n",
      "0.001\n",
      "0.007\n",
      "0.001\n",
      "0.005\n",
      "0.0\n",
      "0.99\n",
      "0.005\n",
      "0.0\n",
      "0.012\n",
      "0.0\n",
      "0.0\n",
      "0.988\n",
      "0.0\n",
      "0.001\n",
      "0.0\n",
      "0.0\n",
      "0.031\n",
      "0.968\n",
      "0.946\n",
      "0.0\n",
      "0.0\n",
      "0.054\n",
      "0.0\n",
      "0.946\n",
      "0.0\n",
      "0.0\n",
      "0.054\n",
      "0.0\n"
     ]
    }
   ]
  }
 ]
}