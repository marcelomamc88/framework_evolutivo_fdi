{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial","metadata":{}},{"cell_type":"code","source":"#!pip install -U git+https://github.com/scikit-multiflow/scikit-multiflow\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom math import floor\nfrom sklearn.metrics import confusion_matrix\nfrom skmultiflow.lazy import SAMKNNClassifier\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-02-10T12:07:42.301525Z","iopub.execute_input":"2022-02-10T12:07:42.30182Z","iopub.status.idle":"2022-02-10T12:07:42.306883Z","shell.execute_reply.started":"2022-02-10T12:07:42.301788Z","shell.execute_reply":"2022-02-10T12:07:42.306177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nEXTERNAL KERNEL\n'''\ngoogle_colab = False\nkaggle = True\n\n'''\nCUDA\n'''\ncuda = False\n\n'''\nDATA REPRESENTATION\n\n1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n'''\nDATA_REPRESENTATION = 2\n\n'''\nDOWNSAMPLE FACTOR\n\n1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n'''\nDOWNSAMPLE_FACTOR = 5\n\n'''\nWINDOWS LENGHT\n\n* needs divisor by datapoints target\n* considering downsample factor = 5\n\n1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n'''\nWINDOW_LENGHT =  1\n\n'''\nLIMITADOR\n\nQuantity of samples in the execution of the tests.\n'''\nLIMITADOR = 500\n\n'''\nLOSS FACTOR [0,1]\n\nIgnores outliers in calculating the stats of losses in regenerated data.\n'''\nLOSS_FACTOR = 1\n\n'''\nTRAIN_SIZE [0,1]\n\nPercentage of samples to be trained\n'''\nTRAIN_SIZE = 0.8\n\n'''\nOUTPUT_FILE_NAME\n\nFile with output results\n'''\nOUTPUT_FILE_NAME = 'output_samknn_VERSAONOVA_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '_folds.txt'\n\n'''\nPATH_OUTPUTS\n\nlocal : ./outputs/\ngoogle colab : /content/drive/My Drive/\n'''\nif google_colab:\n    PATH_OUTPUTS = '/content/drive/My Drive/'\nelse:\n    PATH_OUTPUTS = './outputs/'\n\n\n'''\nPATH_DATASET\n\n'''\nPATH_DATASET = '../../dataset/original/'\n\n'''\nFLUSH FILE\n\nIf output results file is ON\n'''\nFLUSH_FILE = False","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:07:45.007947Z","iopub.execute_input":"2022-02-10T12:07:45.009295Z","iopub.status.idle":"2022-02-10T12:07:45.018645Z","shell.execute_reply.started":"2022-02-10T12:07:45.009253Z","shell.execute_reply":"2022-02-10T12:07:45.017468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if google_colab:\n    !pip install git+https://github.com/online-ml/river --upgrade\n\n    from google.colab import drive\n\n    drive.mount('/content/drive')\n    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n    dict_ds_original = {\n        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n    }\nelif kaggle:\n    !conda install -y gdown\n    !gdown --id 1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n    !gdown --id 1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ\n    !gdown --id 1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n    !gdown --id 1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n    !gdown --id 17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n    !gdown --id 1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n    \n    dict_ds_original = {\n        'data_ds3_normal_t1_original' : pd.read_csv('F16_DS3_normal_t1.csv', header=None),\n        'data_ds3_normal_t2_original' : pd.read_csv('F16_DS3_normal_t2.csv', header=None),\n        'data_ds3_fault1_original' : pd.read_csv('F16_DS3_fault1_leakage.csv', header=None),\n        'data_ds3_fault2_original' : pd.read_csv('F16_DS3_fault2_viscousfriction.csv', header=None),\n        'data_ds3_fault3_original' : pd.read_csv('F16_DS3_fault3_compressibility.csv', header=None),\n        'data_ds3_fault4_original' : pd.read_csv('F16_DS3_fault4_fixedposition.csv', header=None),\n    }\nelse:\n    dict_ds_original = {\n        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n    }","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:07:48.087011Z","iopub.execute_input":"2022-02-10T12:07:48.087334Z","iopub.status.idle":"2022-02-10T12:09:37.466949Z","shell.execute_reply.started":"2022-02-10T12:07:48.087296Z","shell.execute_reply":"2022-02-10T12:09:37.465706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"dict_ds = dict_ds_original.copy()\n\nif dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n    raise Exception('Needs to be ?shape? divisor')\n\nfor n, dataset_name in enumerate(dict_ds):\n    dataset = dict_ds[dataset_name].to_numpy()\n\n    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n\n    x, y = downsampled.shape\n\n    # resample\n    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:09:37.47066Z","iopub.execute_input":"2022-02-10T12:09:37.471103Z","iopub.status.idle":"2022-02-10T12:09:37.480991Z","shell.execute_reply.started":"2022-02-10T12:09:37.471045Z","shell.execute_reply":"2022-02-10T12:09:37.480142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n\nif (DATA_REPRESENTATION == 2):\n    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n\n    for n, dataset_name in enumerate(dict_ds):\n        dataset = dict_ds[dataset_name].to_numpy()\n\n        dimension = dataset.shape[1]\n        samples = dataset.shape[0]\n\n        # GENERATE NEW DIMENSIONS\n        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n\n        for f in np.arange(0,int(samples/frame_size)):\n            # OBTAIN THE FRAME FLIGHT\n            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n\n            # CALCULATE DIFFERENCE\n            chunk = np.diff(frame, axis=0)\n\n            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n\n            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n\n        dict_ds[dataset_name] = pd.DataFrame(dataset)\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:09:37.482492Z","iopub.execute_input":"2022-02-10T12:09:37.482887Z","iopub.status.idle":"2022-02-10T12:09:37.96261Z","shell.execute_reply.started":"2022-02-10T12:09:37.482841Z","shell.execute_reply":"2022-02-10T12:09:37.96197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def predict(clf, classes, data, threshold_distance = 50, neighbors = 2):\n\n    threshold_dist = threshold_distance\n    neighbors = neighbors\n\n    votes = np.zeros((len(data),len(classes)))\n    mu_dist = np.zeros((len(data),len(classes)))\n\n    y_hat = np.zeros((len(data)))\n\n    for k, c in enumerate(classes):\n        #print('TESTE', c)\n        indexes = np.nonzero(np.where(clf.LTMLabels == c, clf.LTMLabels, 0))\n\n        for s in np.arange(0, len(data)):\n            dist_sample = clf.get_distances(data[s, :-1], clf.LTMSamples[indexes])\n            dist_sample_sort = np.sort(dist_sample)\n\n            if (np.mean(dist_sample_sort[:neighbors]) > threshold_dist):\n                votes[s,k] = 1 # set high distance\n\n            mu_dist[s,k] = np.mean(dist_sample_sort[:neighbors])\n\n        mu_class = mu_dist[:,k]\n        #print ('repulse:', mu_class[mu_class > threshold_dist].shape[0]/len(data)*100)\n\n    for k, vote in enumerate(votes):\n        inliers = np.argwhere((vote == [1.]) == False).reshape(-1).shape[0]\n\n        if (inliers == 0): # (1,1,1)\n            y_hat[k] = -1\n        else:\n            founds = np.argwhere(vote == 0).reshape(-1)\n\n            if len(founds) == 1: # (1,0,1)\n                y_hat[k] = founds[0]+1\n            else: # (1,0,0)\n                minor_value = -1\n                minor_index = -1\n\n                for f in founds:\n                    if minor_index == -1:\n                        minor_value = mu_dist[k,f]\n                        minor_index = f+1\n                    elif mu_dist[k,f] < minor_value:\n                        minor_value = mu_dist[k,f]\n                        minor_index = f+1\n\n                y_hat[k] = minor_index\n\n    return y_hat\n\n\ndef generate_y_hat(probas, threshold):\n    y_hat = []\n\n    for p in probas:\n        if (np.max(p) < threshold): #indecisao\n            y_hat.append(-1)\n        else: #certeza\n            y_hat.append(np.argmax(p))\n\n    return y_hat\n\ndef tester(clf, nt, phi, classes, data_test, log):\n    y_hat = predict(clf, classes, data_test, phi, nt)\n    print(len(y_hat[y_hat == -1]) / len(y_hat) * 100, file=log) #-1\n    print(len(y_hat[y_hat == 1]) / len(y_hat) * 100, file=log) #1\n    print(len(y_hat[y_hat == 2]) / len(y_hat) * 100, file=log) #2\n    print(len(y_hat[y_hat == 3]) / len(y_hat) * 100, file=log) #3\n    print(len(y_hat[y_hat == 4]) / len(y_hat) * 100, file=log) #4\n\n# append fault labels\ndef generate_fault_label(dataset, fault_label):\n    labels = np.array([[fault_label]]*dataset.shape[0])\n\n    return labels\n\ndef get_data_reinforce(clf, samples_retrain, fator):\n    indexes_retrain = []\n\n    samples = clf.STMSamples\n    labels = clf.STMLabels\n\n    for c in np.unique(labels):\n        indexes = np.argwhere(labels == c) # retorna indices da classe\n        idx = np.array(indexes[np.random.randint(0,len(indexes),int(samples_retrain*fator))])\n        indexes_retrain.extend(idx.reshape(-1))\n\n    random.shuffle(indexes_retrain)\n\n    X_retrain = samples[indexes_retrain, :]\n    y_retrain = labels[indexes_retrain]\n\n    return np.append(X_retrain, y_retrain.reshape(-1,1), axis=1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:09:44.792753Z","iopub.execute_input":"2022-02-10T12:09:44.793313Z","iopub.status.idle":"2022-02-10T12:09:44.925318Z","shell.execute_reply.started":"2022-02-10T12:09:44.793264Z","shell.execute_reply":"2022-02-10T12:09:44.924089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"faults = np.concatenate((\n    np.append(dict_ds['data_ds3_fault1_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault1_original'], 1), axis = 1),\n    np.append(dict_ds['data_ds3_fault2_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault2_original'], 2), axis = 1)\n))\n\nfaults_shuffled = faults.copy()\nnp.random.shuffle(faults_shuffled)\nfolds = np.split(faults_shuffled, 10)\n\nfold = folds[0]\nX_train, X_test, y_train, y_test = train_test_split(fold[:, :-1], fold[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n\n#fold = faults.copy()\n#X_train, X_test, y_train, y_test = train_test_split(fold[:, :-1], fold[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=True)\n\n\nX_train3, X_test3, y_train3, y_test3 = train_test_split(dict_ds['data_ds3_fault3_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault3_original'], 3).reshape(-1), test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n\nX_train4, X_test4, y_train4, y_test4 = train_test_split(dict_ds['data_ds3_fault4_original'].copy(), generate_fault_label(dict_ds['data_ds3_fault4_original'], 4).reshape(-1), test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T12:09:48.447841Z","iopub.execute_input":"2022-02-10T12:09:48.448199Z","iopub.status.idle":"2022-02-10T12:09:49.670096Z","shell.execute_reply.started":"2022-02-10T12:09:48.448156Z","shell.execute_reply":"2022-02-10T12:09:49.669082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"log = None\nif FLUSH_FILE:\n    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n\nsamples = 10000\nsamples_test = 1600\nsamples_retrain = 1000\n\np = {'max_window_size': [10000],\n     'phis': [2],\n     'neighbors_test': [2],\n     'n_neighbors': [5],\n     'weighting': ['uniform'],\n     'stm_size_option': ['maxACC']}\n\nfold = folds[0]\nX_train, X_test, y_train, y_test = train_test_split(fold[:, :-1], fold[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n\nfor f in np.arange(0,10):\n    fold = folds[f]\n    X_train, X_test, y_train, y_test = train_test_split(fold[:, :-1], fold[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n\n    ss = StandardScaler()\n\n    clf = SAMKNNClassifier(max_window_size=mws, n_neighbors=n, weighting=w, stm_size_option=sso) # stm_size_option=None\n\n    print('FOLD:', f, file=log)\n\n    # MAIN TRAIN WITH FAULT 1 , 2\n    ss.partial_fit(X_train[:samples])\n    clf.partial_fit(ss.transform(X_train[:samples]), y_train[:samples])\n    print('.', end='')\n\n    # TEST\n    data_test = np.append(X_test, y_test.reshape(-1, 1), axis=1)\n\n    X_test_f1 = data_test[data_test[:,-1] == 1][:,:-1]\n    X_test_f2 = data_test[data_test[:,-1] == 2][:,:-1]\n\n    datas_test = [ss.transform(X_test_f1[:samples_test]),\n                  ss.transform(X_test_f2[:samples_test]),\n                  ss.transform(X_test3[:samples_test]),\n                  ss.transform(X_test4[:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t1_original'][:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t2_original'][:samples_test])\n                  ]\n\n    for data_test in datas_test:\n        tester(clf, nt, phi, [1,2], data_test, log)\n    print('.', end='')\n\n    # NEW TRAIN WITH FAULT 3\n    ss.partial_fit(X_train3[:samples_retrain])\n\n    # reinforce\n    data_reinforce = get_data_reinforce(clf, samples_retrain, 0.5)\n    data = np.append(X_train3[:samples_retrain], y_train3[:samples_retrain].reshape(-1,1), axis=1)\n\n    data_retrain = np.concatenate((data_reinforce, data))\n    np.random.shuffle(data_retrain)\n\n    # partial train\n    clf.partial_fit(ss.transform(data_retrain[:, :-1]), data_retrain[:, -1])\n\n    datas_test = [ss.transform(X_test_f1[:samples_test]),\n                  ss.transform(X_test_f2[:samples_test]),\n                  ss.transform(X_test3[:samples_test]),\n                  ss.transform(X_test4[:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t1_original'][:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t2_original'][:samples_test])\n                  ]\n\n    for data_test in datas_test:\n        tester(clf, nt, phi, [1,2,3], data_test, log)\n    print('.', end='')\n\n    # NEW TRAIN WITH FAULT 4\n    ss.partial_fit(X_train4[:samples_retrain])\n\n    # reinforce\n    data_reinforce = get_data_reinforce(clf, samples_retrain, 0.33)\n    data = np.append(X_train4[:samples_retrain], y_train4[:samples_retrain].reshape(-1,1), axis=1)\n\n    data_retrain = np.concatenate((data_reinforce, data))\n    np.random.shuffle(data_retrain)\n\n    # partial train\n    clf.partial_fit(ss.transform(data_retrain[:, :-1]), data_retrain[:, -1])\n\n    datas_test = [ss.transform(X_test_f1[:samples_test]),\n                  ss.transform(X_test_f2[:samples_test]),\n                  ss.transform(X_test3[:samples_test]),\n                  ss.transform(X_test4[:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t1_original'][:samples_test]),\n                  ss.transform(dict_ds['data_ds3_normal_t2_original'][:samples_test])\n                  ]\n\n    for data_test in datas_test:\n        tester(clf, nt, phi, [1,2,3,4], data_test, log)\n    print('.', end='')\n\nif FLUSH_FILE:\n    log.close()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T17:56:34.208954Z","iopub.execute_input":"2022-02-10T17:56:34.209969Z","iopub.status.idle":"2022-02-10T18:02:00.511936Z","shell.execute_reply.started":"2022-02-10T17:56:34.209877Z","shell.execute_reply":"2022-02-10T18:02:00.510778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}