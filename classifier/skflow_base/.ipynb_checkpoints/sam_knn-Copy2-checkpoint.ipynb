{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import floor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skmultiflow.lazy import SAMKNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = 500\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = 1\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output_samknn_skflow_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '_dw_jupyter.txt'\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS\n",
    "\n",
    "local : ./outputs/\n",
    "google colab : /content/drive/My Drive/\n",
    "'''\n",
    "if google_colab:\n",
    "    PATH_OUTPUTS = '/content/drive/My Drive/'\n",
    "else:\n",
    "    PATH_OUTPUTS = './outputs/'\n",
    "\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../../dataset/original/'\n",
    "\n",
    "'''\n",
    "FLUSH FILE\n",
    "\n",
    "If output results file is ON\n",
    "'''\n",
    "FLUSH_FILE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    !pip install git+https://github.com/online-ml/river --upgrade\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown --id 1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown --id 1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ\n",
    "    !gdown --id 1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown --id 1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown --id 17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown --id 1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "#ss = MinMaxScaler()\n",
    "\n",
    "data_ds3_t1_normal = dict_ds['data_ds3_normal_t1_original'].copy()\n",
    "data_ds3_t2_normal = dict_ds['data_ds3_normal_t2_original'].copy()\n",
    "data_ds3_fault1 = dict_ds['data_ds3_fault1_original'].copy()\n",
    "data_ds3_fault2 = dict_ds['data_ds3_fault2_original'].copy()\n",
    "data_ds3_fault3 = dict_ds['data_ds3_fault3_original'].copy()\n",
    "data_ds3_fault4 = dict_ds['data_ds3_fault4_original'].copy()\n",
    "\n",
    "# fit values\n",
    "ss.partial_fit(data_ds3_t1_normal)\n",
    "ss.partial_fit(data_ds3_t2_normal)\n",
    "ss.partial_fit(data_ds3_fault1)\n",
    "ss.partial_fit(data_ds3_fault2)\n",
    "ss.partial_fit(data_ds3_fault3)\n",
    "ss.partial_fit(data_ds3_fault4)\n",
    "\n",
    "# transform values\n",
    "data_ds3_t1_normal = ss.transform(data_ds3_t1_normal)\n",
    "data_ds3_t2_normal = ss.transform(data_ds3_t2_normal)\n",
    "data_ds3_fault1 = ss.transform(data_ds3_fault1)\n",
    "data_ds3_fault2 = ss.transform(data_ds3_fault2)\n",
    "data_ds3_fault3 = ss.transform(data_ds3_fault3)\n",
    "data_ds3_fault4 = ss.transform(data_ds3_fault4)\n",
    "\n",
    "# append normal labels\n",
    "data_ds3_t1_normal = np.append(data_ds3_t1_normal, np.zeros((data_ds3_t1_normal.shape[0],1)), axis = 1)\n",
    "data_ds3_t2_normal = np.append(data_ds3_t2_normal, np.zeros((data_ds3_t2_normal.shape[0],1)), axis = 1)\n",
    "\n",
    "# append fault labels\n",
    "def generate_fault_label(dataset, fault_label):\n",
    "    labels = np.array([[fault_label]]*dataset.shape[0])\n",
    "\n",
    "    return labels\n",
    "\n",
    "data_ds3_fault1 = np.append(data_ds3_fault1, generate_fault_label(data_ds3_fault1, 1), axis = 1)\n",
    "data_ds3_fault2 = np.append(data_ds3_fault2, generate_fault_label(data_ds3_fault2, 2), axis = 1)\n",
    "data_ds3_fault3 = np.append(data_ds3_fault3, generate_fault_label(data_ds3_fault3, 3), axis = 1)\n",
    "data_ds3_fault4 = np.append(data_ds3_fault4, generate_fault_label(data_ds3_fault4, 4), axis = 1)\n",
    "\n",
    "\n",
    "def tester(clf, nt, phi, classes, data_test, log):\n",
    "    y_hat = predict(clf, classes, data_test, phi, nt)\n",
    "    print(len(y_hat[y_hat == -1]) / len(y_hat) * 100, file=log) #-1\n",
    "    print(len(y_hat[y_hat == 1]) / len(y_hat) * 100, file=log) #1\n",
    "    print(len(y_hat[y_hat == 2]) / len(y_hat) * 100, file=log) #2\n",
    "    print(len(y_hat[y_hat == 3]) / len(y_hat) * 100, file=log) #3\n",
    "    print(len(y_hat[y_hat == 4]) / len(y_hat) * 100, file=log) #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# STRATIFIED\n",
    "\n",
    "'''X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(data_ds3_fault1[:LIMITADOR, :-1], data_ds3_fault1[:LIMITADOR, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data_ds3_fault2[:LIMITADOR, :-1], data_ds3_fault2[:LIMITADOR, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(data_ds3_fault3[:LIMITADOR, :-1], data_ds3_fault3[:LIMITADOR, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(data_ds3_fault4[:LIMITADOR, :-1], data_ds3_fault4[:LIMITADOR, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "X_train = np.concatenate((X_train_1, X_train_2, X_train_3, X_train_4))\n",
    "X_test = np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4))\n",
    "y_train = np.concatenate((y_train_1, y_train_2, y_train_3, y_train_4))\n",
    "y_test = np.concatenate((y_test_1, y_test_2, y_test_3, y_test_4))'''\n",
    "\n",
    "faults = np.concatenate((data_ds3_fault1, data_ds3_fault2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(faults[:, :-1], faults[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=True)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(data_ds3_fault3[:, :-1], data_ds3_fault3[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=True)\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(data_ds3_fault4[:, :-1], data_ds3_fault4[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(clf, classes, data, threshold_distance = 50, neighbors = 2):\n",
    "\n",
    "    threshold_dist = threshold_distance\n",
    "    neighbors = neighbors\n",
    "\n",
    "    votes = np.zeros((len(data),len(classes)))\n",
    "    mu_dist = np.zeros((len(data),len(classes)))\n",
    "\n",
    "    y_hat = np.zeros((len(data)))\n",
    "\n",
    "    for k, c in enumerate(classes):\n",
    "        #print('TESTE', c)\n",
    "        indexes = np.nonzero(np.where(clf.LTMLabels == c, clf.LTMLabels, 0))\n",
    "\n",
    "        for s in np.arange(0, len(data)):\n",
    "            dist_sample = clf.get_distances(data[s, :-1], clf.LTMSamples[indexes])\n",
    "            dist_sample_sort = np.sort(dist_sample)\n",
    "\n",
    "            if (np.mean(dist_sample_sort[:neighbors]) > threshold_dist):\n",
    "                votes[s,k] = 1 # set high distance\n",
    "\n",
    "            mu_dist[s,k] = np.mean(dist_sample_sort[:neighbors])\n",
    "\n",
    "        mu_class = mu_dist[:,k]\n",
    "        #print ('repulse:', mu_class[mu_class > threshold_dist].shape[0]/len(data)*100)\n",
    "\n",
    "    for k, vote in enumerate(votes):\n",
    "        inliers = np.argwhere((vote == [1.]) == False).reshape(-1).shape[0]\n",
    "\n",
    "        if (inliers == 0): # (1,1,1)\n",
    "            y_hat[k] = -1\n",
    "        else:\n",
    "            founds = np.argwhere(vote == 0).reshape(-1)\n",
    "\n",
    "            if len(founds) == 1: # (1,0,1)\n",
    "                y_hat[k] = founds[0]+1\n",
    "            else: # (1,0,0)\n",
    "                minor_value = -1\n",
    "                minor_index = -1\n",
    "\n",
    "                for f in founds:\n",
    "                    if minor_index == -1:\n",
    "                        minor_value = mu_dist[k,f]\n",
    "                        minor_index = f+1\n",
    "                    elif mu_dist[k,f] < minor_value:\n",
    "                        minor_value = mu_dist[k,f]\n",
    "                        minor_index = f+1\n",
    "\n",
    "                y_hat[k] = minor_index\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def generate_y_hat(probas, threshold):\n",
    "    y_hat = []\n",
    "\n",
    "    for p in probas:\n",
    "        if (np.max(p) < threshold): #indecisao\n",
    "            y_hat.append(-1)\n",
    "        else: #certeza\n",
    "            y_hat.append(np.argmax(p))\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "knn_p = {\n",
    "    'n_neighbors': [5], # def 5\n",
    "    'leaf_size': [30], # def 30\n",
    "    'window_size': [1000], # def 1000\n",
    "    'p': [2] # 1 Minkowski | def 2 euclidean\n",
    "}\n",
    "\n",
    "phis = [.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200*500*2*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "log = None\n",
    "if FLUSH_FILE:\n",
    "    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n",
    "\n",
    "samples = 8000\n",
    "samples_test = 400\n",
    "samples_retrain = 800\n",
    "\n",
    "datas_test = [data_ds3_fault1[:samples_test], data_ds3_fault2[:samples_test], data_ds3_fault3[:samples_test], data_ds3_fault4[:samples_test], data_ds3_t1_normal[:samples_test], data_ds3_t2_normal[:samples_test]]\n",
    "\n",
    "p = {'max_window_size': [2000],\n",
    "     'phis': [1],\n",
    "     'neighbors_test': [2],\n",
    "     'n_neighbors': [5],\n",
    "     'weighting': ['distance'],\n",
    "     'stm_size_option': ['maxACC']}\n",
    "\n",
    "p = {'max_window_size': [2000],\n",
    "     'phis': [1,2,5,10,20],\n",
    "     'neighbors_test': [2,5],\n",
    "     'n_neighbors': [2, 5],\n",
    "     'weighting': ['distance', 'uniform'],\n",
    "     'stm_size_option': ['maxACC', 'maxACCApprox']}\n",
    "\n",
    "for mws in p['max_window_size']:\n",
    "    for n in p['n_neighbors']:\n",
    "        for w in p['weighting']:\n",
    "            for sso in p['stm_size_option']:\n",
    "\n",
    "                clf = SAMKNNClassifier(max_window_size=mws, n_neighbors=n, weighting=w, stm_size_option=sso) # stm_size_option=None\n",
    "\n",
    "                for nt in p['neighbors_test']:\n",
    "                    for phi in p['phis']:\n",
    "                        print('.', end='')\n",
    "                        print('SAMKNN', mws, phi, nt, n, w, sso, samples, samples_test, samples_retrain, file=log)\n",
    "\n",
    "                        # MAIN TRAIN WITH FAULT 1 , 2\n",
    "                        clf.partial_fit(X_train[:samples], y_train[:samples])\n",
    "                        for data_test in datas_test:\n",
    "                            tester(clf, nt, phi, [1,2], data_test, log)\n",
    "                        print('.', end='')\n",
    "                        # NEW TRAIN WITH FAULT 3\n",
    "                        clf.partial_fit(X_train3[:samples_retrain], y_train3[:samples_retrain])\n",
    "                        for data_test in datas_test:\n",
    "                            tester(clf, nt, phi, [1,2,3], data_test, log)\n",
    "                        print('.', end='')\n",
    "                        # NEW TRAIN WITH FAULT 4\n",
    "                        clf.partial_fit(X_train4[:samples_retrain], y_train4[:samples_retrain])\n",
    "                        for data_test in datas_test:\n",
    "                            tester(clf, nt, phi, [1,2,3,4], data_test, log)\n",
    "                        print('.', end='')\n",
    "\n",
    "if FLUSH_FILE:\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
