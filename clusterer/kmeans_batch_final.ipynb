{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:26:27.101004Z",
     "iopub.status.busy": "2022-02-14T21:26:27.10024Z",
     "iopub.status.idle": "2022-02-14T21:27:07.561108Z",
     "shell.execute_reply": "2022-02-14T21:27:07.560314Z",
     "shell.execute_reply.started": "2022-02-14T21:26:27.100971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import v_measure_score, silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "from sklearn.cluster import  MiniBatchKMeans, Birch, AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn import neighbors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:27:07.592284Z",
     "iopub.status.busy": "2022-02-14T21:27:07.591385Z",
     "iopub.status.idle": "2022-02-14T21:27:07.605392Z",
     "shell.execute_reply": "2022-02-14T21:27:07.60467Z",
     "shell.execute_reply.started": "2022-02-14T21:27:07.592245Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = 500\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = 1\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output_aggclus_final_ds_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '.txt'\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS\n",
    "\n",
    "local : ./outputs/\n",
    "google colab : /content/drive/My Drive/\n",
    "'''\n",
    "if google_colab:\n",
    "    PATH_OUTPUTS = '/content/drive/My Drive/'\n",
    "elif kaggle:    \n",
    "    PATH_OUTPUTS = ''\n",
    "else:\n",
    "    PATH_OUTPUTS = './outputs/'\n",
    "\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../dataset/original/'\n",
    "\n",
    "'''\n",
    "FLUSH FILE\n",
    "\n",
    "If output results file is ON\n",
    "'''\n",
    "FLUSH_FILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:27:07.607053Z",
     "iopub.status.busy": "2022-02-14T21:27:07.606533Z",
     "iopub.status.idle": "2022-02-14T21:28:08.229783Z",
     "shell.execute_reply": "2022-02-14T21:28:08.227823Z",
     "shell.execute_reply.started": "2022-02-14T21:27:07.607023Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    !pip install git+https://github.com/online-ml/river --upgrade\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ \n",
    "    !gdown https://drive.google.com/u/0/uc?id=1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown https://drive.google.com/u/0/uc?id=17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv('F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv('F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv('F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv('F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv('F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv('F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T19:24:58.347375Z",
     "iopub.status.busy": "2022-02-14T19:24:58.346785Z",
     "iopub.status.idle": "2022-02-14T19:24:58.354661Z",
     "shell.execute_reply": "2022-02-14T19:24:58.353565Z",
     "shell.execute_reply.started": "2022-02-14T19:24:58.347334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T19:25:00.926321Z",
     "iopub.status.busy": "2022-02-14T19:25:00.926031Z",
     "iopub.status.idle": "2022-02-14T19:25:01.462014Z",
     "shell.execute_reply": "2022-02-14T19:25:01.461041Z",
     "shell.execute_reply.started": "2022-02-14T19:25:00.926292Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_scenario(scenario, dict_ds, idxs_n, idxs_f):\n",
    "    if scenario == 'n1, n2, f2, f3':\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_normal_t2_original'].iloc[idxs_n, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples,\n",
    "            [0]*samples))\n",
    "    elif scenario == 'f1, f2, f3, f4': # scenario 2\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples,\n",
    "            [4]*samples))\n",
    "    elif scenario == 'n1, f1, f2, f3': # scenario 3\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples))\n",
    "    elif scenario == 'n2, f2, f4': # scenario 4\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t2_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [2]*samples,\n",
    "            [4]*samples))\n",
    "    elif  scenario == 'n1, f1, f2, f4': # scenario 5\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [4]*samples))\n",
    "    else:\n",
    "        print('Scenario not found!')\n",
    "\n",
    "    return data_x, data_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:26:15.059025Z",
     "iopub.status.busy": "2022-02-14T21:26:15.058711Z",
     "iopub.status.idle": "2022-02-14T21:26:15.078842Z",
     "shell.execute_reply": "2022-02-14T21:26:15.077645Z",
     "shell.execute_reply.started": "2022-02-14T21:26:15.058992Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Batch 1 ['n1', 'n2', 'f2', 'f3'] 300 auto\n",
      "[[0.34726511 0.99606134]\n",
      " [0.34310674 0.99212023]\n",
      " [0.34689502 0.99394834]\n",
      " [0.35143707 0.99277404]\n",
      " [0.34688398 0.99439462]\n",
      " [0.35228808 0.99479749]\n",
      " [0.33570812 0.98826859]\n",
      " [0.34148121 0.99527032]\n",
      " [0.34177009 0.98728445]\n",
      " [0.33955342 0.99114903]]\n",
      "Kmeans Batch 2 ['f1', 'f2', 'f3', 'f4'] 300 auto\n",
      "[[0.36386379 0.99478439]\n",
      " [0.36099577 0.99248156]\n",
      " [0.36355513 0.99208414]\n",
      " [0.36577337 0.99336401]\n",
      " [0.36996875 0.99441756]\n",
      " [0.35876431 0.99588683]\n",
      " [0.37451987 0.99640564]\n",
      " [0.36894059 0.99286935]\n",
      " [0.35917357 0.9930695 ]\n",
      " [0.36357638 0.99392378]]\n",
      "Kmeans Batch 3 ['n1', 'f1', 'f2', 'f3'] 300 auto\n",
      "[[0.3570164  0.96682433]\n",
      " [0.36162685 0.9658197 ]\n",
      " [0.35784882 0.97335937]\n",
      " [0.36564368 0.96614888]\n",
      " [0.35326607 0.97466491]\n",
      " [0.35376185 0.96950234]\n",
      " [0.36230272 0.96869577]\n",
      " [0.35435366 0.96291507]\n",
      " [0.35462746 0.96883828]\n",
      " [0.35000799 0.97860457]]\n",
      "Kmeans Batch 4 ['n2', 'f2', 'f4'] 300 auto\n",
      "[[0.35730166 0.99852783]\n",
      " [0.35020225 0.99797047]\n",
      " [0.38041918 0.99768659]\n",
      " [0.36470781 0.99768659]\n",
      " [0.36280073 0.99831752]\n",
      " [0.35486649 0.99852783]\n",
      " [0.36690296 0.99705566]\n",
      " [0.36159208 0.99831752]\n",
      " [0.36941921 0.99617516]\n",
      " [0.36161899 0.99894845]]\n",
      "Kmeans Batch 5 ['n1', 'f1', 'f2', 'f4'] 300 auto\n",
      "[[0.36270452 0.98444125]\n",
      " [0.37602144 0.98112835]\n",
      " [0.37147049 0.98416434]\n",
      " [0.36525215 0.98157135]\n",
      " [0.37247789 0.97583351]\n",
      " [0.36581377 0.98031381]\n",
      " [0.37484486 0.97898829]\n",
      " [0.36825689 0.98142563]\n",
      " [0.35950833 0.97883636]\n",
      " [0.37530415 0.97888013]]\n"
     ]
    }
   ],
   "source": [
    "log = None\n",
    "if FLUSH_FILE:\n",
    "    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n",
    "\n",
    "options = {'n1': [dict_ds['data_ds3_normal_t1_original'], 0, [0]],\n",
    "           'n2': [dict_ds['data_ds3_normal_t2_original'], 0, [0]],\n",
    "           'f1': [dict_ds['data_ds3_fault1_original'], 1, [1]],\n",
    "           'f2': [dict_ds['data_ds3_fault2_original'], 1, [2]],\n",
    "           'f3': [dict_ds['data_ds3_fault3_original'], 1, [3]],\n",
    "           'f4': [dict_ds['data_ds3_fault4_original'], 1, [4]]\n",
    "}\n",
    "\n",
    "samples = 2000\n",
    "n_folds = 10\n",
    "n_clusters = 300\n",
    "alghorithm = 'auto' #def\n",
    "scenarios = [\n",
    "    ['n1', 'n2', 'f2', 'f3'], # scenario 1\n",
    "    ['f1', 'f2', 'f3', 'f4'], # scenario 2\n",
    "    ['n1', 'f1', 'f2', 'f3'], # scenario 3\n",
    "    ['n2', 'f2', 'f4'], # scenario 4\n",
    "    ['n1', 'f1', 'f2', 'f4'], # scenario 5\n",
    "]\n",
    "outputs = np.zeros((n_folds, 2))\n",
    "\n",
    "n = 0\n",
    "for scenario in scenarios:\n",
    "    n = n + 1\n",
    "    print ('Kmeans Batch', n, scenario, n_clusters, alghorithm, file=log)\n",
    "\n",
    "    for fold in np.arange(0, n_folds):\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        clf = KMeans(n_clusters=n_clusters, algorithm=alghorithm)\n",
    "        y_pred = []\n",
    "\n",
    "        # get random indexes for each dataset, but the datasets are ordered\n",
    "        idxs = np.random.randint(0,100000,samples)\n",
    "\n",
    "        data_x = options[scenario[0]][0].iloc[idxs, :]\n",
    "        data_y = options[scenario[0]][2]*samples\n",
    "        for x in np.arange(1,len(scenario)):\n",
    "            data_x = np.concatenate((data_x, options[scenario[x]][0].iloc[idxs, :]))\n",
    "            data_y = np.concatenate((data_y, options[scenario[x]][2]*samples))\n",
    "\n",
    "        data = np.concatenate((data_x, data_y.reshape(-1,1)), axis=1)\n",
    "\n",
    "        for pred in clf.fit_predict(ss.fit_transform(data_x)):\n",
    "            y_pred.append(pred)\n",
    "\n",
    "        if (len(np.unique(y_pred)) == 1):\n",
    "            outputs[fold][0] = -1\n",
    "        else:\n",
    "            outputs[fold][0] = silhouette_score(ss.transform(data_x), y_pred)\n",
    "\n",
    "        outputs[fold][1] = v_measure_score(data_y, y_pred, beta=0) # 0 plus homogeneity | 2 completeness\n",
    "\n",
    "    print(outputs)\n",
    "\n",
    "if FLUSH_FILE:\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n       299])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(clf.labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([('n1', 'n2', 'f4', 'f1'), ('f4', 'f1', 'n2', 'f2'),\n       ('f2', 'f3', 'n2', 'f4')], dtype=object)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 3 scenarios\n",
    "from itertools import permutations\n",
    "\n",
    "sets = []\n",
    "\n",
    "caracteres = ['n1', 'n2', 'f1', 'f2', 'f3', 'f4']\n",
    "for l in np.arange(2,6):\n",
    "    for subset in permutations(caracteres, int(l)):\n",
    "        sets.append(subset)\n",
    "\n",
    "sets.append(tuple(caracteres))\n",
    "\n",
    "sets = np.array(sets)\n",
    "sets[[np.random.randint(0, len(sets), 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.28118271],\n       [0.250944  ],\n       [0.34313435],\n       [0.19724831],\n       [0.2661418 ],\n       [0.03996498],\n       [1.        ],\n       [0.0399065 ],\n       [0.05178145],\n       [0.07086851],\n       [0.00527846],\n       [0.01318267],\n       [0.05070029],\n       [0.        ],\n       [0.03199842],\n       [0.03088117],\n       [0.01071505],\n       [0.055989  ],\n       [0.00241754],\n       [0.02662884]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = MinMaxScaler()\n",
    "ss.fit_transform(np.array([0.0002190936356,\n",
    "                           0.0001962164166,\n",
    "                           0.0002659634117,\n",
    "                           0.000155592714,\n",
    "                           0.0002077143736,\n",
    "                           0.00003659935181,\n",
    "                           0.0007629178845,\n",
    "                           0.00003655510654,\n",
    "                           0.00004553915078,\n",
    "                           0.00005997954617,\n",
    "                           0.00001035711518,\n",
    "                           0.00001633708228,\n",
    "                           0.00004472119254,\n",
    "                           0.000006363675767,\n",
    "                           0.00003057221736,\n",
    "                           0.00002972695524,\n",
    "                           0.00001447019433,\n",
    "                           0.00004872239223,\n",
    "                           0.000008192676174,\n",
    "                           0.00002650983864,]).reshape(-1,1))\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "ss.fit_transform(np.array([0.0002190936356,\n",
    "                           0.0001962164166,\n",
    "                           0.0002659634117,\n",
    "                           0.000155592714,\n",
    "                           0.0002077143736,\n",
    "                           0.00003659935181,\n",
    "                           0.0007629178845,\n",
    "                           0.00003655510654,\n",
    "                           0.00004553915078,\n",
    "                           0.00005997954617,\n",
    "                           0.00001035711518,\n",
    "                           0.00001633708228,\n",
    "                           0.00004472119254,\n",
    "                           0.000006363675767,\n",
    "                           0.00003057221736,\n",
    "                           0.00002972695524,\n",
    "                           0.00001447019433,\n",
    "                           0.00004872239223,\n",
    "                           0.000008192676174,\n",
    "                           0.00002650983864, ]).reshape(-1, 1))\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "ss.fit_transform(np.array([0.0002190936356,\n",
    "                           0.0001962164166,\n",
    "                           0.0002659634117,\n",
    "                           0.000155592714,\n",
    "                           0.0002077143736,\n",
    "                           0.00003659935181,\n",
    "                           0.0007629178845,\n",
    "                           0.00003655510654,\n",
    "                           0.00004553915078,\n",
    "                           0.00005997954617,\n",
    "                           0.00001035711518,\n",
    "                           0.00001633708228,\n",
    "                           0.00004472119254,\n",
    "                           0.000006363675767,\n",
    "                           0.00003057221736,\n",
    "                           0.00002972695524,\n",
    "                           0.00001447019433,\n",
    "                           0.00004872239223,\n",
    "                           0.000008192676174,\n",
    "                           0.00002650983864, ]).reshape(-1, 1))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}