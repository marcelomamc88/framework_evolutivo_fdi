{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:26:27.101004Z",
     "iopub.status.busy": "2022-02-14T21:26:27.10024Z",
     "iopub.status.idle": "2022-02-14T21:27:07.561108Z",
     "shell.execute_reply": "2022-02-14T21:27:07.560314Z",
     "shell.execute_reply.started": "2022-02-14T21:26:27.100971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import v_measure_score, silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:27:07.592284Z",
     "iopub.status.busy": "2022-02-14T21:27:07.591385Z",
     "iopub.status.idle": "2022-02-14T21:27:07.605392Z",
     "shell.execute_reply": "2022-02-14T21:27:07.60467Z",
     "shell.execute_reply.started": "2022-02-14T21:27:07.592245Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = 500\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = 1\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output_aggclus_pca099_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '.txt'\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS\n",
    "\n",
    "local : ./outputs/\n",
    "google colab : /content/drive/My Drive/\n",
    "'''\n",
    "if google_colab:\n",
    "    PATH_OUTPUTS = '/content/drive/My Drive/'\n",
    "elif kaggle:    \n",
    "    PATH_OUTPUTS = ''\n",
    "else:\n",
    "    PATH_OUTPUTS = './outputs/'\n",
    "\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../dataset/original/'\n",
    "\n",
    "'''\n",
    "FLUSH FILE\n",
    "\n",
    "If output results file is ON\n",
    "'''\n",
    "FLUSH_FILE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:27:07.607053Z",
     "iopub.status.busy": "2022-02-14T21:27:07.606533Z",
     "iopub.status.idle": "2022-02-14T21:28:08.229783Z",
     "shell.execute_reply": "2022-02-14T21:28:08.227823Z",
     "shell.execute_reply.started": "2022-02-14T21:27:07.607023Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    !pip install git+https://github.com/online-ml/river --upgrade\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ \n",
    "    !gdown https://drive.google.com/u/0/uc?id=1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown https://drive.google.com/u/0/uc?id=17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown https://drive.google.com/u/0/uc?id=1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv('F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv('F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv('F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv('F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv('F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv('F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T19:24:58.347375Z",
     "iopub.status.busy": "2022-02-14T19:24:58.346785Z",
     "iopub.status.idle": "2022-02-14T19:24:58.354661Z",
     "shell.execute_reply": "2022-02-14T19:24:58.353565Z",
     "shell.execute_reply.started": "2022-02-14T19:24:58.347334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T19:25:00.926321Z",
     "iopub.status.busy": "2022-02-14T19:25:00.926031Z",
     "iopub.status.idle": "2022-02-14T19:25:01.462014Z",
     "shell.execute_reply": "2022-02-14T19:25:01.461041Z",
     "shell.execute_reply.started": "2022-02-14T19:25:00.926292Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_scenario(scenario, dict_ds, idxs_n, idxs_f):\n",
    "    if scenario == 'n1, n2, f2, f3':\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_normal_t2_original'].iloc[idxs_n, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples,\n",
    "            [0]*samples))\n",
    "    elif scenario == 'f1, f2, f3, f4': # scenario 2\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples,\n",
    "            [4]*samples))\n",
    "    elif scenario == 'n1, f1, f2, f3': # scenario 3\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault3_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [3]*samples))\n",
    "    elif scenario == 'n2, f2, f4': # scenario 4\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t2_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [2]*samples,\n",
    "            [4]*samples))\n",
    "    elif  scenario == 'n1, f1, f2, f4': # scenario 5\n",
    "        data_x = np.concatenate((\n",
    "            dict_ds['data_ds3_normal_t1_original'].iloc[idxs_n, :],\n",
    "            dict_ds['data_ds3_fault1_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault2_original'].iloc[idxs_f, :],\n",
    "            dict_ds['data_ds3_fault4_original'].iloc[idxs_f, :]))\n",
    "        data_y = np.concatenate((\n",
    "            [0]*samples,\n",
    "            [1]*samples,\n",
    "            [2]*samples,\n",
    "            [4]*samples))\n",
    "    else:\n",
    "        print('Scenario not found!')\n",
    "\n",
    "    return data_x, data_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:26:15.059025Z",
     "iopub.status.busy": "2022-02-14T21:26:15.058711Z",
     "iopub.status.idle": "2022-02-14T21:26:15.078842Z",
     "shell.execute_reply": "2022-02-14T21:26:15.077645Z",
     "shell.execute_reply.started": "2022-02-14T21:26:15.058992Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 11.010993242263794 seconds\n",
      "3 6.743004083633423 seconds\n",
      "4 4.615996837615967 seconds\n",
      "6 5.913997650146484 seconds\n",
      "7 7.295003652572632 seconds\n",
      "8 2.999999523162842 seconds\n",
      "9 7.307996034622192 seconds\n",
      "10 5.852995872497559 seconds\n",
      "11 5.985999822616577 seconds\n",
      "12 3.7550039291381836 seconds\n",
      "14 6.161994218826294 seconds\n",
      "15 7.760006666183472 seconds\n",
      "16 5.714000940322876 seconds\n",
      "18 12.548999309539795 seconds\n",
      "19 9.00999665260315 seconds\n",
      "20 3.293002128601074 seconds\n",
      "21 6.885000705718994 seconds\n",
      "22 7.860001802444458 seconds\n",
      "23 6.1349992752075195 seconds\n",
      "24 3.8879971504211426 seconds\n",
      "26 7.976028919219971 seconds\n",
      "27 8.785997152328491 seconds\n",
      "28 4.569000244140625 seconds\n",
      "30 6.278999328613281 seconds\n",
      "31 7.040003299713135 seconds\n",
      "32 2.8049983978271484 seconds\n",
      "33 7.095001697540283 seconds\n",
      "34 6.581004858016968 seconds\n",
      "35 6.059997320175171 seconds\n",
      "36 3.436004638671875 seconds\n",
      "38 6.139997959136963 seconds\n",
      "39 6.603002309799194 seconds\n",
      "40 3.428001880645752 seconds\n",
      "42 6.640004873275757 seconds\n",
      "43 5.426004648208618 seconds\n",
      "44 3.4920008182525635 seconds\n",
      "45 6.286994695663452 seconds\n",
      "46 6.917996168136597 seconds\n",
      "47 6.180003643035889 seconds\n",
      "48 3.5259952545166016 seconds\n",
      "2 5.080996513366699 seconds\n",
      "3 5.85599946975708 seconds\n",
      "4 3.3410048484802246 seconds\n",
      "6 5.57499885559082 seconds\n",
      "7 5.136000633239746 seconds\n",
      "8 3.7060012817382812 seconds\n",
      "9 5.512002944946289 seconds\n",
      "10 6.477001905441284 seconds\n",
      "11 5.565009117126465 seconds\n",
      "12 3.607999563217163 seconds\n",
      "14 4.852002859115601 seconds\n",
      "15 6.049000024795532 seconds\n",
      "16 3.236003875732422 seconds\n",
      "18 5.7389960289001465 seconds\n",
      "19 4.740998029708862 seconds\n",
      "20 3.497004747390747 seconds\n",
      "21 5.958003282546997 seconds\n",
      "22 7.046999216079712 seconds\n",
      "23 6.575997591018677 seconds\n",
      "24 3.81099796295166 seconds\n",
      "26 5.527008295059204 seconds\n",
      "27 6.0549986362457275 seconds\n",
      "28 3.6020076274871826 seconds\n",
      "30 5.43499755859375 seconds\n",
      "31 5.4469990730285645 seconds\n",
      "32 3.615999937057495 seconds\n",
      "33 5.528001308441162 seconds\n",
      "34 6.2470011711120605 seconds\n",
      "35 7.152001619338989 seconds\n",
      "36 4.16100001335144 seconds\n",
      "38 5.987996816635132 seconds\n",
      "39 5.680005311965942 seconds\n",
      "40 3.8470029830932617 seconds\n",
      "42 5.824996471405029 seconds\n",
      "43 5.054006814956665 seconds\n",
      "44 3.5620017051696777 seconds\n",
      "45 5.494001388549805 seconds\n",
      "46 6.444004535675049 seconds\n",
      "47 5.585996389389038 seconds\n",
      "48 3.845006227493286 seconds\n",
      "2 4.9709954261779785 seconds\n",
      "3 5.761001110076904 seconds\n",
      "4 3.244002342224121 seconds\n",
      "6 5.8630006313323975 seconds\n",
      "7 4.6310038566589355 seconds\n",
      "8 3.7770047187805176 seconds\n",
      "9 5.294000625610352 seconds\n",
      "10 6.0630035400390625 seconds\n",
      "11 4.863999128341675 seconds\n",
      "12 3.800006628036499 seconds\n",
      "14 5.14899754524231 seconds\n",
      "15 5.586006164550781 seconds\n",
      "16 3.5709996223449707 seconds\n",
      "18 4.679988622665405 seconds\n",
      "19 5.4360034465789795 seconds\n",
      "20 3.1469993591308594 seconds\n",
      "21 6.091996669769287 seconds\n",
      "22 5.152997970581055 seconds\n",
      "23 5.753997325897217 seconds\n",
      "24 3.1040070056915283 seconds\n",
      "26 5.912004470825195 seconds\n",
      "27 5.091001510620117 seconds\n",
      "28 4.026000738143921 seconds\n",
      "30 4.31699800491333 seconds\n",
      "31 6.019002914428711 seconds\n",
      "32 2.8580009937286377 seconds\n",
      "33 6.157001495361328 seconds\n",
      "34 5.207000494003296 seconds\n",
      "35 6.291997909545898 seconds\n",
      "36 2.9160022735595703 seconds\n",
      "38 5.692003011703491 seconds\n",
      "39 5.020002365112305 seconds\n",
      "40 3.721001148223877 seconds\n",
      "42 4.906002521514893 seconds\n",
      "43 5.281999826431274 seconds\n",
      "44 3.426002025604248 seconds\n",
      "45 5.164006233215332 seconds\n",
      "46 6.029007196426392 seconds\n",
      "47 6.39599347114563 seconds\n",
      "48 3.690004825592041 seconds\n",
      "2 2.497997999191284 seconds\n",
      "3 2.9520044326782227 seconds\n",
      "4 2.3660004138946533 seconds\n",
      "6 2.5519967079162598 seconds\n",
      "7 2.413001298904419 seconds\n",
      "8 1.7740018367767334 seconds\n",
      "9 3.9370007514953613 seconds\n",
      "10 2.9070045948028564 seconds\n",
      "11 3.148996591567993 seconds\n",
      "12 2.391993761062622 seconds\n",
      "14 3.4590022563934326 seconds\n",
      "15 2.3299996852874756 seconds\n",
      "16 1.869999885559082 seconds\n",
      "18 2.96899676322937 seconds\n",
      "19 2.8749992847442627 seconds\n",
      "20 1.7269999980926514 seconds\n",
      "21 2.9309966564178467 seconds\n",
      "22 3.8230040073394775 seconds\n",
      "23 3.134998321533203 seconds\n",
      "24 1.6159992218017578 seconds\n",
      "26 2.5129988193511963 seconds\n",
      "27 3.784003496170044 seconds\n",
      "28 2.619999647140503 seconds\n",
      "30 2.6860032081604004 seconds\n",
      "31 4.05799674987793 seconds\n",
      "32 2.3280045986175537 seconds\n",
      "33 2.9270009994506836 seconds\n",
      "34 2.9659957885742188 seconds\n",
      "35 3.6909945011138916 seconds\n",
      "36 1.8430006504058838 seconds\n",
      "38 2.286005973815918 seconds\n",
      "39 2.4280056953430176 seconds\n",
      "40 2.3950014114379883 seconds\n",
      "42 2.7979962825775146 seconds\n",
      "43 2.225998640060425 seconds\n",
      "44 1.9710071086883545 seconds\n",
      "45 4.806000471115112 seconds\n",
      "46 3.6870033740997314 seconds\n",
      "47 3.875004291534424 seconds\n",
      "48 2.5320029258728027 seconds\n",
      "2 4.905992269515991 seconds\n",
      "3 5.531002998352051 seconds\n",
      "4 3.5669994354248047 seconds\n",
      "6 4.655990362167358 seconds\n",
      "7 5.838003396987915 seconds\n",
      "8 2.848994255065918 seconds\n",
      "9 6.341002941131592 seconds\n",
      "10 4.675995588302612 seconds\n",
      "11 5.8629982471466064 seconds\n",
      "12 2.9031171798706055 seconds\n",
      "14 5.607002019882202 seconds\n",
      "15 4.7000041007995605 seconds\n",
      "16 3.957005023956299 seconds\n",
      "18 4.581998825073242 seconds\n",
      "19 6.012999057769775 seconds\n",
      "20 2.9070050716400146 seconds\n",
      "21 5.490996837615967 seconds\n",
      "22 5.1040050983428955 seconds\n",
      "23 5.533997058868408 seconds\n",
      "24 3.3909974098205566 seconds\n",
      "26 4.350340366363525 seconds\n",
      "27 5.668998956680298 seconds\n",
      "28 3.1079983711242676 seconds\n",
      "30 5.37800145149231 seconds\n",
      "31 4.58600378036499 seconds\n",
      "32 3.541940927505493 seconds\n",
      "33 5.3999998569488525 seconds\n",
      "34 5.515010833740234 seconds\n",
      "35 4.998000144958496 seconds\n",
      "36 3.414997100830078 seconds\n",
      "38 5.0100016593933105 seconds\n",
      "39 5.086224317550659 seconds\n",
      "40 3.8550028800964355 seconds\n",
      "42 4.374996185302734 seconds\n",
      "43 5.852002859115601 seconds\n",
      "44 2.8069965839385986 seconds\n",
      "45 6.1740052700042725 seconds\n",
      "46 5.123997211456299 seconds\n",
      "47 7.214994430541992 seconds\n",
      "48 3.074007987976074 seconds\n"
     ]
    }
   ],
   "source": [
    "log = None\n",
    "if FLUSH_FILE:\n",
    "    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n",
    "\n",
    "samples = 2000\n",
    "\n",
    "scenarios = ['n1, n2, f2, f3',\n",
    "             'f1, f2, f3, f4',\n",
    "             'n1, f1, f2, f3',\n",
    "             'n2, f2, f4',\n",
    "             'n1, f1, f2, f4'\n",
    "             ]\n",
    "\n",
    "p = {\n",
    "    'n_clusters': [25, 50, 100, 300],\n",
    "    'affinity': ['cosine', 'manhattan', 'euclidean'],\n",
    "    'linkage': ['ward', 'complete', 'average', 'single'],\n",
    "}\n",
    "\n",
    "fold = 1\n",
    "#scenario = 0 # 0 - 4\n",
    "\n",
    "idxs_n = np.random.randint(0,180000,samples)\n",
    "idxs_f = np.random.randint(0,100000,samples)\n",
    "\n",
    "for scenario in np.arange(0, len(scenarios)):\n",
    "    n = 0\n",
    "    data_x, data_y = get_scenario(scenarios[scenario], dict_ds, idxs_n, idxs_f)\n",
    "    data = np.concatenate((data_x, data_y.reshape(-1,1)), axis=1)\n",
    "\n",
    "    for nc in p['n_clusters']:\n",
    "        for a in p['affinity']:\n",
    "            for l in p['linkage']:\n",
    "                n = n+1\n",
    "\n",
    "                if l == 'ward' and a != 'euclidean':\n",
    "                    continue\n",
    "\n",
    "                ini = time.time()\n",
    "                print(n, end=' ')\n",
    "                print ('Agglomerative Clustering', n, scenarios[scenario], nc, a, l, file=log)\n",
    "\n",
    "                ss = StandardScaler()\n",
    "                clf = AgglomerativeClustering(n_clusters=nc, affinity=a, linkage=l)\n",
    "                y_pred = []\n",
    "\n",
    "                x_nd = data[:,:-1]\n",
    "\n",
    "                pca = PCA(.99)\n",
    "                x_nd = pca.fit_transform(x_nd)\n",
    "\n",
    "                for pred in clf.fit_predict(ss.fit_transform(x_nd)):\n",
    "                    y_pred.append(pred)\n",
    "\n",
    "                print(silhouette_score(ss.transform(x_nd), y_pred), file=log)\n",
    "                print(v_measure_score(data_y, y_pred, beta=0), file=log) # 0 plus homogeneity | 2 completeness\n",
    "\n",
    "                fim = time.time()\n",
    "                print(fim-ini, 'seconds')\n",
    "\n",
    "if FLUSH_FILE:\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-4.05579517e+06, -1.59708592e+05, -2.26837633e+06, ...,\n         1.00678157e+05,  1.22457007e+04, -2.62862924e+03],\n       [-3.67162502e+06, -3.02659208e+04, -2.11680793e+06, ...,\n         8.31597601e+04, -1.80985137e+04, -3.82459698e+03],\n       [-8.50461668e+05,  8.96838245e+05, -1.81391353e+06, ...,\n        -5.88350403e+03, -1.05005285e+05,  4.67521176e+03],\n       ...,\n       [ 1.16320108e+07, -1.19363726e+06,  7.29425047e+04, ...,\n        -3.27189096e+04, -8.77522092e+03,  1.79719056e+04],\n       [ 1.16144586e+07, -1.19782733e+06,  1.76473965e+05, ...,\n         1.68142617e+04,  4.46586247e+03,  3.99942780e+03],\n       [ 1.16134685e+07, -1.19910395e+06,  1.84158370e+05, ...,\n         2.09314558e+04,  5.41472705e+03,  3.46104417e+03]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "\n",
    "x_nd = pca.fit_transform(data[:,:-1])\n",
    "\n",
    "x_nd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}