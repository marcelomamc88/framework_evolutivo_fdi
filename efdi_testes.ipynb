{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> EVOLUTIVE FAULT DETECTION AND ISOLATION (EFDI)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from river.utils import numpy2dict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from river import datasets\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "from river import linear_model, metrics, multiclass, preprocessing\n",
    "from river import ensemble\n",
    "from river import neighbors\n",
    "from river import tree\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o CSV para DataFrame\n",
    "dataset_original = pd.read_csv(datasets.ImageSegments().path)\n",
    "\n",
    "#Realizando o LabelEncoder da variável dependente\n",
    "dataset_original['categorical_label'] = LabelEncoder().fit_transform(dataset_original['category'])\n",
    "\n",
    "# Dados != PATH\n",
    "df_train = dataset_original[dataset_original['category'] != 'path']\n",
    "\n",
    "# Dados == PATH\n",
    "category_path = dataset_original[dataset_original['category'] == 'path']\n",
    "\n",
    "Y_train_simbolic = df_train.pop('category')\n",
    "Y_train_numeric = df_train.pop('categorical_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>METODOLOGIA\n",
    "\n",
    "### DEFINIR DATASETS\n",
    "\n",
    "S1 = {brickface, cement, foliage, grass, sky, window}\n",
    "\n",
    "S1_1 = 70% S1 (de cada classe)\n",
    "\n",
    "S1_2 = 30% S1 (de cada classe)\n",
    "\n",
    "S2 = {path}\n",
    "\n",
    "S2_1 = 30% S2\n",
    "\n",
    "S2_2 = 70% S2\n",
    "\n",
    "S3 = {S1_2 + S2_2)\n",
    "\n",
    "### APLICAR METODOLOGIA\n",
    "\n",
    "1. train S1_1\n",
    "\n",
    "2. predict S2_1\n",
    "\n",
    "3. cluster S2_1\n",
    "\n",
    "4. train S2_1\n",
    "\n",
    "5. validar S3 => métricas acurácia\n",
    "\n",
    "6. Anomaly Detection (detector phase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## preparando dados"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAS AS AMOSTRAS (EXCETO PATH)\n",
    "S1 = dataset_original[dataset_original['category'] != 'path']\n",
    "\n",
    "#TRAIN / TEST ESTRATIFICADO\n",
    "X_train, X_test, y_train, y_test = train_test_split(S1.iloc[:,:-2], S1.iloc[:, -1], stratify=S1.iloc[:, -1], test_size=0.3)\n",
    "\n",
    "#70% CADA CLASSE (EXCETO PATH)\n",
    "S1_1 = pd.concat([X_train,y_train], axis=1)\n",
    "S1_1_x = S1_1.iloc[:, :-1]\n",
    "S1_1_y = S1_1.iloc[:, -1]\n",
    "\n",
    "#30% CADA CLASSE (EXCETO PATH)\n",
    "S1_2 = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "#PATH\n",
    "S2 = dataset_original[dataset_original['category'] == 'path']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(S2.iloc[:,:-2], S2.iloc[:, -1], test_size=0.7)\n",
    "\n",
    "#30% DE PATH\n",
    "S2_1 = pd.concat([X_train,y_train], axis=1)\n",
    "S2_1_x = S2_1.iloc[:, :-1]\n",
    "\n",
    "#70% DE PATH\n",
    "S2_2 = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "#PATH\n",
    "S3 = pd.concat([S2_1, S2_2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center> CLASSIFIER PHASE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### RIVER KNN ADWIN_CLASSIFIER\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "classifier = ensemble.BaggingClassifier(\n",
    "    model=(\n",
    "            preprocessing.StandardScaler() |\n",
    "            neighbors.KNNADWINClassifier()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Treinando o classificador com as classes conhecidas\n",
    "for k, x in S1_1_x.iterrows():\n",
    "    classifier.predict_one(numpy2dict(x.to_numpy()))\n",
    "    classifier = classifier.learn_one(numpy2dict(x.to_numpy()), int(S1_1_y.loc[k]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as know 2.02% : as unknown 97.98%\n"
     ]
    }
   ],
   "source": [
    "# 70% dos dados da classe 4 (path) foram classificadas, e devem ser marcadas como unknown\n",
    "# nova categoria\n",
    "# selecionar com THRESHOLD 0.95\n",
    "# quantidade UNK -> K (erro)\n",
    "# quantidade UNK -> UNK (acerto)\n",
    "# enviar UNK -> UNK para Cluster\n",
    "\n",
    "threshold = 0.95\n",
    "classified_as_known = 0\n",
    "DATASET_3 = []\n",
    "\n",
    "#new_class = S2.iloc[:, :-2]\n",
    "new_class = S2_1_x\n",
    "\n",
    "# Testando o classificador com a nova classe (PATH)\n",
    "for idx, x in new_class.iterrows():\n",
    "    y_pred_proba = classifier.predict_proba_one(numpy2dict(x.to_numpy()))\n",
    "\n",
    "    max_proba = max(y_pred_proba.items(), key = lambda k : k[1])\n",
    "    if (max_proba[1] < threshold): #indecisao\n",
    "        DATASET_3.append(idx)\n",
    "    else: #certeza\n",
    "        classified_as_known = classified_as_known + 1\n",
    "\n",
    "\n",
    "print('as know {:.2f}% : as unknown {:.2f}%'.format(classified_as_known/new_class.shape[0]*100, len(DATASET_3)/new_class.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### SCIKIT_MULTIFLOW KNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.lazy import KNNClassifier\n",
    "\n",
    "classifier_knn_sk_mult = KNNClassifier(n_neighbors=8, max_window_size=2000, leaf_size=40)\n",
    "\n",
    "# Treinando o classificador com as classes conhecidas\n",
    "for k, x in S1_1_x.iterrows():\n",
    "    classifier_knn_sk_mult = classifier_knn_sk_mult.partial_fit(x.to_numpy().reshape(1, x.shape[0]), [int(S1_1_y.loc[k])])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_1_DATASET_3 = S2_1_x.loc[DATASET_3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> EVOLVER PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cluster classe 4 (path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# precisa ser INCREMENTAL PCA\n",
    "# precisa ser INCREMENTAL StandardScaler\n",
    "\n",
    "pca = PCA(.95)\n",
    "\n",
    "S2_1_cluster_pca = pd.DataFrame(pca.fit_transform(S2_1_DATASET_3).tolist())\n",
    "\n",
    "S2_1_cluster_pca_ss = StandardScaler().fit_transform(S2_1_cluster_pca.to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Incremental DBSTREAM"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from river import cluster\n",
    "\n",
    "y_pred_dbstream = []\n",
    "dbstream = cluster.DBSTREAM(clustering_threshold = 2)\n",
    "\n",
    "for x in S2_1_cluster_pca_ss:\n",
    "    dbstream.learn_one(numpy2dict(x))\n",
    "    y_pred_dbstream.append(dbstream.predict_one(numpy2dict(x)))\n",
    "\n",
    "res =  [(el, y_pred_dbstream.count(el)) for el in y_pred_dbstream]\n",
    "y_pred_dbstream_grouped = list(OrderedDict(res).items())\n",
    "print(dbstream.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### DenStream"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "denstream = cluster.DenStream()\n",
    "\n",
    "y_pred_denstream = []\n",
    "for x in S2_1_cluster_pca_ss:\n",
    "    denstream.learn_one(numpy2dict(x))\n",
    "\n",
    "for x in S2_1_cluster_pca_ss:\n",
    "    y_pred_denstream.append(denstream.predict_one(numpy2dict(x)))\n",
    "\n",
    "print(denstream.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### DBSCAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd Labels DBSCAN offline: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(metric='manhattan').fit(S2_1_cluster_pca_ss)\n",
    "\n",
    "dbscan_labels_list = dbscan.labels_.reshape(1, dbscan.labels_.shape[0]).tolist()[0]\n",
    "\n",
    "res =  [(el, dbscan_labels_list.count(el)) for el in dbscan_labels_list]\n",
    "y_pred_dbscan_grouped = list(OrderedDict(res).items())\n",
    "\n",
    "print('Qtd Labels DBSCAN offline: ' + str(len(np.unique(dbscan.labels_))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TEDA Cloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "#### CLOUD ####\n",
    "class Cloud:\n",
    "\n",
    "    def __init__(self, sample=[], name='Default Class'):\n",
    "        self.name = name\n",
    "        self.var = 0\n",
    "        self.n = 0\n",
    "        self.covmat = []\n",
    "\n",
    "        if (len(sample) > 0):\n",
    "            self.mean = sample\n",
    "            self.n = 1\n",
    "            self.covmat = np.zeros((sample.shape[0], sample.shape[0]))\n",
    "\n",
    "    def updateCloud(self, mu=[], var=0, n=0, covmat=[]):\n",
    "        self.mean = mu\n",
    "        self.var = var\n",
    "        self.n = n\n",
    "        self.covmat = covmat\n",
    "\n",
    "    def addPoint(self, sample=[]):\n",
    "        if (self.n == 0):\n",
    "            self.n = 1\n",
    "            self.mean = sample\n",
    "            self.var = 0\n",
    "            self.covmat = np.zeros((self.mean.shape[0], self.mean.shape[0]))\n",
    "        else:\n",
    "            self.n = self.n + 1\n",
    "            self.mean = self.calculate_mean(sample, self.n)\n",
    "            self.var = self.calculate_variance(sample, self.n, self.mean)\n",
    "            self.covmat = self.calculate_variance_matrix(sample, self.n, self.mean)\n",
    "\n",
    "\n",
    "    def calculateZeta(self, sample=[], similarity_measure='euclidean'):\n",
    "        if self.n == 0:\n",
    "            zeta = math.inf\n",
    "            return zeta\n",
    "\n",
    "        n_ = self.n + 1\n",
    "        mean_ = self.calculate_mean(sample, n_)\n",
    "        var_ = self.calculate_variance(sample, n_, mean_)\n",
    "        covmat_ = self.calculate_variance_matrix(sample, n_, mean_)\n",
    "\n",
    "\n",
    "        if (similarity_measure.lower() == 'euclidean'):\n",
    "            ksi = np.maximum(self.calculate_eccentricity(sample, self.n, mean_, var_), 0.0001)\n",
    "        else:\n",
    "            ksi = np.maximum(self.calculate_eccentricity(sample, n_, mean_, var_, covmat_, 'mahalanobis'), 0.0001)\n",
    "\n",
    "        zeta = ksi / 2\n",
    "        return zeta\n",
    "\n",
    "    def calculate_mean(self, sample, n):\n",
    "        return ((n - 1) / n) * self.mean + (1 / n) * sample\n",
    "\n",
    "    def calculate_variance(self, sample, n, mean):\n",
    "        return ((n - 1) / n) * self.var + (1 / (n)) * LA.norm(sample - mean) ** 2\n",
    "\n",
    "    def calculate_variance_matrix(self, sample, n, mean):\n",
    "        a = (n - 1) / n\n",
    "        b = a * self.covmat\n",
    "        c = [sample - mean]\n",
    "        d = np.transpose(c)\n",
    "        e = (1 / n) * d    ##TALVEZ SEJA 1 / (n-1)\n",
    "\n",
    "        f = b + e * c\n",
    "\n",
    "        return f\n",
    "\n",
    "    def calculate_eccentricity(self, sample, n, mean, variance, covmat = [], similarity_measure = 'euclidean'):\n",
    "\n",
    "        a = 1 / n\n",
    "\n",
    "        if (similarity_measure == 'euclidean'):\n",
    "            b = [mean - sample]\n",
    "            c = np.transpose(b)\n",
    "            d = n * variance\n",
    "            e = np.dot(b, c)\n",
    "            f = e / d\n",
    "\n",
    "            r = distance.euclidean(sample, mean)\n",
    "\n",
    "            ksi = a + f\n",
    "        else: # mahalanobis_formula => (sample - mean).T * covmat*-1 * (sample - mean)\n",
    "\n",
    "            d_mahalanobis = np.zeros((1,1), dtype=float) + distance.mahalanobis(mean, sample, LA.pinv(covmat))**2\n",
    "\n",
    "            h = n * len(mean) #nao entendi para que isso - parece a ponderacao do n elementos pela qtd de dimensoes\n",
    "            m = d_mahalanobis / h\n",
    "\n",
    "            ksi = a + m\n",
    "\n",
    "        return ksi\n",
    "\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "################################ AUTOCLOUD ################################\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "class AutoCloud:\n",
    "\n",
    "    def __init__(self, m = 2, similarity_measure = 'euclidean', auto_merge = True, display = False):\n",
    "        self.k = 0\n",
    "        self.cloudList = []\n",
    "        self.predictions = {}\n",
    "        self.predictions_by_sample = {}\n",
    "        self.initialize_intersection_list()\n",
    "        self.initialize_intersection_matrix()\n",
    "        self.contMerge = 0\n",
    "        self.dimension = 0\n",
    "        self.similarityMeasure = similarity_measure\n",
    "        self.autoMerge = auto_merge\n",
    "        self.m = m\n",
    "        self.display = display\n",
    "\n",
    "    def run(self, sample = [], label = None):\n",
    "\n",
    "        if (self.dimension == 0):\n",
    "            self.dimension = sample.shape[0]\n",
    "        else:\n",
    "            if (self.dimension != sample.shape[0]):\n",
    "                raise Exception('The dimension of the current data sample is different from the points read so far.')\n",
    "\n",
    "        if (label != None): #para supervisionado - aqui temos também o Y (label) da amostra\n",
    "\n",
    "            while (len(self.cloudList) < label): #cria a qtd de classes\n",
    "                self.create_cloud(sample)\n",
    "\n",
    "            self.cloudList[label - 1].addPoint(sample)\n",
    "            self.k = self.k + 1\n",
    "\n",
    "            self.initialize_intersection_list()\n",
    "            self.membershipList = [0] * len(self.cloudList)\n",
    "            self.membershipList[label - 1] = 1\n",
    "            self.intersectionList[label - 1] = 1\n",
    "            self.initialize_intersection_matrix()\n",
    "\n",
    "            return label, self.membershipList\n",
    "\n",
    "        else:\n",
    "            self.k = self.k + 1\n",
    "            self.initialize_intersection_list()\n",
    "\n",
    "            if (self.k == 1):\n",
    "                self.cloudList.append(Cloud(sample = sample, name = 'Class 1'))\n",
    "                self.initialize_intersection_list()\n",
    "                self.initialize_intersection_matrix()\n",
    "                self.membershipList = [1]\n",
    "            elif (self.k == 2):\n",
    "                self.cloudList[0].addPoint(sample)\n",
    "                self.membershipList = [1]\n",
    "            elif (self.k >= 3):\n",
    "                createCloud = True\n",
    "                tauList = np.zeros((len(self.cloudList), 1))\n",
    "\n",
    "                for i_cloud, cloud in enumerate(self.cloudList):\n",
    "                    zeta = cloud.calculateZeta(sample, self.similarityMeasure) #??eccentricity | ?? norm_eccentricity\n",
    "                    tau = 1 - zeta #??typicality\n",
    "                    tauList[i_cloud] = tau\n",
    "\n",
    "                    if (self.calculate_sample_belongs_to_cloud(zeta, cloud.n)):\n",
    "                        cloud.addPoint(sample)\n",
    "                        self.intersectionList[i_cloud] = 1\n",
    "                        createCloud = False\n",
    "                    else:\n",
    "                        self.intersectionList[i_cloud] = 0\n",
    "\n",
    "                self.membershipList = tauList / sum(tauList)\n",
    "\n",
    "                ### NEW CLOUD ###\n",
    "                if (createCloud == True):\n",
    "                    self.create_cloud(sample)\n",
    "\n",
    "        amax = np.amax(self.membershipList)\n",
    "        where = np.where(self.membershipList == amax)\n",
    "        y_label = where[0][0]+1\n",
    "\n",
    "        if (self.autoMerge):\n",
    "            self.mergeClouds()\n",
    "\n",
    "        return y_label, self.membershipList\n",
    "\n",
    "    def mergeClouds(self):\n",
    "        i = 0\n",
    "\n",
    "        i_end = len(self.cloudList) - 1\n",
    "        while (i < i_end):\n",
    "\n",
    "            merge = False\n",
    "\n",
    "            j_end = np.arange(i + 1, len(self.cloudList)).reshape(-1)\n",
    "            for j in j_end:\n",
    "                if (self.intersectionList[i] == 1 and self.intersectionList[j] == 1):\n",
    "                    self.intersectionMatrix[i,j] = self.intersectionMatrix[i,j] + 1\n",
    "\n",
    "                ### recover information about clouds to be merged ###\n",
    "                n_i = self.cloudList[i].n\n",
    "                n_j = self.cloudList[j].n\n",
    "                mean_i = self.cloudList[i].mean\n",
    "                mean_j = self.cloudList[j].mean\n",
    "                var_i = self.cloudList[i].var\n",
    "                var_j = self.cloudList[j].var\n",
    "                covmat_i = self.cloudList[i].covmat\n",
    "                covmat_j = self.cloudList[j].covmat\n",
    "                nint = self.intersectionMatrix[i,j]\n",
    "\n",
    "                if (nint > (n_i - nint) or nint > (n_j - nint)):\n",
    "                    ### merge\n",
    "                    if (self.display):\n",
    "                        print('Merging clouds ' + str(i+1) + ' and ' + str(j+1) + ' at instant ' + str(self.k))\n",
    "                    ### calculate state of new cloud\n",
    "                    n = n_i + n_j - nint\n",
    "                    mean = ((n_i * mean_i) + (n_j * mean_j)) / (n_i + n_j)\n",
    "                    var = ((n_i - 1) * var_i + (n_j - 1) * var_j) / (n_i + n_j - 2)\n",
    "                    covmat = ((n_i - 1) * covmat_i + (n_j - 1) * covmat_j) / (n_i + n_j - 2)\n",
    "                    ### create new cloud cloud ###\n",
    "                    newCloud = Cloud()\n",
    "                    newCloud.updateCloud(mean, var, n, covmat)\n",
    "                    newCloud.name = 'Class ' + str(i+1) + '/' + str(j+1)\n",
    "\n",
    "                    ### update intersection list ###\n",
    "                    v_il1 = self.intersectionList[0: i]\n",
    "                    v_il2 = np.array([1])\n",
    "                    v_il3 = self.intersectionList[i + 1: j]\n",
    "                    v_il4 = self.intersectionList[j + 1: np.size(self.intersectionList)]\n",
    "                    self.intersectionList = np.concatenate((v_il1, v_il2, v_il3, v_il4), axis=None)\n",
    "\n",
    "                    ## update cloud list ###\n",
    "                    if (self.display):\n",
    "                        print(\"Cloud (label) : \" + str(i))\n",
    "                    v_c1 = self.cloudList[0: i]\n",
    "                    v_c2 = np.array([newCloud])\n",
    "                    v_c3 = self.cloudList[i + 1: j]\n",
    "                    v_c4 = self.cloudList[j + 1: np.size(self.cloudList)]\n",
    "                    self.cloudList = np.concatenate((v_c1, v_c2, v_c3, v_c4), axis=None)\n",
    "\n",
    "                    ### update intersection matrix ###\n",
    "                    A = self.intersectionMatrix\n",
    "\n",
    "                    #remover linha\n",
    "                    vb1_0 = A[0: i, :]\n",
    "                    vb2 = np.zeros((1, len(A)))\n",
    "                    vb3 = A[i + 1: j, :]\n",
    "                    vb4 = A[j + 1: len(A), :]\n",
    "                    B = np.concatenate(([vb1_0, vb2, vb3, vb4]))\n",
    "\n",
    "                    #remover coluna\n",
    "                    vb1_1 = B[:, 0: i]\n",
    "                    vb2 = np.zeros((len(B), 1))\n",
    "                    vb3 = B[:, i + 1: j]\n",
    "                    vb4 = B[:, j + 1: len(A)] ### acho que deveria ser LEN de B\n",
    "                    B = np.concatenate(([vb1_1, vb2, vb3, vb4]), axis=1)\n",
    "\n",
    "                    # calc nova coluna\n",
    "                    col = (A[:, i] + A[:, j]) * (A[:, i] * A[:, j] != 0)\n",
    "                    C = np.concatenate((col[0: j], col[j + 1: np.size(col)]))\n",
    "\n",
    "                    # calc nova linha\n",
    "                    lin = (A[i, :] + A[j, :]) * (A[i, :] * A[j, :] != 0)\n",
    "                    L = np.concatenate((lin[0: j], lin[j + 1: np.size(lin)]))\n",
    "\n",
    "                    #atualizar coluna\n",
    "                    B[:,i] = C\n",
    "\n",
    "                    #atualizar linha\n",
    "                    B[i,:] = L\n",
    "\n",
    "                    vb1_2 = A[[i], i+1 : j]\n",
    "                    vb2 = A[i+1 : j, [j]]\n",
    "                    vb3 = np.transpose(vb2)\n",
    "\n",
    "                    B[[i], i+1 : j] = vb1_2 + vb3\n",
    "\n",
    "                    self.intersectionMatrix = B\n",
    "\n",
    "                    merge = True\n",
    "                    self.contMerge = self.contMerge + 1\n",
    "                    break\n",
    "\n",
    "            if (merge == True):\n",
    "                i = 1\n",
    "            else:\n",
    "                i = i + 1\n",
    "\n",
    "\n",
    "    def initialize_intersection_matrix(self):\n",
    "        if (len(self.cloudList) == 0):\n",
    "            self.intersectionMatrix = np.zeros((1, 1))\n",
    "        else:\n",
    "            self.intersectionMatrix = np.zeros((len(self.cloudList), len(self.cloudList)))\n",
    "\n",
    "    def initialize_intersection_list(self):\n",
    "        if (len(self.cloudList) == 0):\n",
    "            self.intersectionList = []\n",
    "        else:\n",
    "            self.intersectionList = [0] * len(self.cloudList)\n",
    "\n",
    "\n",
    "    def calculateThreshold(self, s=None):\n",
    "        if (self.similarityMeasure.lower() == 'euclidean'):\n",
    "            th = (self.m ** 2 + 1) / (2 * (s))\n",
    "        else:\n",
    "            if (self.similarityMeasure.lower() == 'mahalanobis'):\n",
    "                th = (self.m ** 2 + self.dimension) / (2 * (s) * self.dimension)\n",
    "\n",
    "        threshold = th\n",
    "        return threshold\n",
    "\n",
    "    def create_cloud(self, sample):\n",
    "        cloud_number = len(self.cloudList) + 1\n",
    "        if (self.display):\n",
    "            print('Creating cloud ' + str(cloud_number) + ' at instant ' + str(self.k))\n",
    "\n",
    "        self.cloudList = np.append(self.cloudList, [Cloud(sample, 'Class ' + str(cloud_number))], axis=0)\n",
    "        self.intersectionList.append(1)\n",
    "        self.expand_intersection_matrix_with_zeros()\n",
    "\n",
    "    def calculate_sample_belongs_to_cloud(self, zeta, cloud_n):\n",
    "        return (zeta < math.inf and zeta <= self.calculateThreshold(cloud_n))\n",
    "\n",
    "    def expand_intersection_matrix_with_zeros(self):\n",
    "        self.intersectionMatrix = np.pad(self.intersectionMatrix, ((0, 1), (0, 1)), mode='constant', constant_values=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "clusterer = AutoCloud(2, 'mahalanobis')\n",
    "\n",
    "output = [0] * (S2_1_cluster_pca_ss.shape[0])\n",
    "\n",
    "for k, sample in pd.DataFrame(S2_1_cluster_pca_ss).iterrows():\n",
    "    y, _ = clusterer.run(sample)\n",
    "    output.append(y)\n",
    "\n",
    "print(len(np.unique(output)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cluster class 1 , 4 (foliage, path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "pca = PCA(.95)\n",
    "\n",
    "S2_1_4_cluster_pca = pd.DataFrame(pca.fit_transform(pd.concat([S2_1_DATASET_3,S1_1[S1_1['categorical_label'] == 1].iloc[0:94, :-1]])).tolist())\n",
    "\n",
    "S2_1_4_cluster_pca_ss = StandardScaler().fit_transform(S2_1_4_cluster_pca.to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from river import cluster\n",
    "\n",
    "y_pred_dbstream = []\n",
    "dbstream_1_4 = cluster.DBSTREAM(clustering_threshold = 2)\n",
    "\n",
    "for x in S2_1_4_cluster_pca_ss:\n",
    "    dbstream_1_4.learn_one(numpy2dict(x))\n",
    "    y_pred_dbstream.append(dbstream_1_4.predict_one(numpy2dict(x)))\n",
    "\n",
    "print(dbstream_1_4.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "denstream_1_4 = cluster.DenStream()\n",
    "\n",
    "y_pred_denstream = []\n",
    "for x in S2_1_4_cluster_pca_ss:\n",
    "    denstream_1_4.learn_one(numpy2dict(x))\n",
    "\n",
    "for x in S2_1_4_cluster_pca_ss:\n",
    "    y_pred_denstream.append(denstream_1_4.predict_one(numpy2dict(x)))\n",
    "\n",
    "print(denstream_1_4.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd Labels DBSCAN offline: 4\n",
      "[-1  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0\n",
      "  0  0  0 -1  0 -1  0  0  0  0  0  0 -1 -1 -1  0 -1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 -1  0  0 -1  0  0  0 -1  0  0 -1  0  0  0  0 -1 -1  0\n",
      " -1  0 -1  0  0 -1  0  0 -1  0  0  0  0 -1  0  0  0 -1  0  0  0 -1  0 -1\n",
      "  0  0  2 -1  2  1 -1  1  1  1  1 -1  2 -1  1 -1 -1 -1 -1 -1 -1  1 -1  1\n",
      "  1  1 -1  2 -1  1 -1  2 -1 -1  1  2 -1 -1 -1 -1  1 -1  1  1  1 -1 -1 -1\n",
      " -1  1  2  2 -1  2  1 -1  1 -1  1  2  1  2  2 -1 -1  1 -1 -1  1 -1  1  1\n",
      "  1  2 -1  1 -1  1  1 -1  1  1 -1  1  1  1  1  2 -1  1  2 -1  1  2  2  2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_1_4 = DBSCAN(metric='chebyshev').fit(S2_1_4_cluster_pca_ss)\n",
    "\n",
    "print('Qtd Labels DBSCAN offline: ' + str(len(np.unique(dbscan_1_4.labels_))))\n",
    "print(dbscan_1_4.labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TEDA Cloud"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "autocloud_1_4 = AutoCloud(2, 'mahalanobis')\n",
    "\n",
    "output = [0] * (S2_1_4_cluster_pca_ss.shape[0])\n",
    "\n",
    "for k, sample in pd.DataFrame(S2_1_4_cluster_pca_ss).iterrows():\n",
    "    y, _ = autocloud_1_4.run(sample)\n",
    "    output.append(y)\n",
    "\n",
    "print(len(np.unique(output)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>INCREMENTAL CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### clusterer DBSTREAM -> CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "## COPIEI O CONTEUDO DO ENDPOINT KNOWN E VOU FAZER NA MÃO\n",
    "## POIS NAO TERIA COMO REVERTER O PCA, POIS NÃO TENHO A\n",
    "## REFERENCIA NO ARQUIVO _INIT_.PY\n",
    "\n",
    "slice_factor = 0.2\n",
    "\n",
    "# parameters\n",
    "cluster = 3\n",
    "_class = 35\n",
    "\n",
    "# obter centro cluster 0\n",
    "# obter samples do cluster 0\n",
    "# calcular os __slice_factor__ mais proximos\n",
    "# identificar que sao os samples no vetor original pré PCA e SS (posso utilizar o DB como apoio)\n",
    "# enviar os samples para o CLASSIFICADOR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obter centro do cluster X\n",
    "cluster_center = dbstream.centers[cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from river.utils import dict2numpy\n",
    "import math\n",
    "\n",
    "# pegar as keys dos samples para o cluster selecionado\n",
    "cluster_key_samples = [key for key, value in enumerate(y_pred_dbstream) if value == cluster]\n",
    "\n",
    "# pegar os samples para o cluster selecionado\n",
    "cluster_samples = S2_1_cluster_pca_ss[cluster_key_samples]\n",
    "\n",
    "# calcular distancias para cada sample\n",
    "distances = []\n",
    "for sample in cluster_samples:\n",
    "    distances.append(distance.euclidean(dict2numpy(cluster_center), sample))\n",
    "\n",
    "distances_unsorted = distances.copy()\n",
    "distances.sort()\n",
    "\n",
    "qty_distances_retrieve = math.ceil(len(distances) * slice_factor)\n",
    "distances_retrieve = distances[: qty_distances_retrieve]\n",
    "\n",
    "closest_samples_keys = [idx for idx, element in enumerate(distances_unsorted) if element in distances_retrieve]\n",
    "\n",
    "#x = [cluster_samples[k] for k in closest_samples_keys]\n",
    "\n",
    "#cluster_key_samples[[closest_samples_keys]]\n",
    "cluster_key_samples_closest = [cluster_key_samples[v] for k, v in enumerate(closest_samples_keys)]\n",
    "\n",
    "S2_1_DATASET_3_numpy =  S2_1_DATASET_3.to_numpy()[cluster_key_samples_closest]\n",
    "\n",
    "# treinando INCREMENTALMENTE o classificador\n",
    "\n",
    "for k, x in enumerate(S2_1_DATASET_3_numpy):\n",
    "    classifier.learn_one(numpy2dict(x), _class)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predict (S2_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for k, x in S2_2.iloc[:, :-1].iterrows():\n",
    "    y_pred.append(classifier.predict_one(numpy2dict(x.to_numpy())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc DBSTREAM: 3.463203463203463%\n"
     ]
    }
   ],
   "source": [
    "res =  [(el, y_pred.count(el)) for el in y_pred]\n",
    "predicts_grouped = list(OrderedDict(res).items())\n",
    "\n",
    "for k, x in enumerate(predicts_grouped):\n",
    "    if x[0] == 35:\n",
    "        print('Acc DBSTREAM: ' + str(x[1] / S2_2.shape[0] * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clusterer DBSCAN -> CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "cluster = 1\n",
    "_class = 35\n",
    "\n",
    "cluster_key_samples_match = [k for k, v in enumerate(dbscan.labels_) if v == cluster]\n",
    "\n",
    "S2_1_cluster_send_classifier = S2_1_DATASET_3.iloc[cluster_key_samples_match]\n",
    "\n",
    "# treinando INCREMENTALMENTE o classificador\n",
    "\n",
    "for k, x in S2_1_cluster_send_classifier.iterrows():\n",
    "    classifier.learn_one(numpy2dict(x.to_numpy()), _class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predict (S2_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for k, x in S2_2.iloc[:, :-1].iterrows():\n",
    "    y_pred.append(classifier.predict_one(numpy2dict(x.to_numpy())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc KNN RIVER: 38.52813852813853%\n"
     ]
    }
   ],
   "source": [
    "res =  [(el, y_pred.count(el)) for el in y_pred]\n",
    "predicts_grouped = list(OrderedDict(res).items())\n",
    "\n",
    "for k, x in enumerate(predicts_grouped):\n",
    "    if x[0] == 35:\n",
    "        print('Acc KNN RIVER: ' + str(x[1] / S2_2.shape[0] * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clusterer DBSCAN -> CLASSIFIER KNN SCIKIT_MULTIFLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "cluster = 1\n",
    "_class = 35\n",
    "\n",
    "cluster_key_samples_match = [k for k, v in enumerate(dbscan.labels_) if v == cluster]\n",
    "\n",
    "S2_1_cluster_send_classifier = S2_1_DATASET_3.iloc[cluster_key_samples_match]\n",
    "\n",
    "# treinando INCREMENTALMENTE o classificador\n",
    "\n",
    "for k, x in S2_1_cluster_send_classifier.iterrows():\n",
    "    classifier_knn_sk_mult.partial_fit(x.to_numpy().reshape(1, x.shape[0]), [_class])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predict (S2_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for k, x in S2_2.iloc[:, :-1].iterrows():\n",
    "    y_pred.append(classifier_knn_sk_mult.predict(x.to_numpy().reshape(1,x.shape[0]))[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc KNN SCIKIT MULTILEARN: 48.917748917748916%\n"
     ]
    }
   ],
   "source": [
    "res =  [(el, y_pred.count(el)) for el in y_pred]\n",
    "predicts_grouped = list(OrderedDict(res).items())\n",
    "\n",
    "for k, x in enumerate(predicts_grouped):\n",
    "    if x[0] == 35:\n",
    "        print('Acc KNN SCIKIT MULTILEARN: ' + str(x[1] / S2_2.shape[0] * 100) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center> DETECTOR PHASE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "brickface (0) -> NORMAL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "NORMAL_x_train = S1_1.loc[S1_1['categorical_label'] == 0].iloc[:, :-1]\n",
    "NORMAL_x_test = S1_2.loc[S1_2['categorical_label'] == 0].iloc[:, :-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "from river import anomaly\n",
    "from river import preprocessing\n",
    "from river import compose\n",
    "from sklearn.preprocessing import MinMaxScaler as sk_minmax\n",
    "from river.preprocessing import MinMaxScaler as r_minmax\n",
    "from river.preprocessing import StandardScaler as r_ss\n",
    "from river.preprocessing import RobustScaler as r_rs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HalfSpaceTree RIVER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Identificou 100.00% de amostras NORMAIS corretamente'\n",
      "'Identificou 90.91% de amostras de falhas corretamente'\n",
      "array([ 9.8000000e+01,  1.3300000e+02,  0.0000000e+00,  0.0000000e+00,\n",
      "        5.5555550e-01,  1.7213255e-01,  3.8888887e-01,  3.2773070e-01,\n",
      "        9.6296300e-01,  0.0000000e+00,  2.7777777e+00,  1.1111111e-01,\n",
      "       -2.8888888e+00,  5.4444447e+00, -2.5555556e+00,  2.7777777e+00,\n",
      "        1.0000000e+00, -2.1232540e+00])\n"
     ]
    }
   ],
   "source": [
    "#HIPERPARÂMETROS\n",
    "inliers = 0\n",
    "threshold = 0.5\n",
    "\n",
    "minmax_scaler = sk_minmax()\n",
    "model = compose.Pipeline(\n",
    "    anomaly.HalfSpaceTrees(seed=42)\n",
    ")\n",
    "\n",
    "x_train = NORMAL_x_train.to_numpy()\n",
    "x_val = NORMAL_x_test.to_numpy()\n",
    "x_test = S1[S1['categorical_label'] != 0].iloc[:, :-2].to_numpy()\n",
    "\n",
    "# INITIAL PHASE\n",
    "for x in x_train:\n",
    "    x_scaled = minmax_scaler.partial_fit(x.reshape(1, x.shape[0])).transform(x.reshape(1, x.shape[0]))\n",
    "    model.learn_one(numpy2dict(x_scaled))\n",
    "\n",
    "for x in x_val:\n",
    "    x_scaled = minmax_scaler.transform(x.reshape(1, x.shape[0]))\n",
    "\n",
    "    score = model.score_one(numpy2dict(x_scaled[0]))\n",
    "\n",
    "    if (score <= threshold): #low score - normal observation\n",
    "        inliers = inliers+1\n",
    "\n",
    "pprint(f'Identificou {inliers/len(x_val)*100:.2f}% de amostras NORMAIS corretamente')\n",
    "\n",
    "#DETECTOR PHASE\n",
    "outliers = 0\n",
    "for x in x_test:\n",
    "    x_scaled = minmax_scaler.transform(x.reshape(1, x.shape[0]))\n",
    "    score = model.score_one(numpy2dict(x_scaled[0]))\n",
    "\n",
    "    if (score > threshold):\n",
    "        outliers = outliers+1\n",
    "\n",
    "    #se outlier -> reverte transformações -> envia para detecção\n",
    "\n",
    "pprint(f'Identificou {outliers/len(x_test)*100:.2f}% de amostras de falhas corretamente')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OneClass SVM | IF | LOF - SUPERVISED AS A BASELINE\n",
    "\n",
    "Pode ser utilizado como linha de base para comparação desta fase!\n",
    "\n",
    "Resultados foram excelentes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n",
      "            max_iter=-1, nu=0.5, shrinking=True, tol=0.001, verbose=False)\n",
      "'Identificou 8.080808080808081% de amostras normais corretamente'\n",
      "'Identificou 100.0% de amostras de falhas corretamente'\n",
      "********\n",
      "IsolationForest(behaviour='deprecated', bootstrap=False, contamination='auto',\n",
      "                max_features=1.0, max_samples='auto', n_estimators=100,\n",
      "                n_jobs=None, random_state=0, verbose=0, warm_start=False)\n",
      "'Identificou 80.8080808080808% de amostras normais corretamente'\n",
      "'Identificou 95.9090909090909% de amostras de falhas corretamente'\n",
      "********\n",
      "LocalOutlierFactor(algorithm='auto', contamination='auto', leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=2, novelty=True, p=2)\n",
      "'Identificou 86.86868686868688% de amostras normais corretamente'\n",
      "'Identificou 99.74747474747475% de amostras de falhas corretamente'\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def verify(model, inliers, outliers):\n",
    "    print(model)\n",
    "    inliers_detected = 0\n",
    "    for k, x in enumerate(inliers):\n",
    "        if (x == 1):\n",
    "            inliers_detected = inliers_detected+1\n",
    "\n",
    "    pprint('Identificou ' + str(inliers_detected/len(inliers)*100) + '% de amostras normais corretamente')\n",
    "\n",
    "    outliers_detected = 0\n",
    "    for k, x in enumerate(outliers):\n",
    "        if (x == -1):\n",
    "            outliers_detected = outliers_detected+1\n",
    "\n",
    "    pprint('Identificou ' + str(outliers_detected/len(outliers)*100) + '% de amostras de falhas corretamente')\n",
    "    print('********')\n",
    "\n",
    "clfs = [\n",
    "    OneClassSVM(gamma='auto').fit(S2_2.iloc[:, :-1]),\n",
    "    IsolationForest(random_state=0).fit(S2_2.iloc[:, :-1]),\n",
    "    LocalOutlierFactor(n_neighbors=2, novelty=True).fit(S2_2.iloc[:, :-1]),\n",
    "]\n",
    "\n",
    "for k, clf in enumerate(clfs):\n",
    "    verify(clf, clf.predict(S2_1.iloc[:, :-1]), clf.predict(S1.iloc[:, :-2]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Testes do Min Max SCALER Incremental"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 10.]]\n",
      "array([[0., 0.]])\n",
      "[[0. 5.]]\n",
      "array([[0., 0.]])\n",
      "[[0. 0.]]\n",
      "array([[0., 0.]])\n",
      "[[0. 5.]]\n",
      "array([[0. , 0.5]])\n",
      "[[0. 5.]]\n",
      "array([[0. , 0.5]])\n",
      "[[ 0. 20.]]\n",
      "array([[0., 1.]])\n",
      "[[10. 20.]]\n",
      "array([[1., 1.]])\n",
      "[[5. 5.]]\n",
      "array([[0.5 , 0.25]])\n"
     ]
    }
   ],
   "source": [
    "# MinMax Scikit Learning\n",
    "# FUNCIONOU PERFEITO\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([\n",
    "                [[0.0, 10]],\n",
    "                [[0.0, 5]],\n",
    "                [[0.0, 0]],\n",
    "                [[0.0, 5]],\n",
    "                [[0.0, 5]],\n",
    "                [[0.0, 20]],\n",
    "                [[10, 20]],\n",
    "                [[5, 5]]\n",
    "])\n",
    "\n",
    "sk_scaler = MinMaxScaler()\n",
    "\n",
    "for k, v in enumerate(data):\n",
    "    pprint(sk_scaler.partial_fit(v).transform(v))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}