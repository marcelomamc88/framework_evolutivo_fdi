{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iCIoMaMTBg7l"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ZKjhbuD8Bg7r"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "google_colab = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "path = './'\n",
    "\n",
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_4'\n",
    "\n",
    "\n",
    "dict_ds = {\n",
    "\n",
    "    'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "    'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "    'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "    'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "    'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "    'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXPERIMENT 1\n",
    "\n",
    "- Detector Phase with Normalization (only Normal data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "eLdgonXWBg70"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## normalization"
   ],
   "metadata": {
    "collapsed": false,
    "id": "O3K1g6TsBg71"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "data_ds3_t1_normal = dict_ds['data_ds3_normal_t1_original']\n",
    "data_ds3_t2_normal = dict_ds['data_ds3_normal_t2_original']\n",
    "data_ds3_fault1 = dict_ds['data_ds3_fault1_original']\n",
    "data_ds3_fault2 = dict_ds['data_ds3_fault2_original']\n",
    "data_ds3_fault3 = dict_ds['data_ds3_fault3_original']\n",
    "data_ds3_fault4 = dict_ds['data_ds3_fault4_original']\n",
    "\n",
    "# fit values\n",
    "ss.partial_fit(data_ds3_t1_normal)\n",
    "ss.partial_fit(data_ds3_t2_normal)\n",
    "ss.partial_fit(data_ds3_fault1)\n",
    "ss.partial_fit(data_ds3_fault2)\n",
    "ss.partial_fit(data_ds3_fault3)\n",
    "ss.partial_fit(data_ds3_fault4)\n",
    "\n",
    "# transform values\n",
    "data_ds3_t1_normal = ss.transform(data_ds3_t1_normal)\n",
    "data_ds3_t2_normal = ss.transform(data_ds3_t2_normal)\n",
    "data_ds3_fault1 = ss.transform(data_ds3_fault1)\n",
    "data_ds3_fault2 = ss.transform(data_ds3_fault2)\n",
    "data_ds3_fault3 = ss.transform(data_ds3_fault3)\n",
    "data_ds3_fault4 = ss.transform(data_ds3_fault4)\n",
    "\n",
    "# append normal labels\n",
    "data_ds3_t1_normal = np.append(data_ds3_t1_normal, np.zeros((data_ds3_t1_normal.shape[0],1)), axis = 1)\n",
    "data_ds3_t2_normal = np.append(data_ds3_t2_normal, np.zeros((data_ds3_t2_normal.shape[0],1)), axis = 1)\n",
    "\n",
    "# append fault labels\n",
    "def generate_fault_label(dataset, fault_label):\n",
    "    labels = np.array([[fault_label]]*dataset.shape[0])\n",
    "\n",
    "    return labels\n",
    "\n",
    "data_ds3_fault1 = np.append(data_ds3_fault1, generate_fault_label(data_ds3_fault1, 1), axis = 1)\n",
    "data_ds3_fault2 = np.append(data_ds3_fault2, generate_fault_label(data_ds3_fault2, 2), axis = 1)\n",
    "data_ds3_fault3 = np.append(data_ds3_fault3, generate_fault_label(data_ds3_fault3, 3), axis = 1)\n",
    "data_ds3_fault4 = np.append(data_ds3_fault4, generate_fault_label(data_ds3_fault4, 4), axis = 1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "l5N9KYHoBg71"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder (PyTorch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "NZXjBQcBBg73"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# FUNCTIONS AND CLASSES\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encode_l, decode_l):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(encode_l)\n",
    "        self.decoder = nn.Sequential(decode_l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def run_train(net, train_loader, num_epochs, optimizer, loss_func):\n",
    "    train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        losses = []\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            ### forward ###\n",
    "            if google_colab:\n",
    "                output = net(real_samples.type(torch.FloatTensor).cuda())\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor).cuda())\n",
    "            else:\n",
    "                output = net(real_samples.type(torch.FloatTensor))\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item)\n",
    "\n",
    "            ### backward ###\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        step_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(step_loss)\n",
    "        ### log ###\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss))\n",
    "\n",
    "    return net, output, train_loss, losses\n",
    "\n",
    "def generate_ground_truth(data_test):\n",
    "    dimension = 2700\n",
    "    ground_truth = []\n",
    "\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] == dimension: # X_test\n",
    "            ground_truth.append(0)\n",
    "        else: # % others: get column label\n",
    "            if x[-1] != 0:\n",
    "                ground_truth.append(1)\n",
    "            else:\n",
    "                ground_truth.append(0)\n",
    "\n",
    "    return ground_truth\n",
    "\n",
    "def generate_losses(data_test, net, loss_function):\n",
    "    dimension = 2700\n",
    "    losses = []\n",
    "    #regenerate_data = np.zeros((1, dimension))\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] > dimension:\n",
    "            x = x[:-1]\n",
    "\n",
    "        real = x.reshape(1,-1).astype(np.float32)\n",
    "\n",
    "        regenerate = net(torch.from_numpy(real))\n",
    "        #regenerate_data[j] = regenerate.cpu().detach().numpy()\n",
    "\n",
    "        if google_colab:\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real).cuda()).cpu().detach().numpy()\n",
    "        else:\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real)).cpu().detach().numpy()\n",
    "\n",
    "        losses.append(loss_ae)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def generate_y_hat(losses, loss_threshold):\n",
    "    y_hat = []\n",
    "\n",
    "    for l in losses:\n",
    "        if l < loss_threshold:\n",
    "            y_hat.append(0)\n",
    "        else:\n",
    "            y_hat.append(1)\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def tester(data_test, net, loss_function, loss_threshold = 1):\n",
    "\n",
    "    ground_truth = []\n",
    "    losses = []\n",
    "\n",
    "    for n, dataset_name in enumerate(data_test):\n",
    "        dataset = data_test[dataset_name]\n",
    "\n",
    "        ground_truth = ground_truth + generate_ground_truth(dataset)\n",
    "\n",
    "        losses = losses + generate_losses(dataset, net, loss_function)\n",
    "\n",
    "    y_hat = generate_y_hat(losses, loss_threshold)\n",
    "\n",
    "    return confusion_matrix(ground_truth, y_hat, normalize='true'), losses, ground_truth, y_hat\n",
    "\n",
    "def generate_encode_decode_layers(layers, output_layer):\n",
    "    od_encode = []\n",
    "    od_decode = []\n",
    "    # encode\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_encode.append(('l'+str((len(od_encode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_encode.append(('l'+str((len(od_encode)+1)), nn.ReLU()))\n",
    "\n",
    "    # decode\n",
    "    layers.reverse()\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_decode.append(('l'+str((len(od_decode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), nn.ReLU()))\n",
    "        else:\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), output_layer))\n",
    "\n",
    "    return OrderedDict(od_encode), OrderedDict(od_decode)\n",
    "\n",
    "def train(layers, last_layer, lr, epochs, batch_size, X_train, optim, loss_fnc):\n",
    "    encode_l, decode_l = generate_encode_decode_layers(layers, last_layer)\n",
    "    net = Autoencoder(encode_l, decode_l)\n",
    "\n",
    "    if google_colab:\n",
    "        net.cuda()\n",
    "\n",
    "    if (optim == 'ADAM'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif(optim == 'SGD'):\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    elif(optim == 'RMSprop'):\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    torch.manual_seed(111)\n",
    "\n",
    "    # sets\n",
    "    train_set = [\n",
    "        (X_train, X_train) for i in range(len(X_train))\n",
    "    ]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    net, output, loss, losses = run_train(net, train_loader, epochs, optimizer, loss_fnc)\n",
    "\n",
    "    return net, output, loss, losses, loss_fnc\n",
    "\n",
    "def test(X_train, X_test, net, loss_function):\n",
    "    faults = {'F1': data_ds3_fault1,\n",
    "              'F2': data_ds3_fault2,\n",
    "              'F3': data_ds3_fault3,\n",
    "              'F4': data_ds3_fault4}\n",
    "\n",
    "    datas = {'X_TRAIN':X_train,\n",
    "             'X_TEST':X_test,\n",
    "             'NORMAL 2':data_ds3_t2_normal}\n",
    "\n",
    "    datas.update(faults)\n",
    "\n",
    "    for n, dataset_name in enumerate(datas):\n",
    "        print('Losses ' + dataset_name , file=log)\n",
    "        losses = generate_losses(datas[dataset_name], net, loss_function)\n",
    "\n",
    "        print(\"mu: \", np.mean(losses), file=log)\n",
    "        print(\"std: \", np.std(losses), file=log)\n",
    "\n",
    "        phi_test = np.mean(losses) + np.std(losses)\n",
    "        print(\"phi: \", phi_test, file=log)\n",
    "\n",
    "        if (dataset_name == 'X_TRAIN'):\n",
    "            phi = phi_test\n",
    "\n",
    "        print('******************************', file=log)\n",
    "        print('******************************', file=log)\n",
    "        print('******************************', file=log)\n",
    "\n",
    "    # CONFUSION MATRIX FOR NORMAL_2\n",
    "    cf, losses, ground_truth, y_hat = tester({'NORMAL 2':data_ds3_t2_normal}, net, loss_function, phi)\n",
    "\n",
    "    print('NORMAL 2', file=log)\n",
    "    print(cf, file=log)\n",
    "    print('******************************', file=log)\n",
    "    print('******************************', file=log)\n",
    "    print('******************************', file=log)\n",
    "\n",
    "    for n, fault_name in enumerate(faults):\n",
    "\n",
    "        datas = {'X_TEST': X_test,\n",
    "                 'fault_name': faults[fault_name]}\n",
    "\n",
    "        cf, losses, ground_truth, y_hat = tester(datas, net, loss_function, phi)\n",
    "\n",
    "        print('X_TEST x ' + fault_name, file=log)\n",
    "        print(cf, file=log)\n",
    "        print('******************************', file=log)\n",
    "        print('******************************', file=log)\n",
    "        print('******************************', file=log)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:0.5185\n",
      "epoch [1/1], loss:0.4016\n",
      "epoch [1/1], loss:0.1518\n",
      "epoch [1/1], loss:0.6139\n",
      "epoch [1/1], loss:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:-0.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 2700 128 21|Sigmoid()|64|ADAM|TripletMarginLoss()\n",
      "epoch [1/1], loss:0.8667\n",
      "ERROR: 2700 128 21|Sigmoid()|64|ADAM|NLLLoss()\n",
      "epoch [1/1], loss:1.4833\n",
      "ERROR: 2700 128 21|Sigmoid()|64|ADAM|MarginRankingLoss()\n",
      "epoch [1/1], loss:0.4081\n",
      "epoch [1/1], loss:0.3369\n",
      "epoch [1/1], loss:0.1000\n",
      "ERROR: 2700 128 21|Tanh()|64|ADAM|BCELoss()\n",
      "epoch [1/1], loss:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:-0.2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 2700 128 21|Tanh()|64|ADAM|TripletMarginLoss()\n",
      "epoch [1/1], loss:0.6248\n",
      "ERROR: 2700 128 21|Tanh()|64|ADAM|NLLLoss()\n",
      "epoch [1/1], loss:0.9114\n",
      "ERROR: 2700 128 21|Tanh()|64|ADAM|MarginRankingLoss()\n",
      "epoch [1/1], loss:0.4406\n",
      "epoch [1/1], loss:0.3434\n",
      "ERROR: 2700 128 21|Sigmoid()|32|ADAM|SmoothL1Loss()\n",
      "epoch [1/1], loss:0.3803\n",
      "epoch [1/1], loss:1.0000\n",
      "ERROR: 2700 128 21|Sigmoid()|32|ADAM|HingeEmbeddingLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:-0.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 2700 128 21|Sigmoid()|32|ADAM|TripletMarginLoss()\n",
      "epoch [1/1], loss:0.7762\n",
      "ERROR: 2700 128 21|Sigmoid()|32|ADAM|NLLLoss()\n"
     ]
    }
   ],
   "source": [
    "log = open('output.txt', \"a\", buffering=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_ds3_t1_normal[:, :-1], data_ds3_t1_normal[:, -1], test_size=0.7, random_state=42)\n",
    "\n",
    "\"\"\"architectures = [[2700,128,16]]\n",
    "last_layers = [nn.Sigmoid()]\n",
    "batch_sizes = [64]\n",
    "optims = ['ADAM']\n",
    "losses_fnc = [nn.HingeEmbeddingLoss(), nn.MSELoss()]\"\"\"\n",
    "\n",
    "architectures = [[2700,128,21], [2700,128,16], [2700,128,12], [2700,128,9], [2700,128,6], [2700,128,4], [2700,128,3], [2700,128,2],\n",
    "                 [2700,512,128,21], [2700,512,128,16], [2700,512,128,12], [2700,512,128,9], [2700,512,128,6], [2700,512,128,3], [2700,512,128,2],\n",
    "                 [2700,1024,128,21], [2700,1024,128,16], [2700,1024,128,12], [2700,1024,128,9], [2700,1024,128,6], [2700,1024,128,3], [2700,512,128,2],\n",
    "                 [2700,1024,256,21], [2700,1024,256,16], [2700,1024,256,12], [2700,1024,256,9], [2700,1024,256,6], [2700,1024,256,3], [2700,512,256,2],\n",
    "                 [2700,1024,512,21], [2700,1024,512,16], [2700,1024,512,12], [2700,1024,512,9], [2700,1024,512,6], [2700,1024,512,3], [2700,512,512,2],\n",
    "                 [2700,1024,512,64,21], [2700,1024,512,64,16], [2700,1024,512,64,12], [2700,1024,512,64,9], [2700,1024,512,64,6], [2700,1024,512,64,3], [2700,1024,512,64,2],\n",
    "                 [2700,1024,128,64,21], [2700,1024,128,64,16], [2700,1024,128,64,12], [2700,1024,128,64,9], [2700,1024,128,64,6], [2700,1024,128,64,3], [2700,1024,128,64,2],\n",
    "                 [2700,1024,256,64,21], [2700,1024,256,64,16], [2700,1024,256,64,12], [2700,1024,256,64,9], [2700,1024,256,64,6], [2700,1024,256,64,3], [2700,1024,128,64,2]]\n",
    "\n",
    "last_layers = [nn.Sigmoid(),nn.Tanh()]\n",
    "batch_sizes = [64, 32]\n",
    "optims = ['ADAM', 'SGD', 'RMSprop']\n",
    "losses_fnc = [nn.MSELoss() ,nn.L1Loss(), nn.SmoothL1Loss(), nn.BCELoss(), nn.HingeEmbeddingLoss(), nn.KLDivLoss(), nn.TripletMarginLoss(), nn.BCEWithLogitsLoss(), nn.NLLLoss(), nn.PoissonNLLLoss(), nn.MarginRankingLoss()]\n",
    "\n",
    "for architecture in architectures:\n",
    "    for optim in optims:\n",
    "        for batch_size in batch_sizes:\n",
    "            for last_layer in last_layers:\n",
    "                for loss_fnc in losses_fnc:\n",
    "                    print('*****************************************', file=log)\n",
    "                    print('****************************************', file=log)\n",
    "                    print('************ HYPERPARAMETERS ************', file=log)\n",
    "                    print('*****************************************', file=log)\n",
    "                    print('*****************************************', file=log)\n",
    "                    print(architecture, file=log)\n",
    "                    print(last_layer, file=log)\n",
    "                    print(batch_size, file=log)\n",
    "                    print(optim, file=log)\n",
    "                    print(loss_fnc, file=log)\n",
    "                    print('*****************************************', file=log)\n",
    "                    print('*****************************************', file=log)\n",
    "\n",
    "                    try:\n",
    "                        net, output, loss, losses, loss_function = train(layers=architecture.copy(), last_layer=last_layer, lr=1e-3, epochs=1, batch_size=batch_size, X_train=X_train, optim=optim, loss_fnc=loss_fnc)\n",
    "\n",
    "                        test(X_train, X_test, net, loss_function)\n",
    "                    except:\n",
    "                        print(\"ERROR: \" + ' '.join(map(str,architecture)) + '|' + str(last_layer) + '|' + str(batch_size) + '|' + str(optim) + '|' + str(loss_fnc))\n",
    "                        print(\"ERROR: \" + ' '.join(map(str,architecture)) + '|' + str(last_layer) + '|' + str(batch_size) + '|' + str(optim) + '|' + str(loss_fnc), file=log)\n",
    "\n",
    "log.close()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "WBpHJcvJBg75"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "efdi_F16_maneuver.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}