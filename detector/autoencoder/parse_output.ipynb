{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- LOAD BASE FILES\n",
    "- CONVERT TO 2Hz\n",
    "- FIND BEST NETWORK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = {'arch':5, 'output_layer':6, 'batch_size':7, 'optimizer':8, 'loss_function':9,\n",
    "           'mu':13, 'std':14, 'x_train_loss':15, 'x_test_loss':22, 'normal_2_loss':29, 'f1_loss':36, 'f2_loss':43, 'f3_loss':50, 'f4_loss':57,\n",
    "           'normal_1_acc': 68, 'normal_2_acc':62, 'f1_acc':69, 'f2_acc':75, 'f3_acc':81, 'f4_acc':87}\n",
    "\n",
    "numbered_columns = ['x_train_loss', 'x_test_loss', 'normal_2_loss', 'f1_loss', 'f2_loss', 'f3_loss', 'f4_loss', 'normal_1_acc', 'normal_2_acc', 'f1_acc', 'f2_acc', 'f3_acc', 'f4_acc', 'mu', 'std']\n",
    "\n",
    "percentual_columns = ['normal_1_acc', 'normal_2_acc', 'f1_acc', 'f2_acc', 'f3_acc', 'f4_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE THAT THE \"NORMAL 2\" PATTERN COULD BE OUT OF PATTERN. REQUIRED 2 LINES as 2x2 array. IF THIS HAPPENED, MUST BE MANUALLY CONVERTED TO THE REQUIRED PATTERN AS BELOW!\n",
    "\n",
    "REQUIRED\n",
    "[[1. 0.]\n",
    " [0. 0.]]\n",
    "\n",
    "OUT OF PATTERN\n",
    "[[1.]]\n",
    "'''\n",
    "\n",
    "with open('./outputs/output_dr2 (1).txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "height = 91\n",
    "times = floor(len(lines)/height)\n",
    "\n",
    "output = pd.DataFrame(np.zeros((times, len(columns))), columns=columns)\n",
    "\n",
    "for k, column_name in enumerate(columns):\n",
    "    column_init_position = columns[column_name]\n",
    "\n",
    "    for time in np.arange(0, times):\n",
    "        line_number = column_init_position+(time*height)\n",
    "        without_newline = lines[line_number].rstrip(\"\\n\")\n",
    "        without_phi = without_newline.replace('phi:', '')\n",
    "        without_phi = without_phi.replace('mu:', '')\n",
    "        without_phi = without_phi.replace('std:', '')\n",
    "        value = without_phi.strip()\n",
    "\n",
    "        if (column_name == 'normal_1_acc' or column_name == 'normal_2_acc'):\n",
    "            value = value.replace('[', '')\n",
    "            value = value.replace(']', '')\n",
    "            str_arr = value.split()\n",
    "            value = str_arr[0]\n",
    "        elif (column_name == 'f1_acc' or column_name == 'f2_acc' or column_name == 'f3_acc' or column_name == 'f4_acc'):\n",
    "            value = value.replace('[', '')\n",
    "            value = value.replace(']', '')\n",
    "            str_arr = value.split()\n",
    "            value = str_arr[1]\n",
    "\n",
    "        if (column_name in numbered_columns):\n",
    "            value = float(value)\n",
    "\n",
    "        if (column_name == 'batch_size'):\n",
    "            value = int(value)\n",
    "\n",
    "        if (column_name in percentual_columns):\n",
    "            value = value*100\n",
    "\n",
    "        output.iloc[time,k] = value\n",
    "\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                arch output_layer  batch_size optimizer loss_function  \\\n107     [54, 32, 16]       Tanh()        32.0      ADAM      L1Loss()   \n111      [54, 32, 9]       Tanh()        32.0      ADAM      L1Loss()   \n115      [54, 32, 5]       Tanh()        32.0      ADAM      L1Loss()   \n119      [54, 32, 3]       Tanh()        32.0      ADAM      L1Loss()   \n143      [54, 16, 7]       Tanh()        32.0      ADAM      L1Loss()   \n147         [54, 12]       Tanh()        32.0      ADAM      L1Loss()   \n171         [54, 16]       Tanh()        32.0      ADAM      L1Loss()   \n175  [54, 32, 16, 7]       Tanh()        32.0      ADAM      L1Loss()   \n485     [54, 32, 16]       Tanh()        32.0   RMSprop      L1Loss()   \n489      [54, 32, 9]       Tanh()        32.0   RMSprop      L1Loss()   \n493      [54, 32, 5]       Tanh()        32.0   RMSprop      L1Loss()   \n497      [54, 32, 3]       Tanh()        32.0   RMSprop      L1Loss()   \n505      [54, 16, 9]       Tanh()        32.0   RMSprop      L1Loss()   \n509      [54, 16, 9]       Tanh()        32.0   RMSprop      L1Loss()   \n513      [54, 16, 5]       Tanh()        32.0   RMSprop      L1Loss()   \n517      [54, 16, 3]       Tanh()        32.0   RMSprop      L1Loss()   \n521      [54, 16, 7]       Tanh()        32.0   RMSprop      L1Loss()   \n525         [54, 12]       Tanh()        32.0   RMSprop      L1Loss()   \n549         [54, 16]       Tanh()        32.0   RMSprop      L1Loss()   \n553  [54, 32, 16, 7]       Tanh()        32.0   RMSprop      L1Loss()   \n561  [54, 32, 16, 3]       Tanh()        32.0   RMSprop      L1Loss()   \n\n           mu       std  x_train_loss  x_test_loss  normal_2_loss  ...  \\\n107  0.014855  0.032307      0.047162          NaN            NaN  ...   \n111  0.014877  0.031709      0.046586          NaN            NaN  ...   \n115  0.015633  0.034185      0.049818          NaN            NaN  ...   \n119  0.015931  0.035472      0.051403          NaN            NaN  ...   \n143  0.017172  0.036746      0.053917          NaN            NaN  ...   \n147  0.017104  0.037785      0.054889          NaN            NaN  ...   \n171  0.016247  0.035885      0.052132          NaN            NaN  ...   \n175  0.017219  0.036673      0.053892          NaN            NaN  ...   \n485  0.015740  0.029197      0.044938          NaN            NaN  ...   \n489  0.016272  0.029429      0.045702          NaN            NaN  ...   \n493  0.018628  0.031233      0.049861          NaN            NaN  ...   \n497  0.018467  0.032225      0.050692          NaN            NaN  ...   \n505  0.019897  0.034493      0.054390          NaN            NaN  ...   \n509  0.019897  0.034493      0.054390          NaN            NaN  ...   \n513  0.020339  0.035024      0.055363          NaN            NaN  ...   \n517  0.020051  0.035338      0.055389          NaN            NaN  ...   \n521  0.020516  0.034593      0.055109          NaN            NaN  ...   \n525  0.019606  0.033105      0.052711          NaN            NaN  ...   \n549  0.018808  0.032708      0.051516          NaN            NaN  ...   \n553  0.018032  0.030859      0.048891          NaN            NaN  ...   \n561  0.019632  0.035014      0.054646          NaN            NaN  ...   \n\n     f3_loss  f4_loss  normal_1_acc  normal_2_acc  f1_acc  f2_acc  f3_acc  \\\n107      NaN      NaN         88.43         88.31   69.12   100.0   82.28   \n111      NaN      NaN         87.79         87.20   63.97   100.0   95.46   \n115      NaN      NaN         88.16         87.55   52.23   100.0   89.32   \n119      NaN      NaN         87.47         86.38   56.37   100.0   91.22   \n143      NaN      NaN         88.06         87.53   64.47   100.0   98.89   \n147      NaN      NaN         88.44         88.34   63.32   100.0   99.49   \n171      NaN      NaN         88.49         88.43   62.19   100.0   98.80   \n175      NaN      NaN         87.76         86.97   50.95   100.0   81.52   \n485      NaN      NaN         88.44         87.99   73.80   100.0   97.76   \n489      NaN      NaN         88.02         87.75   81.12   100.0   94.25   \n493      NaN      NaN         88.29         88.17   68.82   100.0   98.78   \n497      NaN      NaN         87.97         87.15   64.30   100.0   71.08   \n505      NaN      NaN         88.29         87.65   60.02   100.0   91.49   \n509      NaN      NaN         88.29         87.65   60.02   100.0   91.49   \n513      NaN      NaN         87.29         86.68   52.51   100.0   97.14   \n517      NaN      NaN         87.75         87.01   65.19   100.0   80.33   \n521      NaN      NaN         87.85         87.58   53.52   100.0   96.14   \n525      NaN      NaN         88.81         88.76   60.55   100.0   96.95   \n549      NaN      NaN         88.51         88.75   59.47   100.0   99.52   \n553      NaN      NaN         88.08         87.73   74.16   100.0   76.87   \n561      NaN      NaN         88.84         88.45   56.40   100.0   71.32   \n\n     f4_acc  d_trainloss_f1loss  d_normal1_normal2_acc  \n107   100.0                 NaN                   0.12  \n111   100.0                 NaN                   0.59  \n115   100.0                 NaN                   0.61  \n119   100.0                 NaN                   1.09  \n143   100.0                 NaN                   0.53  \n147   100.0                 NaN                   0.10  \n171   100.0                 NaN                   0.06  \n175   100.0                 NaN                   0.79  \n485   100.0                 NaN                   0.45  \n489   100.0                 NaN                   0.27  \n493   100.0                 NaN                   0.12  \n497   100.0                 NaN                   0.82  \n505   100.0                 NaN                   0.64  \n509   100.0                 NaN                   0.64  \n513   100.0                 NaN                   0.61  \n517   100.0                 NaN                   0.74  \n521   100.0                 NaN                   0.27  \n525   100.0                 NaN                   0.05  \n549   100.0                 NaN                  -0.24  \n553   100.0                 NaN                   0.35  \n561   100.0                 NaN                   0.39  \n\n[21 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>arch</th>\n      <th>output_layer</th>\n      <th>batch_size</th>\n      <th>optimizer</th>\n      <th>loss_function</th>\n      <th>mu</th>\n      <th>std</th>\n      <th>x_train_loss</th>\n      <th>x_test_loss</th>\n      <th>normal_2_loss</th>\n      <th>...</th>\n      <th>f3_loss</th>\n      <th>f4_loss</th>\n      <th>normal_1_acc</th>\n      <th>normal_2_acc</th>\n      <th>f1_acc</th>\n      <th>f2_acc</th>\n      <th>f3_acc</th>\n      <th>f4_acc</th>\n      <th>d_trainloss_f1loss</th>\n      <th>d_normal1_normal2_acc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>107</th>\n      <td>[54, 32, 16]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.014855</td>\n      <td>0.032307</td>\n      <td>0.047162</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.43</td>\n      <td>88.31</td>\n      <td>69.12</td>\n      <td>100.0</td>\n      <td>82.28</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>[54, 32, 9]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.014877</td>\n      <td>0.031709</td>\n      <td>0.046586</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.79</td>\n      <td>87.20</td>\n      <td>63.97</td>\n      <td>100.0</td>\n      <td>95.46</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.59</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>[54, 32, 5]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.015633</td>\n      <td>0.034185</td>\n      <td>0.049818</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.16</td>\n      <td>87.55</td>\n      <td>52.23</td>\n      <td>100.0</td>\n      <td>89.32</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.61</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>[54, 32, 3]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.015931</td>\n      <td>0.035472</td>\n      <td>0.051403</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.47</td>\n      <td>86.38</td>\n      <td>56.37</td>\n      <td>100.0</td>\n      <td>91.22</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>1.09</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>[54, 16, 7]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.017172</td>\n      <td>0.036746</td>\n      <td>0.053917</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.06</td>\n      <td>87.53</td>\n      <td>64.47</td>\n      <td>100.0</td>\n      <td>98.89</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>[54, 12]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.017104</td>\n      <td>0.037785</td>\n      <td>0.054889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.44</td>\n      <td>88.34</td>\n      <td>63.32</td>\n      <td>100.0</td>\n      <td>99.49</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>[54, 16]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.016247</td>\n      <td>0.035885</td>\n      <td>0.052132</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.49</td>\n      <td>88.43</td>\n      <td>62.19</td>\n      <td>100.0</td>\n      <td>98.80</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>[54, 32, 16, 7]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>ADAM</td>\n      <td>L1Loss()</td>\n      <td>0.017219</td>\n      <td>0.036673</td>\n      <td>0.053892</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.76</td>\n      <td>86.97</td>\n      <td>50.95</td>\n      <td>100.0</td>\n      <td>81.52</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.79</td>\n    </tr>\n    <tr>\n      <th>485</th>\n      <td>[54, 32, 16]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.015740</td>\n      <td>0.029197</td>\n      <td>0.044938</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.44</td>\n      <td>87.99</td>\n      <td>73.80</td>\n      <td>100.0</td>\n      <td>97.76</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.45</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>[54, 32, 9]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.016272</td>\n      <td>0.029429</td>\n      <td>0.045702</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.02</td>\n      <td>87.75</td>\n      <td>81.12</td>\n      <td>100.0</td>\n      <td>94.25</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>493</th>\n      <td>[54, 32, 5]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.018628</td>\n      <td>0.031233</td>\n      <td>0.049861</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.29</td>\n      <td>88.17</td>\n      <td>68.82</td>\n      <td>100.0</td>\n      <td>98.78</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>[54, 32, 3]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.018467</td>\n      <td>0.032225</td>\n      <td>0.050692</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.97</td>\n      <td>87.15</td>\n      <td>64.30</td>\n      <td>100.0</td>\n      <td>71.08</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.82</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>[54, 16, 9]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.019897</td>\n      <td>0.034493</td>\n      <td>0.054390</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.29</td>\n      <td>87.65</td>\n      <td>60.02</td>\n      <td>100.0</td>\n      <td>91.49</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>509</th>\n      <td>[54, 16, 9]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.019897</td>\n      <td>0.034493</td>\n      <td>0.054390</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.29</td>\n      <td>87.65</td>\n      <td>60.02</td>\n      <td>100.0</td>\n      <td>91.49</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>[54, 16, 5]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.020339</td>\n      <td>0.035024</td>\n      <td>0.055363</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.29</td>\n      <td>86.68</td>\n      <td>52.51</td>\n      <td>100.0</td>\n      <td>97.14</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.61</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>[54, 16, 3]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.020051</td>\n      <td>0.035338</td>\n      <td>0.055389</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.75</td>\n      <td>87.01</td>\n      <td>65.19</td>\n      <td>100.0</td>\n      <td>80.33</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>521</th>\n      <td>[54, 16, 7]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.020516</td>\n      <td>0.034593</td>\n      <td>0.055109</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87.85</td>\n      <td>87.58</td>\n      <td>53.52</td>\n      <td>100.0</td>\n      <td>96.14</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>525</th>\n      <td>[54, 12]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.019606</td>\n      <td>0.033105</td>\n      <td>0.052711</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.81</td>\n      <td>88.76</td>\n      <td>60.55</td>\n      <td>100.0</td>\n      <td>96.95</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>[54, 16]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.018808</td>\n      <td>0.032708</td>\n      <td>0.051516</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.51</td>\n      <td>88.75</td>\n      <td>59.47</td>\n      <td>100.0</td>\n      <td>99.52</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>-0.24</td>\n    </tr>\n    <tr>\n      <th>553</th>\n      <td>[54, 32, 16, 7]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.018032</td>\n      <td>0.030859</td>\n      <td>0.048891</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.08</td>\n      <td>87.73</td>\n      <td>74.16</td>\n      <td>100.0</td>\n      <td>76.87</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>561</th>\n      <td>[54, 32, 16, 3]</td>\n      <td>Tanh()</td>\n      <td>32.0</td>\n      <td>RMSprop</td>\n      <td>L1Loss()</td>\n      <td>0.019632</td>\n      <td>0.035014</td>\n      <td>0.054646</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>88.84</td>\n      <td>88.45</td>\n      <td>56.40</td>\n      <td>100.0</td>\n      <td>71.32</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>0.39</td>\n    </tr>\n  </tbody>\n</table>\n<p>21 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = output.query('f1_acc > 0 and normal_1_acc > 50 and normal_2_acc > 50 and f1_acc > 50 and f2_acc > 50 and f3_acc > 50 and f4_acc > 50')\n",
    "\n",
    "q['d_trainloss_f1loss'] = q['f1_loss'] - q['x_train_loss']\n",
    "q['d_normal1_normal2_acc'] = q['normal_1_acc'] - q['normal_2_acc']\n",
    "\n",
    "q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "samples = [10000, 10000, 10000, 10000, 10000, 10000]\n",
    "\n",
    "q['samples_n1'] = q['normal_1_acc'] * samples[0] / 100\n",
    "q['samples_n2'] = q['normal_2_acc'] * samples[1] / 100\n",
    "q['samples_f1'] = q['f1_acc'] * samples[2] / 100\n",
    "q['samples_f2'] = q['f2_acc'] * samples[3] / 100\n",
    "q['samples_f3'] = q['f3_acc'] * samples[4] / 100\n",
    "q['samples_f4'] = q['f4_acc'] * samples[5] / 100\n",
    "\n",
    "idx = []\n",
    "for i, e in q.iterrows():\n",
    "    idx.append(i)\n",
    "\n",
    "q1 = q.loc[idx]\n",
    "\n",
    "for n, e in q1.iterrows():\n",
    "    tp = ceil(e['samples_n1']) + ceil(e['samples_n2'])\n",
    "    fp = samples[0] + samples[1] - tp\n",
    "    tn = ceil(e['samples_f1']) + ceil(e['samples_f2']) + ceil(e['samples_f3']) + ceil(e['samples_f4'])\n",
    "    fn = samples[2] + samples[3] + samples[4] + samples[5] - tn\n",
    "\n",
    "    y_true = [0] * (samples[0]+samples[1]) + [1] * (samples[2]+samples[3]+samples[4]+samples[5])\n",
    "    y_pred = [0] * tp + [1] * fp + [0] * fn + [1] * tn\n",
    "\n",
    "    q1.loc[n, 'ba'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    q1.loc[n, 'precision'] = precision_score(y_true, y_pred)\n",
    "    q1.loc[n, 'recall'] = recall_score(y_true, y_pred) #(tn / (tn+fn))\n",
    "    q1.loc[n, 'specificity'] = tn / (tn+fp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "q1.iloc[:, :].to_pickle('outputs/autoencoder_resultado.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}