{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iCIoMaMTBg7l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pymongo import MongoClient\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('6240f18b08d0c9c605fbeba5'), 'threshold': 0.12458635450553127}]\n"
     ]
    }
   ],
   "source": [
    "fdi_db = MongoClient('localhost', 27017).fdi\n",
    "detector_metadata = fdi_db.detector_metadata\n",
    "meta = list(detector_metadata.find({}))\n",
    "print(meta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = 20000\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = .96\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../../dataset/original/'\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output-dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-wl_' + str(WINDOW_LENGHT) + '_final.txt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CILVSdmV43uh",
    "outputId": "ccfdfad7-dc42-41e9-9c07-d4cfe527aa98"
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown --id 1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown --id 1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ\n",
    "    !gdown --id 1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown --id 1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown --id 17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown --id 1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l5N9KYHoBg71"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "data_ds3_t1_normal = dict_ds['data_ds3_normal_t1_original']\n",
    "data_ds3_t2_normal = dict_ds['data_ds3_normal_t2_original']\n",
    "data_ds3_fault1 = dict_ds['data_ds3_fault1_original']\n",
    "data_ds3_fault2 = dict_ds['data_ds3_fault2_original']\n",
    "data_ds3_fault3 = dict_ds['data_ds3_fault3_original']\n",
    "data_ds3_fault4 = dict_ds['data_ds3_fault4_original']\n",
    "\n",
    "# fit values\n",
    "ss.partial_fit(data_ds3_t1_normal)\n",
    "ss.partial_fit(data_ds3_t2_normal)\n",
    "ss.partial_fit(data_ds3_fault1)\n",
    "ss.partial_fit(data_ds3_fault2)\n",
    "ss.partial_fit(data_ds3_fault3)\n",
    "ss.partial_fit(data_ds3_fault4)\n",
    "\n",
    "# transform values\n",
    "data_ds3_t1_normal = ss.transform(data_ds3_t1_normal)\n",
    "data_ds3_t2_normal = ss.transform(data_ds3_t2_normal)\n",
    "data_ds3_fault1 = ss.transform(data_ds3_fault1)\n",
    "data_ds3_fault2 = ss.transform(data_ds3_fault2)\n",
    "data_ds3_fault3 = ss.transform(data_ds3_fault3)\n",
    "data_ds3_fault4 = ss.transform(data_ds3_fault4)\n",
    "\n",
    "# append normal labels\n",
    "data_ds3_t1_normal = np.append(data_ds3_t1_normal, np.zeros((data_ds3_t1_normal.shape[0],1)), axis = 1)\n",
    "data_ds3_t2_normal = np.append(data_ds3_t2_normal, np.zeros((data_ds3_t2_normal.shape[0],1)), axis = 1)\n",
    "\n",
    "# append fault labels\n",
    "def generate_fault_label(dataset, fault_label):\n",
    "    labels = np.array([[fault_label]]*dataset.shape[0])\n",
    "\n",
    "    return labels\n",
    "\n",
    "data_ds3_fault1 = np.append(data_ds3_fault1, generate_fault_label(data_ds3_fault1, 1), axis = 1)\n",
    "data_ds3_fault2 = np.append(data_ds3_fault2, generate_fault_label(data_ds3_fault2, 2), axis = 1)\n",
    "data_ds3_fault3 = np.append(data_ds3_fault3, generate_fault_label(data_ds3_fault3, 3), axis = 1)\n",
    "data_ds3_fault4 = np.append(data_ds3_fault4, generate_fault_label(data_ds3_fault4, 4), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aCbDYmvL43u2"
   },
   "outputs": [],
   "source": [
    "dimension = data_ds3_t2_normal.shape[1]-1\n",
    "\n",
    "# FUNCTIONS AND CLASSES\n",
    "'''class Autoencoder(nn.Module):\n",
    "    def __init__(self, encode_l, decode_l):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(encode_l)\n",
    "        self.decoder = nn.Sequential(decode_l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))'''\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(54, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 9),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(9, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 54),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def run_train(net, train_loader, num_epochs, optimizer, loss_func):\n",
    "    train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        losses = []\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            ### forward ###\n",
    "            if cuda:\n",
    "                output = net(real_samples.type(torch.FloatTensor).cuda())\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor).cuda())\n",
    "            else:\n",
    "                output = net(real_samples.type(torch.FloatTensor))\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item)\n",
    "\n",
    "            ### backward ###\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        step_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(step_loss)\n",
    "\n",
    "        ### log ###\n",
    "        #print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss))\n",
    "\n",
    "    return net, output, train_loss, losses\n",
    "\n",
    "def generate_ground_truth(data_test):\n",
    "    ground_truth = []\n",
    "\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] == dimension: # X_test\n",
    "            ground_truth.append(0)\n",
    "        else: # % others: get column label\n",
    "            if x[-1] != 0:\n",
    "                ground_truth.append(1)\n",
    "            else:\n",
    "                ground_truth.append(0)\n",
    "\n",
    "    return ground_truth\n",
    "\n",
    "def generate_losses(data_test, net, loss_function):\n",
    "    losses = []\n",
    "    #regenerate_data = np.zeros((1, dimension))\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] > dimension:\n",
    "            x = x[:-1]\n",
    "\n",
    "        real = x.reshape(1,-1).astype(np.float32)\n",
    "\n",
    "        #regenerate_data[j] = regenerate.cpu().detach().numpy()\n",
    "\n",
    "        if cuda:\n",
    "            regenerate = net(torch.from_numpy(real).cuda())\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real).cuda()).item()\n",
    "        else:\n",
    "            regenerate = net(torch.from_numpy(real))\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real)).item()\n",
    "\n",
    "        losses.append(loss_ae)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def generate_y_hat(losses, loss_threshold):\n",
    "    y_hat = []\n",
    "\n",
    "    for l in losses:\n",
    "        if l < loss_threshold:\n",
    "            y_hat.append(0)\n",
    "        else:\n",
    "            y_hat.append(1)\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def tester(data_test, net, loss_function, loss_threshold = 1):\n",
    "\n",
    "    ground_truth = []\n",
    "    losses = []\n",
    "\n",
    "    for n, dataset_name in enumerate(data_test):\n",
    "        dataset = data_test[dataset_name]\n",
    "\n",
    "        ground_truth = ground_truth + generate_ground_truth(dataset)\n",
    "\n",
    "        losses = losses + generate_losses(dataset, net, loss_function)\n",
    "\n",
    "    y_hat = generate_y_hat(losses, loss_threshold)\n",
    "\n",
    "    return confusion_matrix(ground_truth, y_hat, normalize='true'), losses, ground_truth, y_hat\n",
    "\n",
    "def generate_encode_decode_layers(layers, output_layer):\n",
    "    od_encode = []\n",
    "    od_decode = []\n",
    "    # encode\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_encode.append(('l'+str((len(od_encode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_encode.append(('l'+str((len(od_encode)+1)), nn.ReLU()))\n",
    "\n",
    "    # decode\n",
    "    layers.reverse()\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_decode.append(('l'+str((len(od_decode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), nn.ReLU()))\n",
    "        else:\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), output_layer))\n",
    "\n",
    "    return OrderedDict(od_encode), OrderedDict(od_decode)\n",
    "\n",
    "def train(layers, last_layer, lr, epochs, batch_size, X_train, optim, loss_fnc, net = []):\n",
    "    encode_l, decode_l = generate_encode_decode_layers(layers, last_layer)\n",
    "\n",
    "    if (net == []):\n",
    "        net = Autoencoder()\n",
    "        '''net = Autoencoder(encode_l, decode_l)'''\n",
    "\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "\n",
    "    if (optim == 'ADAM'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif(optim == 'SGD'):\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    elif(optim == 'RMSprop'):\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    torch.manual_seed(111)\n",
    "\n",
    "    # sets\n",
    "    train_set = [\n",
    "        (X_train, X_train) for i in range(len(X_train))\n",
    "    ]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    net, output, loss, losses = run_train(net, train_loader, epochs, optimizer, loss_fnc)\n",
    "\n",
    "    return net, output, loss, losses, loss_fnc\n",
    "\n",
    "def test(X_train, X_test, net, loss_function, phi = 0):\n",
    "\n",
    "    faults = {'F1': data_ds3_fault1[:LIMITADOR],\n",
    "              'F2': data_ds3_fault2[:LIMITADOR],\n",
    "              'F3': data_ds3_fault3[:LIMITADOR],\n",
    "              'F4': data_ds3_fault4[:LIMITADOR]}\n",
    "\n",
    "    # CONFUSION MATRIX FOR X_TRAIN\n",
    "    losses = generate_losses(X_train, net, loss_function)\n",
    "    np_losses = np.sort(np.array(losses))\n",
    "    losses = np_losses[:int(len(np_losses)*LOSS_FACTOR)]\n",
    "    phi_test = np.mean(losses, dtype=np.float64) + np.std(losses, ddof=1, dtype=np.float64)\n",
    "\n",
    "    if phi == 0:\n",
    "        phi = phi_test\n",
    "\n",
    "    print(\"phi_full: \", phi_test, file=log)\n",
    "    print(\"phi_step: \", phi, file=log)\n",
    "\n",
    "    # CONFUSION MATRIX FOR NORMAL_2\n",
    "    losses = generate_losses(data_ds3_t2_normal[:LIMITADOR], net, loss_function)\n",
    "    y_hat_n2 = generate_y_hat(losses, phi)\n",
    "    print('Normal 2 hit :', len([0 for t in y_hat_n2 if t == 0])/len(y_hat_n2))\n",
    "\n",
    "    for n, fault_name in enumerate(faults):\n",
    "        losses = generate_losses(faults[fault_name][:LIMITADOR], net, loss_function)\n",
    "        y_hat = generate_y_hat(losses, phi)\n",
    "        print(fault_name + ' hit :', len([1 for t in y_hat if t == 1])/len(generate_y_hat(losses, phi)))\n",
    "\n",
    "    return y_hat_n2, phi\n",
    "\n",
    "class Incremental_Stats():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.mu = 0.0\n",
    "        self.var = 0.0\n",
    "\n",
    "    def add(self, x):\n",
    "        if self.count == 0:\n",
    "            new_mu = x\n",
    "            new_var = 0\n",
    "        else:\n",
    "            new_mu = (self.count * self.mu + x) / (self.count + 1)\n",
    "            new_var = ((self.count + 1) / self.count) * (((self.count * self.var) / (self.count + 1)) + (((x - new_mu)**2) / self.count))\n",
    "\n",
    "        self.count = self.count+1\n",
    "        self.mu = new_mu\n",
    "        self.var = new_var\n",
    "\n",
    "        return self.mu, self.var, self.count\n",
    "\n",
    "    def get_mean(self):\n",
    "        return self.mu\n",
    "\n",
    "    def get_variance(self):\n",
    "        return self.var\n",
    "\n",
    "    def get_std(self):\n",
    "        return self.var**0.5\n",
    "\n",
    "    def get_n(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_ds3_t1_normal[:, :-1], data_ds3_t1_normal[:, -1], test_size=1-TRAIN_SIZE, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBpHJcvJBg75",
    "outputId": "1c754107-785f-459d-ee5b-4dc524e468b7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ HYPERPARAMETERS ************\n",
      "LIMITER     : 20000\n",
      "LOSS FACTOR : 0.96\n",
      "[54, 32, 9]\n",
      "Tanh()\n",
      "32\n",
      "RMSprop\n",
      "L1Loss()\n",
      ".phi: 0.12458635450553127\n",
      ".phi: 0.12333840381209206\n",
      ".phi: 0.10401053837693275\n",
      ".phi: 0.07594055611606934\n",
      ".phi: 0.06903621211985375\n",
      ".phi: 0.05249702413015703\n",
      ".phi: 0.05037288863225624\n",
      ".phi: 0.05238376668147732\n",
      ".phi: 0.06431533322627478\n",
      ".phi: 0.048644381437628045\n",
      ".phi: 0.05431889207008923\n",
      ".phi: 0.08311589505702362\n",
      ".phi: 0.05782066838343851\n",
      ".phi: 0.06288807208851334\n",
      ".phi: 0.06005841486648893\n",
      ".phi: 0.05618768782099826\n",
      ".phi: 0.05876458592104486\n",
      ".phi: 0.05615278092737651\n",
      ".phi: 0.050605167150092126\n",
      ".phi: 0.0486181789168065\n",
      ".phi: 0.04999599414162257\n",
      ".phi: 0.04473757042958891\n",
      ".phi: 0.05466548112267809\n",
      ".phi: 0.0422608606589862\n",
      ".phi: 0.047354688646212666\n",
      ".phi: 0.07158334269170451\n",
      ".phi: 0.03434020085579869\n",
      ".phi: 0.04826904589528434\n",
      ".phi: 0.04042012381772025\n",
      ".phi: 0.04897038275879831\n",
      ".phi: 0.05374730963635747\n",
      ".phi: 0.03976445352522302\n",
      ".phi: 0.04755682883411544\n",
      ".phi: 0.03774375968667017\n",
      ".phi: 0.043956880294649336\n",
      ".phi: 0.03778959317299076\n",
      ".phi: 0.06434167556832306\n",
      ".phi: 0.0461135619368042\n",
      ".phi: 0.04255157757034128\n",
      ".phi: 0.04828355135425548\n",
      ".phi: 0.052220325057610775\n",
      ".phi: 0.03959887221456023\n",
      ".phi: 0.04313433066309349\n",
      ".phi: 0.03327597803685088\n",
      ".phi: 0.052908404917485156\n",
      ".phi: 0.043299583899811284\n",
      ".phi: 0.042336904319618285\n",
      ".phi: 0.05942463809004085\n",
      ".phi: 0.054599482848994525\n",
      ".phi: 0.05295348641389998\n",
      ".phi: 0.03671130150841157\n",
      ".phi: 0.04534142866872354\n",
      ".phi: 0.046959717733735695\n",
      ".phi: 0.0493929800990492\n",
      ".phi: 0.04663122581814898\n",
      ".phi: 0.04451970313333694\n",
      ".phi: 0.032597765215653225\n",
      ".phi: 0.05127856768028334\n",
      ".phi: 0.04289226103111339\n",
      ".phi: 0.048269205174539254\n",
      ".phi: 0.053863449155280296\n",
      ".phi: 0.034660980194361646\n",
      ".phi: 0.04804816393435497\n",
      ".phi: 0.0472587763119601\n",
      ".phi: 0.0465679956200362\n",
      ".phi: 0.05176060862181409\n",
      ".phi: 0.0445192314207407\n",
      ".phi: 0.03984695575137471\n",
      ".phi: 0.045874615288667236\n",
      ".phi: 0.05543812687154635\n",
      ".phi: 0.033974440450284596\n",
      ".phi: 0.038150824333852355\n",
      ".phi: 0.037178010395547856\n",
      ".phi: 0.040761526437997575\n",
      ".phi: 0.049740481710162564\n",
      ".phi: 0.026261541867973755\n",
      ".phi: 0.031122671973328235\n",
      ".phi: 0.04783006537057054\n",
      ".phi: 0.04026962658200146\n",
      ".phi: 0.04948095352161558\n",
      ".phi: 0.05603316286081317\n",
      ".phi: 0.040810610138506426\n",
      ".phi: 0.03715777526569634\n",
      ".phi: 0.02780011015052778\n",
      ".phi: 0.046706544928744015\n",
      ".phi: 0.04581284881367871\n",
      ".phi: 0.0467477332790798\n",
      ".phi: 0.040210830795822876\n",
      ".phi: 0.035661199529361684\n",
      ".phi: 0.0359313822250727\n",
      ".phi: 0.05901594361443954\n",
      ".phi: 0.050564879355995754\n",
      ".phi: 0.03942693513892962\n",
      ".phi: 0.032560482401261345\n",
      ".phi: 0.04765194610314389\n",
      ".phi: 0.033674000077284084\n",
      ".phi: 0.03594461770002198\n",
      ".phi: 0.05590015016723504\n",
      ".phi: 0.048628673957254495\n",
      ".phi: 0.034481300495248245\n",
      ".phi: 0.033133384519887316\n",
      ".phi: 0.03783084027840362\n",
      ".phi: 0.04242492083756862\n",
      ".phi: 0.033484092127419915\n",
      ".phi: 0.038830681284063064\n",
      ".phi: 0.0420038641051583\n",
      ".phi: 0.03834077573150149\n",
      ".phi: 0.05371305688744547\n",
      ".phi: 0.05153032630375429\n",
      ".phi: 0.04118739033027935\n",
      ".phi: 0.028363772540258663\n",
      ".phi: 0.04500503601073108\n",
      ".phi: 0.03729589934084547\n",
      ".phi: 0.03659448202236415\n",
      ".phi: 0.038915887890246546\n",
      ".phi: 0.045573935281814595\n",
      ".phi: 0.030268701159128192\n",
      ".phi: 0.03704252793456968\n",
      ".phi: 0.03696991687804267\n",
      ".phi: 0.04115164009624178\n",
      ".phi: 0.03942898333573078\n",
      ".phi: 0.0436300613742606\n",
      ".phi: 0.0331773110181741\n",
      ".phi: 0.04350799873798723\n",
      ".phi: 0.03166253676787336\n",
      ".phi: 0.028697774097757596\n",
      ".phi: 0.051853495181467486\n",
      ".phi: 0.04912029207735258\n",
      ".phi: 0.039911775408604515\n",
      ".phi: 0.034939941023740054\n",
      ".phi: 0.03478153478440805\n",
      ".phi: 0.04212952825279021\n",
      ".phi: 0.044285307152847994\n",
      ".phi: 0.04358473305866306\n",
      ".phi: 0.040767562974221995\n",
      ".phi: 0.04476265316548743\n",
      ".phi: 0.04182405459328675\n",
      ".phi: 0.04303330069586514\n",
      ".phi: 0.03464605663349035\n",
      ".phi: 0.04499522193824937\n",
      ".phi: 0.03345549097978966\n",
      ".phi: 0.03042030766554772\n",
      ".phi: 0.055794848126117375\n",
      ".phi: 0.044130547209882215\n"
     ]
    },
    {
     "data": {
      "text/plain": "                          0         1         2         3         4    \\\nmu_full              0.000000  0.000000  0.000000  0.000000  0.000000   \nmu_step              0.059666  0.053358  0.042612  0.031436  0.029232   \nmu_agg               0.000000  0.000000  0.000000  0.000000  0.000000   \nmu_online            0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_full             0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_step             0.064920  0.069981  0.061398  0.044504  0.039804   \nstd_agg              0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_online           0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_full             0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_step             0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_agg              0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_online           0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_full      0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_step      0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_agg       0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_online    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_full    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_step    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_agg     0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_online  0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \n\n                          5         6         7         8         9    ...  \\\nmu_full              0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nmu_step              0.023912  0.023555  0.021563  0.024915  0.019593  ...   \nmu_agg               0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nmu_online            0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nstd_full             0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nstd_step             0.028585  0.026818  0.030821  0.039400  0.029051  ...   \nstd_agg              0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nstd_online           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nphi_full             0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nphi_step             0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nphi_agg              0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nphi_online           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nx_test_hit_full      0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nx_test_hit_step      0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nx_test_hit_agg       0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nx_test_hit_online    0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nnormal_2_hit_full    0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nnormal_2_hit_step    0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nnormal_2_hit_agg     0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nnormal_2_hit_online  0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf1_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf2_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf3_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf4_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf1_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf2_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf3_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf4_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf1_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf2_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf3_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf4_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf1_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf2_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf3_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  ...   \nf4_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  ...   \n\n                          134       135       136       137       138  \\\nmu_full              0.000000  0.000000  0.000000  0.000000  0.000000   \nmu_step              0.012975  0.013760  0.012155  0.013508  0.011582   \nmu_agg               0.000000  0.000000  0.000000  0.000000  0.000000   \nmu_online            0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_full             0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_step             0.027792  0.031003  0.029669  0.029526  0.023064   \nstd_agg              0.000000  0.000000  0.000000  0.000000  0.000000   \nstd_online           0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_full             0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_step             0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_agg              0.000000  0.000000  0.000000  0.000000  0.000000   \nphi_online           0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_full      0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_step      0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_agg       0.000000  0.000000  0.000000  0.000000  0.000000   \nx_test_hit_online    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_full    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_step    0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_agg     0.000000  0.000000  0.000000  0.000000  0.000000   \nnormal_2_hit_online  0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000   \nf1_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf2_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf3_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \nf4_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000   \n\n                          139       140       141       142       143  \nmu_full              0.000000  0.000000  0.000000  0.000000  0.000000  \nmu_step              0.014007  0.011525  0.010555  0.016550  0.013721  \nmu_agg               0.000000  0.000000  0.000000  0.000000  0.000000  \nmu_online            0.000000  0.000000  0.000000  0.000000  0.000000  \nstd_full             0.000000  0.000000  0.000000  0.000000  0.000000  \nstd_step             0.030988  0.021930  0.019865  0.039245  0.030409  \nstd_agg              0.000000  0.000000  0.000000  0.000000  0.000000  \nstd_online           0.000000  0.000000  0.000000  0.000000  0.000000  \nphi_full             0.000000  0.000000  0.000000  0.000000  0.000000  \nphi_step             0.000000  0.000000  0.000000  0.000000  0.000000  \nphi_agg              0.000000  0.000000  0.000000  0.000000  0.000000  \nphi_online           0.000000  0.000000  0.000000  0.000000  0.000000  \nx_test_hit_full      0.000000  0.000000  0.000000  0.000000  0.000000  \nx_test_hit_step      0.000000  0.000000  0.000000  0.000000  0.000000  \nx_test_hit_agg       0.000000  0.000000  0.000000  0.000000  0.000000  \nx_test_hit_online    0.000000  0.000000  0.000000  0.000000  0.000000  \nnormal_2_hit_full    0.000000  0.000000  0.000000  0.000000  0.000000  \nnormal_2_hit_step    0.000000  0.000000  0.000000  0.000000  0.000000  \nnormal_2_hit_agg     0.000000  0.000000  0.000000  0.000000  0.000000  \nnormal_2_hit_online  0.000000  0.000000  0.000000  0.000000  0.000000  \nf1_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  \nf2_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  \nf3_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  \nf4_hit_full          0.000000  0.000000  0.000000  0.000000  0.000000  \nf1_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  \nf2_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  \nf3_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  \nf4_hit_step          0.000000  0.000000  0.000000  0.000000  0.000000  \nf1_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  \nf2_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  \nf3_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  \nf4_hit_agg           0.000000  0.000000  0.000000  0.000000  0.000000  \nf1_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  \nf2_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  \nf3_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  \nf4_hit_online        0.000000  0.000000  0.000000  0.000000  0.000000  \n\n[36 rows x 144 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>134</th>\n      <th>135</th>\n      <th>136</th>\n      <th>137</th>\n      <th>138</th>\n      <th>139</th>\n      <th>140</th>\n      <th>141</th>\n      <th>142</th>\n      <th>143</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mu_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>mu_step</th>\n      <td>0.059666</td>\n      <td>0.053358</td>\n      <td>0.042612</td>\n      <td>0.031436</td>\n      <td>0.029232</td>\n      <td>0.023912</td>\n      <td>0.023555</td>\n      <td>0.021563</td>\n      <td>0.024915</td>\n      <td>0.019593</td>\n      <td>...</td>\n      <td>0.012975</td>\n      <td>0.013760</td>\n      <td>0.012155</td>\n      <td>0.013508</td>\n      <td>0.011582</td>\n      <td>0.014007</td>\n      <td>0.011525</td>\n      <td>0.010555</td>\n      <td>0.016550</td>\n      <td>0.013721</td>\n    </tr>\n    <tr>\n      <th>mu_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>mu_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>std_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>std_step</th>\n      <td>0.064920</td>\n      <td>0.069981</td>\n      <td>0.061398</td>\n      <td>0.044504</td>\n      <td>0.039804</td>\n      <td>0.028585</td>\n      <td>0.026818</td>\n      <td>0.030821</td>\n      <td>0.039400</td>\n      <td>0.029051</td>\n      <td>...</td>\n      <td>0.027792</td>\n      <td>0.031003</td>\n      <td>0.029669</td>\n      <td>0.029526</td>\n      <td>0.023064</td>\n      <td>0.030988</td>\n      <td>0.021930</td>\n      <td>0.019865</td>\n      <td>0.039245</td>\n      <td>0.030409</td>\n    </tr>\n    <tr>\n      <th>std_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>std_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>phi_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>phi_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>phi_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>phi_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>x_test_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>x_test_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>x_test_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>x_test_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>normal_2_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>normal_2_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>normal_2_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>normal_2_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f1_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f2_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f3_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f4_hit_full</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f1_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f2_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f3_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f4_hit_step</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f1_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f2_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f3_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f4_hit_agg</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f1_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f2_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f3_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>f4_hit_online</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>36 rows × 144 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "architecture = [54, 32, 9]\n",
    "last_layer = nn.Tanh()\n",
    "batch_size = 32\n",
    "optim = 'RMSprop'\n",
    "loss_fnc = nn.L1Loss()\n",
    "\n",
    "EPOCHS = 1\n",
    "block_size = 1000\n",
    "log = None\n",
    "\n",
    "block_train = int(X_train.shape[0] / block_size)\n",
    "net = []\n",
    "agg_loss = []\n",
    "count = 0\n",
    "\n",
    "print('************ HYPERPARAMETERS ************', file=log)\n",
    "print('LIMITER     :', LIMITADOR, file=log)\n",
    "print('LOSS FACTOR :', LOSS_FACTOR, file=log)\n",
    "print(architecture, file=log)\n",
    "print(last_layer, file=log)\n",
    "print(batch_size, file=log)\n",
    "print(optim, file=log)\n",
    "print(loss_fnc, file=log)\n",
    "\n",
    "values = np.zeros((block_train, 36))\n",
    "columns = ['mu_full', 'mu_step', 'mu_agg', 'mu_online', 'std_full',\n",
    "           'std_step', 'std_agg', 'std_online', 'phi_full', 'phi_step', 'phi_agg', 'phi_online',\n",
    "           'x_test_hit_full', 'normal_2_hit_full', 'f1_hit_full', 'f2_hit_full', 'f3_hit_full', 'f4_hit_full',\n",
    "           'x_test_hit_step', 'normal_2_hit_step', 'f1_hit_step', 'f2_hit_step', 'f3_hit_step', 'f4_hit_step',\n",
    "           'x_test_hit_agg', 'normal_2_hit_agg', 'f1_hit_agg', 'f2_hit_agg', 'f3_hit_agg', 'f4_hit_agg',\n",
    "           'x_test_hit_online', 'normal_2_hit_online', 'f1_hit_online', 'f2_hit_online', 'f3_hit_online', 'f4_hit_online'] #noorder\n",
    "\n",
    "\n",
    "columns_order = ['mu_full', 'mu_step', 'mu_agg', 'mu_online', 'std_full',\n",
    "                 'std_step', 'std_agg', 'std_online', 'phi_full', 'phi_step', 'phi_agg',\n",
    "                 'phi_online', 'x_test_hit_full', 'x_test_hit_step', 'x_test_hit_agg', 'x_test_hit_online',\n",
    "                 'normal_2_hit_full', 'normal_2_hit_step', 'normal_2_hit_agg', 'normal_2_hit_online',\n",
    "                 'f1_hit_full', 'f2_hit_full', 'f3_hit_full', 'f4_hit_full', 'f1_hit_step', 'f2_hit_step',\n",
    "                 'f3_hit_step', 'f4_hit_step', 'f1_hit_agg', 'f2_hit_agg', 'f3_hit_agg', 'f4_hit_agg',\n",
    "                 'f1_hit_online', 'f2_hit_online', 'f3_hit_online', 'f4_hit_online']\n",
    "\n",
    "stats = Incremental_Stats()\n",
    "\n",
    "for n in np.arange(0, block_train):\n",
    "\n",
    "    count = count+1\n",
    "    print('.', end='')\n",
    "\n",
    "    if block_size*(n+1) < X_train.shape[0]:\n",
    "        data = X_train[block_size * n : block_size*(n+1)]\n",
    "    else:\n",
    "        data = X_train[block_size * n:]\n",
    "\n",
    "    net, output, loss, losses, loss_function = train(layers=architecture.copy(), last_layer=last_layer, lr=1e-3, epochs=EPOCHS, batch_size=batch_size, X_train=data, optim=optim, loss_fnc=loss_fnc, net=net)\n",
    "\n",
    "    '''losses_geral_step = generate_losses(data_test=X_train, net=net, loss_function=loss_fnc)\n",
    "    np_losses = np.sort(np.array(losses_geral_step))\n",
    "    losses_geral_step = np_losses[:int(len(np_losses)*LOSS_FACTOR)]\n",
    "    values[n][0] = np.mean(losses_geral_step, dtype=np.float64)\n",
    "    values[n][4] = np.std(losses_geral_step, ddof=1, dtype=np.float64)\n",
    "    phi_full = np.mean(losses_geral_step, dtype=np.float64) + np.std(losses_geral_step, ddof=1, dtype=np.float64)'''\n",
    "\n",
    "    losses_batch_step_src = generate_losses(data_test=data, net=net, loss_function=loss_fnc)\n",
    "    np_losses = np.sort(np.array(losses_batch_step_src))\n",
    "    losses_batch_step = np_losses[:int(len(np_losses)*LOSS_FACTOR)]\n",
    "    values[n][1] = np.mean(losses_batch_step, dtype=np.float64)\n",
    "    values[n][5] = np.std(losses_batch_step, ddof=1, dtype=np.float64)\n",
    "    phi_batch_step = np.mean(losses_batch_step, dtype=np.float64) + np.std(losses_batch_step, ddof=1, dtype=np.float64)\n",
    "    print(\"phi:\", phi_batch_step)\n",
    "    '''agg_loss = np.concatenate((agg_loss, losses_batch_step_src), axis = 0)\n",
    "    np_losses = np.sort(np.array(agg_loss))\n",
    "    agg_loss = np_losses[:int(len(np_losses)*LOSS_FACTOR)]\n",
    "    values[n][2] = np.mean(agg_loss, dtype=np.float64)\n",
    "    values[n][6] = np.std(agg_loss, ddof=1, dtype=np.float64)\n",
    "    phi_aggregate = np.mean(agg_loss, dtype=np.float64) + np.std(agg_loss, ddof=1, dtype=np.float64)'''\n",
    "\n",
    "    '''for loss_step in losses_batch_step:\n",
    "        stats.add(loss_step)\n",
    "\n",
    "    values[n][3] = stats.get_mean()\n",
    "    values[n][7] = stats.get_std()\n",
    "    phi_online = stats.get_mean() + stats.get_std()\n",
    "\n",
    "    values[n][8] = phi_full\n",
    "    values[n][9] = phi_batch_step\n",
    "    values[n][10] = phi_aggregate\n",
    "    values[n][11] = phi_online\n",
    "\n",
    "    faults = {'F1': data_ds3_fault1[:LIMITADOR],\n",
    "              'F2': data_ds3_fault2[:LIMITADOR],\n",
    "              'F3': data_ds3_fault3[:LIMITADOR],\n",
    "              'F4': data_ds3_fault4[:LIMITADOR]}'''\n",
    "\n",
    "    position = 0\n",
    "    '''for p, phi in enumerate([phi_full, phi_batch_step, phi_aggregate, phi_online]):\n",
    "        # FOR X_TEST\n",
    "        losses = generate_losses(X_test[:LIMITADOR], net, loss_fnc)\n",
    "        y_hat_n2 = generate_y_hat(losses, phi)\n",
    "        values[n][12+position] = len([0 for t in y_hat_n2 if t == 0])/len(y_hat_n2)\n",
    "        position = position + 1\n",
    "\n",
    "        # FOR NORMAL_2\n",
    "        losses = generate_losses(data_ds3_t2_normal[:LIMITADOR], net, loss_fnc)\n",
    "        y_hat_n2 = generate_y_hat(losses, phi)\n",
    "        values[n][12+position] = len([0 for t in y_hat_n2 if t == 0])/len(y_hat_n2)\n",
    "        position = position + 1\n",
    "\n",
    "        for c, fault_name in enumerate(faults):\n",
    "            losses = generate_losses(faults[fault_name][:LIMITADOR], net, loss_fnc)\n",
    "            y_hat = generate_y_hat(losses, phi)\n",
    "            values[n][12+position] = len([1 for t in y_hat if t == 1])/len(y_hat)\n",
    "            position = position + 1'''\n",
    "\n",
    "    detector_metadata.update_one({\"_id\" : meta[0][\"_id\"]},\n",
    "                                 {\"$set\": {\"threshold\": phi_batch_step}})\n",
    "\n",
    "    torch.save(net.state_dict(), '../app/autoencoder_model.pt')\n",
    "    pickle.dump(ss, open('../app/scaler.pkl','wb'))\n",
    "\n",
    "values_pd = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "values_pd[columns_order].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "pickle.dump(ss, open('../app/scaler.pkl','wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027232205029577017\n"
     ]
    }
   ],
   "source": [
    "_x = data_ds3_t1_normal[552:553, :-1]\n",
    "real = _x.reshape(1,-1).astype(np.float32)\n",
    "\n",
    "regenerate = net(torch.from_numpy(real))\n",
    "l = loss_function(regenerate, torch.from_numpy(real)).item()\n",
    "\n",
    "print(l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE_NAME,'a') as f:\n",
    "    f.write('LOSS FACTOR: ' + str(LOSS_FACTOR) + '\\n')\n",
    "    f.write('ARC: ' + str(architecture) + '\\n')\n",
    "    f.write('LAST LAYER: ' + str(last_layer) + '\\n')\n",
    "    f.write('BATCH: ' + str(batch_size) + '\\n')\n",
    "    f.write('OPTIM: ' + str(optim) + '\\n')\n",
    "    f.write('LOSS FNC: ' + str(loss_fnc) + '\\n')\n",
    "    np.savetxt(f, np.array(columns_order).reshape(1,-1), delimiter=';', fmt='%s')\n",
    "    np.savetxt(f, values_pd[columns_order].values, delimiter=';', fmt='%f')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}