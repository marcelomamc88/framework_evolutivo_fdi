{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iCIoMaMTBg7l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "fdi_db = MongoClient('localhost', 27017).fdi\n",
    "detector_metadata = fdi_db.detector_metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('6240f18b08d0c9c605fbeba5'), 'threshold': 0.13679087517555244}]\n"
     ]
    }
   ],
   "source": [
    "meta = list(detector_metadata.find({}))\n",
    "print(meta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = .96\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../../dataset/original/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CILVSdmV43uh",
    "outputId": "ccfdfad7-dc42-41e9-9c07-d4cfe527aa98"
   },
   "outputs": [],
   "source": [
    "dict_ds_original = {\n",
    "    'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l5N9KYHoBg71"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "data_ds3_t1_normal = dict_ds['data_ds3_normal_t1_original']\n",
    "\n",
    "# fit values\n",
    "ss.partial_fit(data_ds3_t1_normal)\n",
    "\n",
    "# transform values\n",
    "data_ds3_t1_normal = ss.transform(data_ds3_t1_normal)\n",
    "\n",
    "# append normal labels\n",
    "data_ds3_t1_normal = np.append(data_ds3_t1_normal, np.zeros((data_ds3_t1_normal.shape[0],1)), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aCbDYmvL43u2"
   },
   "outputs": [],
   "source": [
    "dimension = data_ds3_t1_normal.shape[1]-1\n",
    "\n",
    "# FUNCTIONS AND CLASSES\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encode_l, decode_l):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(encode_l)\n",
    "        self.decoder = nn.Sequential(decode_l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def run_train(net, train_loader, num_epochs, optimizer, loss_func):\n",
    "    train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        losses = []\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            ### forward ###\n",
    "            if cuda:\n",
    "                output = net(real_samples.type(torch.FloatTensor).cuda())\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor).cuda())\n",
    "            else:\n",
    "                output = net(real_samples.type(torch.FloatTensor))\n",
    "                loss = loss_func(output, real_samples.type(torch.FloatTensor))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item)\n",
    "\n",
    "            ### backward ###\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        step_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(step_loss)\n",
    "\n",
    "        ### log ###\n",
    "        #print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss))\n",
    "\n",
    "    return net, output, train_loss, losses\n",
    "\n",
    "def generate_ground_truth(data_test):\n",
    "    ground_truth = []\n",
    "\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] == dimension: # X_test\n",
    "            ground_truth.append(0)\n",
    "        else: # % others: get column label\n",
    "            if x[-1] != 0:\n",
    "                ground_truth.append(1)\n",
    "            else:\n",
    "                ground_truth.append(0)\n",
    "\n",
    "    return ground_truth\n",
    "\n",
    "def generate_losses(data_test, net, loss_function):\n",
    "    losses = []\n",
    "    #regenerate_data = np.zeros((1, dimension))\n",
    "    for j, x in enumerate(data_test):\n",
    "\n",
    "        if x.shape[0] > dimension:\n",
    "            x = x[:-1]\n",
    "\n",
    "        real = x.reshape(1,-1).astype(np.float32)\n",
    "\n",
    "        #regenerate_data[j] = regenerate.cpu().detach().numpy()\n",
    "\n",
    "        if cuda:\n",
    "            regenerate = net(torch.from_numpy(real).cuda())\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real).cuda()).item()\n",
    "        else:\n",
    "            regenerate = net(torch.from_numpy(real))\n",
    "            loss_ae = loss_function(regenerate, torch.from_numpy(real)).item()\n",
    "\n",
    "        losses.append(loss_ae)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def generate_y_hat(losses, loss_threshold):\n",
    "    y_hat = []\n",
    "\n",
    "    for l in losses:\n",
    "        if l < loss_threshold:\n",
    "            y_hat.append(0)\n",
    "        else:\n",
    "            y_hat.append(1)\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def tester(data_test, net, loss_function, loss_threshold = 1):\n",
    "\n",
    "    ground_truth = []\n",
    "    losses = []\n",
    "\n",
    "    for n, dataset_name in enumerate(data_test):\n",
    "        dataset = data_test[dataset_name]\n",
    "\n",
    "        ground_truth = ground_truth + generate_ground_truth(dataset)\n",
    "\n",
    "        losses = losses + generate_losses(dataset, net, loss_function)\n",
    "\n",
    "    y_hat = generate_y_hat(losses, loss_threshold)\n",
    "\n",
    "    return confusion_matrix(ground_truth, y_hat, normalize='true'), losses, ground_truth, y_hat\n",
    "\n",
    "def generate_encode_decode_layers(layers, output_layer):\n",
    "    od_encode = []\n",
    "    od_decode = []\n",
    "    # encode\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_encode.append(('l'+str((len(od_encode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_encode.append(('l'+str((len(od_encode)+1)), nn.ReLU()))\n",
    "\n",
    "    # decode\n",
    "    layers.reverse()\n",
    "    for _, layer in enumerate(layers):\n",
    "        n = _ + 1\n",
    "\n",
    "        if (len(layers) == n):\n",
    "            break\n",
    "\n",
    "        od_decode.append(('l'+str((len(od_decode)+1)), nn.Linear(layers[_],layers[n])))\n",
    "\n",
    "        if (len(layers) != n+1):\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), nn.ReLU()))\n",
    "        else:\n",
    "            od_decode.append(('l'+str((len(od_decode)+1)), output_layer))\n",
    "\n",
    "    return OrderedDict(od_encode), OrderedDict(od_decode)\n",
    "\n",
    "def train(layers, last_layer, lr, epochs, batch_size, X_train, optim, loss_fnc, net = []):\n",
    "    encode_l, decode_l = generate_encode_decode_layers(layers, last_layer)\n",
    "\n",
    "    if (net == []):\n",
    "        net = Autoencoder(encode_l, decode_l)\n",
    "\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "\n",
    "    if (optim == 'ADAM'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif(optim == 'SGD'):\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    elif(optim == 'RMSprop'):\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    torch.manual_seed(111)\n",
    "\n",
    "    # sets\n",
    "    train_set = [\n",
    "        (X_train, X_train) for i in range(len(X_train))\n",
    "    ]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    net, output, loss, losses = run_train(net, train_loader, epochs, optimizer, loss_fnc)\n",
    "\n",
    "    return net, output, loss, losses, loss_fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_ds3_t1_normal[:, :-1], data_ds3_t1_normal[:, -1], test_size=1-TRAIN_SIZE, random_state=42, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBpHJcvJBg75",
    "outputId": "1c754107-785f-459d-ee5b-4dc524e468b7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".phi: 0.2969941943292295\n",
      ".phi: 0.19515313703368675\n",
      ".phi: 0.18028076762194914\n",
      ".phi: 0.12488509613060511\n",
      ".phi: 0.13076750638562645\n",
      ".phi: 0.17554508145248443\n",
      ".phi: 0.18885256351303725\n",
      ".phi: 0.14012438813752456\n",
      ".phi: 0.16858088908191013\n",
      ".phi: 0.13440636290710278\n",
      ".phi: 0.1788091194091116\n",
      ".phi: 0.2621101106345405\n",
      ".phi: 0.17586927895139368\n",
      ".phi: 0.13271064897370474\n",
      ".phi: 0.06616884569958618\n",
      ".phi: 0.14394144175795046\n",
      ".phi: 0.10401891783247258\n",
      ".phi: 0.08571296871560836\n",
      ".phi: 0.0784030742355902\n",
      ".phi: 0.059181556625000895\n",
      ".phi: 0.09111931257224157\n",
      ".phi: 0.11318406010056314\n",
      ".phi: 0.12089598157555027\n",
      ".phi: 0.0962307704563925\n",
      ".phi: 0.08317586455717778\n",
      ".phi: 0.10748307679591465\n",
      ".phi: 0.11250507691112879\n",
      ".phi: 0.13636472962367446\n",
      ".phi: 0.09776424589473351\n",
      ".phi: 0.4913555399283283\n",
      ".phi: 0.09334189070944353\n",
      ".phi: 0.05631887732979393\n",
      ".phi: 0.15906892703675138\n",
      ".phi: 0.17471327724418367\n",
      ".phi: 0.1333009794152476\n",
      ".phi: 0.2062370123285352\n",
      ".phi: 0.16324255190147902\n",
      ".phi: 0.062323119125353085\n",
      ".phi: 0.13762233708250926\n",
      ".phi: 0.04359786997426966\n",
      ".phi: 0.2232683827620402\n",
      ".phi: 0.7593884679685339\n",
      ".phi: 0.22744063822291832\n",
      ".phi: 0.14998871318924595\n",
      ".phi: 0.05826530507083719\n",
      ".phi: 0.13278326503113713\n",
      ".phi: 0.06012439242152158\n",
      ".phi: 0.10355325305107468\n",
      ".phi: 0.07236042727147646\n",
      ".phi: 0.09003988216389358\n",
      ".phi: 0.08919680287533256\n",
      ".phi: 0.11081457268116623\n",
      ".phi: 0.10680318477554882\n",
      ".phi: 0.07096308911817162\n",
      ".phi: 0.07589573267055563\n",
      ".phi: 0.11750126272560685\n",
      ".phi: 0.060986471460017636\n",
      ".phi: 0.06803303351002273\n",
      ".phi: 0.15164086470820154\n",
      ".phi: 0.102552510139511\n",
      ".phi: 0.594292361706977\n",
      ".phi: 0.0990859826063307\n",
      ".phi: 0.12811555362314442\n",
      ".phi: 0.0661098610072652\n",
      ".phi: 0.14607844456645042\n",
      ".phi: 0.1515484783800733\n",
      ".phi: 0.1085300589305104\n",
      ".phi: 0.18480575098859273\n",
      ".phi: 0.1344798755530046\n",
      ".phi: 0.13366087680338307\n",
      ".phi: 0.09965731488154868\n",
      ".phi: 0.07576549463812174\n",
      ".phi: 0.097182885126995\n",
      ".phi: 0.12441794729028818\n",
      ".phi: 0.16960834640428396\n",
      ".phi: 0.101258078646107\n",
      ".phi: 0.14904678093829238\n",
      ".phi: 0.07727127955681726\n",
      ".phi: 0.11378991224396393\n",
      ".phi: 0.1681937533053529\n",
      ".phi: 0.11926813598681762\n",
      ".phi: 0.13716018366290822\n",
      ".phi: 0.37634750877576406\n",
      ".phi: 0.08975419494841898\n",
      ".phi: 0.3238248764757183\n",
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-20-313e0ef6aeaa>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mblock_size\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m     \u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0marchitecture\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_layer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlast_layer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1e-3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fnc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mloss_fnc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[0mlosses_batch_step_src\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgenerate_losses\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_test\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mloss_fnc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-17-6ded9c7b12ec>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(layers, last_layer, lr, epochs, batch_size, X_train, optim, loss_fnc, net)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    169\u001B[0m     \u001B[1;31m# train\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m     \u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fnc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fnc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-17-6ded9c7b12ec>\u001B[0m in \u001B[0;36mrun_train\u001B[1;34m(net, train_loader, num_epochs, optimizer, loss_func)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m             \u001B[1;31m### backward ###\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    154\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 156\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "architecture = [54, 32, 9]\n",
    "last_layer = nn.Tanh()\n",
    "batch_size = 32\n",
    "optim = 'RMSprop'\n",
    "loss_fnc = nn.L1Loss()\n",
    "\n",
    "EPOCHS = 1\n",
    "block_size = 1000\n",
    "log = None\n",
    "\n",
    "block_train = int(X_train.shape[0] / block_size)\n",
    "net = []\n",
    "agg_loss = []\n",
    "count = 0\n",
    "\n",
    "for n in np.arange(0, block_train):\n",
    "\n",
    "    count = count+1\n",
    "    print('.', end='')\n",
    "\n",
    "    if block_size*(n+1) < X_train.shape[0]:\n",
    "        data = X_train[block_size * n : block_size*(n+1)]\n",
    "    else:\n",
    "        data = X_train[block_size * n:]\n",
    "\n",
    "    net, output, loss, losses, loss_function = train(layers=architecture.copy(), last_layer=last_layer, lr=1e-3, epochs=EPOCHS, batch_size=batch_size, X_train=data, optim=optim, loss_fnc=loss_fnc, net=net)\n",
    "\n",
    "    losses_batch_step_src = generate_losses(data_test=data, net=net, loss_function=loss_fnc)\n",
    "    np_losses = np.sort(np.array(losses_batch_step_src))\n",
    "    losses_batch_step = np_losses[:int(len(np_losses)*LOSS_FACTOR)]\n",
    "    phi_batch_step = np.mean(losses_batch_step, dtype=np.float64) + np.std(losses_batch_step, ddof=1, dtype=np.float64)\n",
    "\n",
    "    print(\"phi:\", phi_batch_step)\n",
    "\n",
    "    detector_metadata.update_one({\"_id\" : meta[0][\"_id\"]},\n",
    "                                 {\"$set\": {\"threshold\": phi_batch_step}})\n",
    "\n",
    "    torch.save(net, '../app/autoencoder_model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}