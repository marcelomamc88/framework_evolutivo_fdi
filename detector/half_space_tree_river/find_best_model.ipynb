{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\MARCELO\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from river import anomaly, preprocessing, compose, ensemble, neighbors, datasets\n",
    "from river.utils import numpy2dict\n",
    "from math import ceil, floor\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "'''\n",
    "EXTERNAL KERNEL\n",
    "'''\n",
    "google_colab = False\n",
    "kaggle = False\n",
    "\n",
    "'''\n",
    "CUDA\n",
    "'''\n",
    "cuda = False\n",
    "\n",
    "'''\n",
    "DATA REPRESENTATION\n",
    "\n",
    "1 => SINGLE READ | 2 => ADD FEATURES | 3 => WINDOW TO FEATURES\n",
    "'''\n",
    "DATA_REPRESENTATION = 2\n",
    "\n",
    "'''\n",
    "DOWNSAMPLE FACTOR\n",
    "\n",
    "1 => 10hz *original rate* | 2 => 5Hz | 5 => 2Hz | 10 => 1hz\n",
    "'''\n",
    "DOWNSAMPLE_FACTOR = 5\n",
    "\n",
    "'''\n",
    "WINDOWS LENGHT\n",
    "\n",
    "* needs divisor by datapoints target\n",
    "* considering downsample factor = 5\n",
    "\n",
    "1 => WINDOW DISABLED | 2 => 1 second | 4 => 2 seconds | 10 => 5 seconds | 20 => 10 seconds | 200 => 100 seconds *full flight*\n",
    "'''\n",
    "WINDOW_LENGHT =  1\n",
    "\n",
    "'''\n",
    "LIMITADOR\n",
    "\n",
    "Quantity of samples in the execution of the tests.\n",
    "'''\n",
    "LIMITADOR = None\n",
    "\n",
    "'''\n",
    "LOSS FACTOR [0,1]\n",
    "\n",
    "Ignores outliers in calculating the stats of losses in regenerated data.\n",
    "'''\n",
    "LOSS_FACTOR = .96\n",
    "\n",
    "'''\n",
    "TRAIN_SIZE [0,1]\n",
    "\n",
    "Percentage of samples to be trained\n",
    "'''\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "'''\n",
    "OUTPUT_FILE_NAME\n",
    "\n",
    "File with output results\n",
    "'''\n",
    "OUTPUT_FILE_NAME = 'output_hst_river_dr_' + str(DATA_REPRESENTATION) + '-ts_' + str(TRAIN_SIZE) + '-lf_' + str.replace(str(LOSS_FACTOR), '.', '') + '-limit_' + str(LIMITADOR) + '-wl_' + str(WINDOW_LENGHT) + '.txt'\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS\n",
    "\n",
    "'''\n",
    "PATH_OUTPUTS = './outputs/'\n",
    "\n",
    "'''\n",
    "PATH_DATASET\n",
    "\n",
    "'''\n",
    "PATH_DATASET = '../../dataset/original/'\n",
    "\n",
    "'''\n",
    "FLUSH FILE\n",
    "\n",
    "If output results file is ON\n",
    "'''\n",
    "FLUSH_FILE = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/ACADÊMICO/MESTRADO/DISSERTAÇÃO/CHAPTERS/5 EXPERIMENTO/dataset/data_representation_1'\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(path+'/F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(path+'/F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(path+'/F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(path+'/F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(path+'/F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(path+'/F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }\n",
    "elif kaggle:\n",
    "    !conda install -y gdown\n",
    "    !gdown --id 1G88okIVmdcgLFlmd7rDRhHvHv98yK3UB\n",
    "    !gdown --id 1fX3utfHMjwKTt7IW4D01bnm-hv88yzrJ\n",
    "    !gdown --id 1yUG3R5zK2AIxtS9Q4Fk-udkKBZeYShgb\n",
    "    !gdown --id 1OBRDtuqNEZ-3Z-q0helWh2xGiAxeLACH\n",
    "    !gdown --id 17oDi60sWYsWHHxzj2aA9m6ARm8zQ81m_\n",
    "    !gdown --id 1jKEK4s5sYJh8PHtpHeV8ABOsHjuB26RA\n",
    "else:\n",
    "    dict_ds_original = {\n",
    "        'data_ds3_normal_t1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t1.csv', header=None),\n",
    "        'data_ds3_normal_t2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_normal_t2.csv', header=None),\n",
    "        'data_ds3_fault1_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault1_leakage.csv', header=None),\n",
    "        'data_ds3_fault2_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault2_viscousfriction.csv', header=None),\n",
    "        'data_ds3_fault3_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault3_compressibility.csv', header=None),\n",
    "        'data_ds3_fault4_original' : pd.read_csv(PATH_DATASET+'F16_DS3_fault4_fixedposition.csv', header=None),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dict_ds = dict_ds_original.copy()\n",
    "\n",
    "if dict_ds['data_ds3_normal_t1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0 or dict_ds['data_ds3_fault1_original'].shape[0] % DOWNSAMPLE_FACTOR != 0:\n",
    "    raise Exception('Needs to be ?shape? divisor')\n",
    "\n",
    "for n, dataset_name in enumerate(dict_ds):\n",
    "    dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "    downsampled = dataset[::DOWNSAMPLE_FACTOR]\n",
    "\n",
    "    x, y = downsampled.shape\n",
    "\n",
    "    # resample\n",
    "    dict_ds[dataset_name] = pd.DataFrame(downsampled.reshape((int(x/WINDOW_LENGHT),y*WINDOW_LENGHT)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# ADD COLUMNS WITH DIFF PREVIOUS VALUES\n",
    "\n",
    "if (DATA_REPRESENTATION == 2):\n",
    "    frame_size = int(1000/DOWNSAMPLE_FACTOR)\n",
    "\n",
    "    for n, dataset_name in enumerate(dict_ds):\n",
    "        dataset = dict_ds[dataset_name].to_numpy()\n",
    "\n",
    "        dimension = dataset.shape[1]\n",
    "        samples = dataset.shape[0]\n",
    "\n",
    "        # GENERATE NEW DIMENSIONS\n",
    "        dataset = np.concatenate((dataset, np.zeros((samples,dimension))), axis=1)\n",
    "\n",
    "        for f in np.arange(0,int(samples/frame_size)):\n",
    "            # OBTAIN THE FRAME FLIGHT\n",
    "            frame = dataset[f*frame_size:(f+1)*frame_size, 0:dimension]\n",
    "\n",
    "            # CALCULATE DIFFERENCE\n",
    "            chunk = np.diff(frame, axis=0)\n",
    "\n",
    "            # DONT CALCULATE THE DIFFERENCE FOR EACH FIRST TIMESTEP\n",
    "            chunk = np.insert(chunk, 0, frame[0, 0:dimension], axis=0)\n",
    "\n",
    "            # UPDATE DATASET WITH NEW FRAME INTO NEW DIMENSIONS\n",
    "            dataset[f*frame_size:(f+1)*frame_size,dimension:dimension*2] = chunk\n",
    "\n",
    "        dict_ds[dataset_name] = pd.DataFrame(dataset)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#ss = StandardScaler()\n",
    "ss = MinMaxScaler()\n",
    "\n",
    "data_ds3_t1_normal = dict_ds['data_ds3_normal_t1_original'].copy()\n",
    "data_ds3_t2_normal = dict_ds['data_ds3_normal_t2_original'].copy()\n",
    "data_ds3_fault1 = dict_ds['data_ds3_fault1_original'].copy()\n",
    "data_ds3_fault2 = dict_ds['data_ds3_fault2_original'].copy()\n",
    "data_ds3_fault3 = dict_ds['data_ds3_fault3_original'].copy()\n",
    "data_ds3_fault4 = dict_ds['data_ds3_fault4_original'].copy()\n",
    "\n",
    "# fit values\n",
    "ss.partial_fit(data_ds3_t1_normal)\n",
    "ss.partial_fit(data_ds3_t2_normal)\n",
    "ss.partial_fit(data_ds3_fault1)\n",
    "ss.partial_fit(data_ds3_fault2)\n",
    "ss.partial_fit(data_ds3_fault3)\n",
    "ss.partial_fit(data_ds3_fault4)\n",
    "\n",
    "# transform values\n",
    "data_ds3_t1_normal = ss.transform(data_ds3_t1_normal)\n",
    "data_ds3_t2_normal = ss.transform(data_ds3_t2_normal)\n",
    "data_ds3_fault1 = ss.transform(data_ds3_fault1)\n",
    "data_ds3_fault2 = ss.transform(data_ds3_fault2)\n",
    "data_ds3_fault3 = ss.transform(data_ds3_fault3)\n",
    "data_ds3_fault4 = ss.transform(data_ds3_fault4)\n",
    "\n",
    "# append normal labels\n",
    "data_ds3_t1_normal = np.append(data_ds3_t1_normal, np.zeros((data_ds3_t1_normal.shape[0],1)), axis = 1)\n",
    "data_ds3_t2_normal = np.append(data_ds3_t2_normal, np.zeros((data_ds3_t2_normal.shape[0],1)), axis = 1)\n",
    "\n",
    "# append fault labels\n",
    "def generate_fault_label(dataset, fault_label):\n",
    "    labels = np.array([[fault_label]]*dataset.shape[0])\n",
    "\n",
    "    return labels\n",
    "\n",
    "data_ds3_fault1 = np.append(data_ds3_fault1, generate_fault_label(data_ds3_fault1, 1), axis = 1)\n",
    "data_ds3_fault2 = np.append(data_ds3_fault2, generate_fault_label(data_ds3_fault2, 2), axis = 1)\n",
    "data_ds3_fault3 = np.append(data_ds3_fault3, generate_fault_label(data_ds3_fault3, 3), axis = 1)\n",
    "data_ds3_fault4 = np.append(data_ds3_fault4, generate_fault_label(data_ds3_fault4, 4), axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    losses = []\n",
    "\n",
    "    for k, x in enumerate(data):\n",
    "        loss = model.score_one(numpy2dict(x))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def percentual(losses, threshold, inlier = True):\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    if inlier:\n",
    "        percentual =  losses[losses <= threshold].shape[0] / losses.shape[0] * 100\n",
    "    else:\n",
    "        percentual = losses[losses > threshold].shape[0] / losses.shape[0] * 100\n",
    "\n",
    "    #print(percentual, file=log)\n",
    "\n",
    "    return percentual\n",
    "\n",
    "hst_p = {\n",
    "    'n_trees': [5, 10, 20, 30],\n",
    "    'height': [2, 4, 8],\n",
    "    'window_size': [50, 100, 250]\n",
    "}\n",
    "\n",
    "hst_p = {\n",
    "    'n_trees': [20],\n",
    "    'height': [8],\n",
    "    'window_size': [100]\n",
    "}\n",
    "\n",
    "phis = [1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_ds3_t1_normal[:, :-1], data_ds3_t1_normal[:, -1], test_size=1-TRAIN_SIZE, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "79.69444444444444\n",
      "NORMAL_2\n",
      "79.79833333333333\n",
      "F1\n",
      "52.196\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "23.464\n",
      "F4\n",
      "99.899\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "77.44722222222222\n",
      "NORMAL_2\n",
      "77.44\n",
      "F1\n",
      "54.379999999999995\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "76.60555555555555\n",
      "NORMAL_2\n",
      "76.8\n",
      "F1\n",
      "65.853\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "76.58888888888889\n",
      "NORMAL_2\n",
      "76.79277777777777\n",
      "F1\n",
      "65.862\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "76.48611111111111\n",
      "NORMAL_2\n",
      "76.67777777777778\n",
      "F1\n",
      "66.512\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "72.66388888888888\n",
      "NORMAL_2\n",
      "72.21111111111111\n",
      "F1\n",
      "66.513\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "100.0\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "76.59722222222223\n",
      "NORMAL_2\n",
      "76.7988888888889\n",
      "F1\n",
      "65.862\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "75.90833333333333\n",
      "NORMAL_2\n",
      "76.10055555555556\n",
      "F1\n",
      "66.513\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "75.90833333333333\n",
      "NORMAL_2\n",
      "76.10055555555556\n",
      "F1\n",
      "66.513\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n",
      "-HalfSpaceTrees 20 8 0.10000 100 1\n",
      "X_TEST\n",
      "76.01944444444445\n",
      "NORMAL_2\n",
      "76.21777777777777\n",
      "F1\n",
      "65.863\n",
      "F2\n",
      "100.0\n",
      "F3\n",
      "70.053\n",
      "F4\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Increment Parameters\n",
    "'''\n",
    "increments = 10\n",
    "block_size = int(floor(len(X_train) / increments))\n",
    "\n",
    "log = None\n",
    "if FLUSH_FILE:\n",
    "    log = open(PATH_OUTPUTS+OUTPUT_FILE_NAME, \"a\", buffering=1)\n",
    "\n",
    "i = 0\n",
    "for n in hst_p['n_trees']:\n",
    "    for h in hst_p['height']:\n",
    "        for ws in hst_p['window_size']:\n",
    "            print('.', end='')\n",
    "            clf = compose.Pipeline(anomaly.HalfSpaceTrees(n_trees=n, height=h, window_size=ws, seed=42))\n",
    "\n",
    "            samples_block = []\n",
    "            for c, x in enumerate(X_train, 1):\n",
    "                samples_block.append(x)\n",
    "                clf.score_one(numpy2dict(x))\n",
    "                clf = clf.learn_one(numpy2dict(x), 0)\n",
    "\n",
    "\n",
    "                r = np.zeros((increments, 6))\n",
    "\n",
    "                if c % block_size == 0:\n",
    "                    print('-', end='')\n",
    "                    scores_original = predict(clf, samples_block) #n1\n",
    "                    scores_original = np.sort(scores_original)\n",
    "                    samples_block = []\n",
    "\n",
    "                    scores_n1 = predict(clf, X_test)\n",
    "                    scores_n2 = predict(clf, data_ds3_t2_normal[:LIMITADOR, :-1])\n",
    "                    scores_f1 = predict(clf, data_ds3_fault1[:LIMITADOR, :-1])\n",
    "                    scores_f2 = predict(clf, data_ds3_fault2[:LIMITADOR, :-1])\n",
    "                    scores_f3 = predict(clf, data_ds3_fault3[:LIMITADOR, :-1])\n",
    "                    scores_f4 = predict(clf, data_ds3_fault4[:LIMITADOR, :-1])\n",
    "\n",
    "                    for phi in phis:\n",
    "                        scores = scores_original[:int(len(scores_original)*phi)]\n",
    "                        t_ = np.mean(scores) + np.std(scores, ddof=1)\n",
    "                        t = min(0.1, t_)\n",
    "\n",
    "                        r[i][0] = percentual(scores_n1, t, True)\n",
    "                        r[i][1] = percentual(scores_n2, t, True)\n",
    "                        r[i][2] = percentual(scores_f1, t, False)\n",
    "                        r[i][3] = percentual(scores_f2, t, False)\n",
    "                        r[i][4] = percentual(scores_f3, t, False)\n",
    "                        r[i][5] = percentual(scores_f4, t, False)\n",
    "\n",
    "                        print(clf, n, h, '%.5f'%(t), ws, phi, file=log)\n",
    "                        print('X_TEST', file=log)\n",
    "                        print(r[i][0], file=log)\n",
    "                        print('NORMAL_2', file=log)\n",
    "                        print(r[i][1], file=log)\n",
    "                        print('F1', file=log)\n",
    "                        print(r[i][2], file=log)\n",
    "                        print('F2', file=log)\n",
    "                        print(r[i][3], file=log)\n",
    "                        print('F3', file=log)\n",
    "                        print(r[i][4], file=log)\n",
    "                        print('F4', file=log)\n",
    "                        print(r[i][5], file=log)\n",
    "\n",
    "                        i = i + 1\n",
    "\n",
    "if FLUSH_FILE:\n",
    "    log.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [ 76.01944444,  76.21777778,  65.863     , 100.        ,\n         70.053     , 100.        ]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "  # `density=False` would make counts\n",
    "plt.hist(scores_f1, density=False, bins=50, color='blue')\n",
    "plt.plot([np.mean(scores_f1)]*1000,np.arange(0,1000), color='blue')\n",
    "plt.plot([np.mean(scores_f1)+np.std(scores_f1, ddof=1)]*1000,np.arange(0,1000), color='blue')\n",
    "\n",
    "plt.hist(scores_f2, density=False, bins=50, color='yellow')\n",
    "plt.plot([np.mean(scores_f2)]*1000,np.arange(0,1000), color='yellow')\n",
    "plt.plot([np.mean(scores_f2)+np.std(scores_f2, ddof=1)]*1000,np.arange(0,1000), color='yellow')\n",
    "\n",
    "plt.hist(scores_f3, density=False, bins=50, color='green')\n",
    "plt.plot([np.mean(scores_f3)]*1000,np.arange(0,1000), color='green')\n",
    "plt.plot([np.mean(scores_f3)+np.std(scores_f3, ddof=1)]*1000,np.arange(0,1000), color='green')\n",
    "\n",
    "plt.hist(scores_f4, density=False, bins=50, color='pink')\n",
    "plt.plot([np.mean(scores_f4)]*1000,np.arange(0,1000), color='pink')\n",
    "plt.plot([np.mean(scores_f4)+np.std(scores_f4, ddof=1)]*1000,np.arange(0,1000), color='pink')\n",
    "\n",
    "plt.hist(scores_original, density=False, bins=50, color='red')\n",
    "plt.plot([np.mean(scores)]*1000,np.arange(0,1000), color='red')\n",
    "plt.plot([np.mean(scores)+np.std(scores, ddof=1)]*1000,np.arange(0,1000), color='red')\n",
    "plt.plot([np.median(scores)]*1000,np.arange(0,1000), color='red')\n",
    "plt.plot([0.1]*1000,np.arange(0,1000), color='yellow')\n",
    "\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.std(scores, ddof=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}